{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "import queue\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "plot_queue = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(x):\n",
    "    return np.where(x < -5,\n",
    "              np.sin(x) + np.cos(2*x) + 0.5*x**2 - 2,\n",
    "              np.where(x < 0,\n",
    "                np.sin(x) + np.cos(2*x) + 0.5*x**2 - 2*x,\n",
    "                np.sin(x) + np.cos(2*x) + np.log(x + 2) + np.sqrt(x) + 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DINGNUOCHEN\\AppData\\Local\\Temp\\ipykernel_23844\\3560043911.py:6: RuntimeWarning: invalid value encountered in log\n",
      "  np.sin(x) + np.cos(2*x) + np.log(x + 2) + np.sqrt(x) + 10))\n",
      "C:\\Users\\DINGNUOCHEN\\AppData\\Local\\Temp\\ipykernel_23844\\3560043911.py:6: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sin(x) + np.cos(2*x) + np.log(x + 2) + np.sqrt(x) + 10))\n"
     ]
    }
   ],
   "source": [
    "# 生成訓練數據\n",
    "x_train = np.linspace(-10, 10, 1000)\n",
    "y_train = true_function(x_train)\n",
    "\n",
    "# 生成驗證數據\n",
    "x_val = np.linspace(-11, 11, 200)\n",
    "y_val = true_function(x_val)\n",
    "\n",
    "# 轉換成 PyTorch 張量\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).view(-1, 1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x_train_tensor = x_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "x_val_tensor = x_val_tensor.to(device)\n",
    "y_val_tensor = y_val_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 繪製結果\n",
    "plt.plot(x_train, y_train, label='True Function')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Comparison of Regression Techniques')\n",
    "plt.grid(True)\n",
    "\n",
    "# 設定 y 軸的最大值為數值中的最大值*1.1\n",
    "plt.ylim([min(y_train), max(y_train) * 1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialApproximator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolynomialApproximator, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 128) \n",
    "        self.fc2 = nn.Linear(128, 256) \n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 1)  \n",
    "        self.dropout = nn.Dropout(p=0.5)  \n",
    "        self.batch_norm1 = nn.BatchNorm1d(128)  \n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batch_norm1(x)  \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout(x)  \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# 創建模型\n",
    "model = PolynomialApproximator()\n",
    "model.to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10000], Training Loss: 312.8001, Validation Loss: 435.3118\n",
      "Epoch [10/10000], Training Loss: 285.5858, Validation Loss: 400.8710\n",
      "Epoch [15/10000], Training Loss: 269.9369, Validation Loss: 360.1030\n",
      "Epoch [20/10000], Training Loss: 260.9765, Validation Loss: 334.7549\n",
      "Epoch [25/10000], Training Loss: 256.8050, Validation Loss: 334.4390\n",
      "Epoch [30/10000], Training Loss: 253.4585, Validation Loss: 331.0607\n",
      "Epoch [35/10000], Training Loss: 253.7773, Validation Loss: 294.4449\n",
      "Epoch [40/10000], Training Loss: 247.9371, Validation Loss: 271.5818\n",
      "Epoch [45/10000], Training Loss: 245.8454, Validation Loss: 262.5264\n",
      "Epoch [50/10000], Training Loss: 242.2085, Validation Loss: 248.0855\n",
      "Epoch [55/10000], Training Loss: 240.6565, Validation Loss: 237.9167\n",
      "Epoch [60/10000], Training Loss: 236.4565, Validation Loss: 227.3607\n",
      "Epoch [65/10000], Training Loss: 233.3846, Validation Loss: 223.4449\n",
      "Epoch [70/10000], Training Loss: 226.9123, Validation Loss: 209.8293\n",
      "Epoch [75/10000], Training Loss: 220.0020, Validation Loss: 203.5574\n",
      "Epoch [80/10000], Training Loss: 215.4747, Validation Loss: 206.2605\n",
      "Epoch [85/10000], Training Loss: 205.0849, Validation Loss: 197.0220\n",
      "Epoch [90/10000], Training Loss: 192.6657, Validation Loss: 183.4512\n",
      "Epoch [95/10000], Training Loss: 184.4495, Validation Loss: 169.4693\n",
      "Epoch [100/10000], Training Loss: 173.9624, Validation Loss: 152.1568\n",
      "Epoch [105/10000], Training Loss: 154.0148, Validation Loss: 122.4736\n",
      "Epoch [110/10000], Training Loss: 141.7117, Validation Loss: 114.2555\n",
      "Epoch [115/10000], Training Loss: 131.3750, Validation Loss: 95.7826\n",
      "Epoch [120/10000], Training Loss: 115.6789, Validation Loss: 83.4366\n",
      "Epoch [125/10000], Training Loss: 102.9930, Validation Loss: 73.3163\n",
      "Epoch [130/10000], Training Loss: 89.1601, Validation Loss: 54.4534\n",
      "Epoch [135/10000], Training Loss: 80.6775, Validation Loss: 33.3004\n",
      "Epoch [140/10000], Training Loss: 84.0482, Validation Loss: 23.5313\n",
      "Epoch [145/10000], Training Loss: 73.2975, Validation Loss: 24.0891\n",
      "Epoch [150/10000], Training Loss: 71.7238, Validation Loss: 20.4126\n",
      "Epoch [155/10000], Training Loss: 68.0168, Validation Loss: 27.8006\n",
      "Epoch [160/10000], Training Loss: 59.7304, Validation Loss: 20.3686\n",
      "Epoch [165/10000], Training Loss: 59.0914, Validation Loss: 20.3162\n",
      "Epoch [170/10000], Training Loss: 62.9271, Validation Loss: 20.0431\n",
      "Epoch [175/10000], Training Loss: 59.5264, Validation Loss: 20.1423\n",
      "Epoch [180/10000], Training Loss: 57.4069, Validation Loss: 20.0518\n",
      "Epoch [185/10000], Training Loss: 55.5146, Validation Loss: 16.2271\n",
      "Epoch [190/10000], Training Loss: 50.2859, Validation Loss: 14.6636\n",
      "Epoch [195/10000], Training Loss: 45.0558, Validation Loss: 14.5511\n",
      "Epoch [200/10000], Training Loss: 49.1864, Validation Loss: 13.7314\n",
      "Epoch [205/10000], Training Loss: 48.1501, Validation Loss: 10.4915\n",
      "Epoch [210/10000], Training Loss: 46.8246, Validation Loss: 9.1923\n",
      "Epoch [215/10000], Training Loss: 48.2066, Validation Loss: 10.7692\n",
      "Epoch [220/10000], Training Loss: 43.3606, Validation Loss: 12.5110\n",
      "Epoch [225/10000], Training Loss: 42.2863, Validation Loss: 13.5496\n",
      "Epoch [230/10000], Training Loss: 41.3328, Validation Loss: 9.4905\n",
      "Epoch [235/10000], Training Loss: 41.1391, Validation Loss: 8.9634\n",
      "Epoch [240/10000], Training Loss: 43.8103, Validation Loss: 9.0497\n",
      "Epoch [245/10000], Training Loss: 40.8028, Validation Loss: 9.6613\n",
      "Epoch [250/10000], Training Loss: 34.3535, Validation Loss: 9.8800\n",
      "Epoch [255/10000], Training Loss: 34.4503, Validation Loss: 7.6761\n",
      "Epoch [260/10000], Training Loss: 39.4659, Validation Loss: 6.4112\n",
      "Epoch [265/10000], Training Loss: 35.8092, Validation Loss: 6.9491\n",
      "Epoch [270/10000], Training Loss: 32.5903, Validation Loss: 8.5308\n",
      "Epoch [275/10000], Training Loss: 33.7691, Validation Loss: 7.8633\n",
      "Epoch [280/10000], Training Loss: 30.5712, Validation Loss: 7.3602\n",
      "Epoch [285/10000], Training Loss: 36.5932, Validation Loss: 6.9977\n",
      "Epoch [290/10000], Training Loss: 28.7010, Validation Loss: 7.3449\n",
      "Epoch [295/10000], Training Loss: 31.7640, Validation Loss: 8.7165\n",
      "Epoch [300/10000], Training Loss: 31.8936, Validation Loss: 7.5891\n",
      "Epoch [305/10000], Training Loss: 32.6921, Validation Loss: 8.0384\n",
      "Epoch [310/10000], Training Loss: 27.8023, Validation Loss: 5.4394\n",
      "Epoch [315/10000], Training Loss: 30.4660, Validation Loss: 6.7151\n",
      "Epoch [320/10000], Training Loss: 30.8201, Validation Loss: 5.6502\n",
      "Epoch [325/10000], Training Loss: 27.4821, Validation Loss: 5.2681\n",
      "Epoch [330/10000], Training Loss: 30.8067, Validation Loss: 4.8495\n",
      "Epoch [335/10000], Training Loss: 29.5068, Validation Loss: 5.8260\n",
      "Epoch [340/10000], Training Loss: 28.9879, Validation Loss: 5.9800\n",
      "Epoch [345/10000], Training Loss: 28.7212, Validation Loss: 5.8107\n",
      "Epoch [350/10000], Training Loss: 27.5707, Validation Loss: 6.0210\n",
      "Epoch [355/10000], Training Loss: 28.6385, Validation Loss: 4.5502\n",
      "Epoch [360/10000], Training Loss: 26.6959, Validation Loss: 4.6909\n",
      "Epoch [365/10000], Training Loss: 25.7764, Validation Loss: 3.8921\n",
      "Epoch [370/10000], Training Loss: 27.7308, Validation Loss: 5.1696\n",
      "Epoch [375/10000], Training Loss: 27.3560, Validation Loss: 4.9260\n",
      "Epoch [380/10000], Training Loss: 24.8918, Validation Loss: 4.7547\n",
      "Epoch [385/10000], Training Loss: 24.6356, Validation Loss: 6.0428\n",
      "Epoch [390/10000], Training Loss: 23.7169, Validation Loss: 5.3277\n",
      "Epoch [395/10000], Training Loss: 23.5792, Validation Loss: 4.6218\n",
      "Epoch [400/10000], Training Loss: 24.4563, Validation Loss: 3.4959\n",
      "Epoch [405/10000], Training Loss: 24.1397, Validation Loss: 3.3625\n",
      "Epoch [410/10000], Training Loss: 24.1809, Validation Loss: 3.4206\n",
      "Epoch [415/10000], Training Loss: 23.3885, Validation Loss: 3.6684\n",
      "Epoch [420/10000], Training Loss: 24.1658, Validation Loss: 5.1200\n",
      "Epoch [425/10000], Training Loss: 21.8236, Validation Loss: 3.9506\n",
      "Epoch [430/10000], Training Loss: 24.7464, Validation Loss: 3.5562\n",
      "Epoch [435/10000], Training Loss: 21.4011, Validation Loss: 4.4819\n",
      "Epoch [440/10000], Training Loss: 17.9571, Validation Loss: 4.2327\n",
      "Epoch [445/10000], Training Loss: 19.9642, Validation Loss: 3.5350\n",
      "Epoch [450/10000], Training Loss: 21.6928, Validation Loss: 3.7467\n",
      "Epoch [455/10000], Training Loss: 22.2381, Validation Loss: 2.9870\n",
      "Epoch [460/10000], Training Loss: 21.3250, Validation Loss: 3.8613\n",
      "Epoch [465/10000], Training Loss: 23.6709, Validation Loss: 3.9545\n",
      "Epoch [470/10000], Training Loss: 22.5721, Validation Loss: 3.8213\n",
      "Epoch [475/10000], Training Loss: 19.9086, Validation Loss: 5.5949\n",
      "Epoch [480/10000], Training Loss: 20.0580, Validation Loss: 3.3260\n",
      "Epoch [485/10000], Training Loss: 18.4571, Validation Loss: 4.4968\n",
      "Epoch [490/10000], Training Loss: 18.1419, Validation Loss: 4.4684\n",
      "Epoch [495/10000], Training Loss: 20.4978, Validation Loss: 3.8556\n",
      "Epoch [500/10000], Training Loss: 20.4575, Validation Loss: 4.4988\n",
      "Epoch [505/10000], Training Loss: 21.8887, Validation Loss: 4.5948\n",
      "Epoch [510/10000], Training Loss: 19.0058, Validation Loss: 3.3653\n",
      "Epoch [515/10000], Training Loss: 20.0459, Validation Loss: 4.1328\n",
      "Epoch [520/10000], Training Loss: 16.9115, Validation Loss: 3.6144\n",
      "Epoch [525/10000], Training Loss: 19.6713, Validation Loss: 3.6288\n",
      "Epoch [530/10000], Training Loss: 17.0411, Validation Loss: 4.7598\n",
      "Epoch [535/10000], Training Loss: 18.0787, Validation Loss: 3.3500\n",
      "Epoch [540/10000], Training Loss: 18.0465, Validation Loss: 4.9882\n",
      "Epoch [545/10000], Training Loss: 19.2116, Validation Loss: 4.5475\n",
      "Epoch [550/10000], Training Loss: 18.7353, Validation Loss: 4.1525\n",
      "Epoch [555/10000], Training Loss: 19.3946, Validation Loss: 5.1826\n",
      "Epoch [560/10000], Training Loss: 16.6909, Validation Loss: 3.9312\n",
      "Epoch [565/10000], Training Loss: 17.9231, Validation Loss: 4.7365\n",
      "Epoch [570/10000], Training Loss: 18.3018, Validation Loss: 3.4922\n",
      "Epoch [575/10000], Training Loss: 19.5980, Validation Loss: 3.3183\n",
      "Epoch [580/10000], Training Loss: 16.0935, Validation Loss: 3.4160\n",
      "Epoch [585/10000], Training Loss: 16.1573, Validation Loss: 3.8125\n",
      "Epoch [590/10000], Training Loss: 15.6268, Validation Loss: 2.7834\n",
      "Epoch [595/10000], Training Loss: 17.4554, Validation Loss: 3.1554\n",
      "Epoch [600/10000], Training Loss: 15.9023, Validation Loss: 2.8467\n",
      "Epoch [605/10000], Training Loss: 15.5175, Validation Loss: 2.8652\n",
      "Epoch [610/10000], Training Loss: 17.8381, Validation Loss: 2.8080\n",
      "Epoch [615/10000], Training Loss: 16.0624, Validation Loss: 2.7536\n",
      "Epoch [620/10000], Training Loss: 15.2609, Validation Loss: 3.5603\n",
      "Epoch [625/10000], Training Loss: 17.8049, Validation Loss: 2.9355\n",
      "Epoch [630/10000], Training Loss: 14.0547, Validation Loss: 2.8147\n",
      "Epoch [635/10000], Training Loss: 15.5472, Validation Loss: 3.2703\n",
      "Epoch [640/10000], Training Loss: 15.3419, Validation Loss: 2.0664\n",
      "Epoch [645/10000], Training Loss: 14.7359, Validation Loss: 3.2449\n",
      "Epoch [650/10000], Training Loss: 14.3155, Validation Loss: 2.4160\n",
      "Epoch [655/10000], Training Loss: 13.9420, Validation Loss: 3.9022\n",
      "Epoch [660/10000], Training Loss: 14.1354, Validation Loss: 2.7128\n",
      "Epoch [665/10000], Training Loss: 15.2013, Validation Loss: 2.3680\n",
      "Epoch [670/10000], Training Loss: 14.0503, Validation Loss: 2.0293\n",
      "Epoch [675/10000], Training Loss: 14.3261, Validation Loss: 1.7519\n",
      "Epoch [680/10000], Training Loss: 15.5048, Validation Loss: 2.0858\n",
      "Epoch [685/10000], Training Loss: 14.1208, Validation Loss: 2.4079\n",
      "Epoch [690/10000], Training Loss: 14.4731, Validation Loss: 3.5019\n",
      "Epoch [695/10000], Training Loss: 14.6487, Validation Loss: 1.9931\n",
      "Epoch [700/10000], Training Loss: 14.1868, Validation Loss: 1.8163\n",
      "Epoch [705/10000], Training Loss: 13.2524, Validation Loss: 2.6045\n",
      "Epoch [710/10000], Training Loss: 14.0805, Validation Loss: 1.9070\n",
      "Epoch [715/10000], Training Loss: 15.0337, Validation Loss: 2.5126\n",
      "Epoch [720/10000], Training Loss: 14.3059, Validation Loss: 2.0963\n",
      "Epoch [725/10000], Training Loss: 11.7491, Validation Loss: 1.9276\n",
      "Epoch [730/10000], Training Loss: 13.4056, Validation Loss: 2.0400\n",
      "Epoch [735/10000], Training Loss: 13.3085, Validation Loss: 1.7868\n",
      "Epoch [740/10000], Training Loss: 12.4506, Validation Loss: 2.3742\n",
      "Epoch [745/10000], Training Loss: 13.1204, Validation Loss: 2.5930\n",
      "Epoch [750/10000], Training Loss: 12.0404, Validation Loss: 1.9530\n",
      "Epoch [755/10000], Training Loss: 13.8173, Validation Loss: 1.9220\n",
      "Epoch [760/10000], Training Loss: 13.4219, Validation Loss: 2.0300\n",
      "Epoch [765/10000], Training Loss: 13.8946, Validation Loss: 2.1437\n",
      "Epoch [770/10000], Training Loss: 10.5456, Validation Loss: 2.2669\n",
      "Epoch [775/10000], Training Loss: 12.7771, Validation Loss: 1.8769\n",
      "Epoch [780/10000], Training Loss: 11.9126, Validation Loss: 1.9061\n",
      "Epoch [785/10000], Training Loss: 11.8881, Validation Loss: 2.5048\n",
      "Epoch [790/10000], Training Loss: 11.7545, Validation Loss: 2.3542\n",
      "Epoch [795/10000], Training Loss: 11.7228, Validation Loss: 2.9941\n",
      "Epoch [800/10000], Training Loss: 11.4716, Validation Loss: 2.2911\n",
      "Epoch [805/10000], Training Loss: 11.6097, Validation Loss: 3.3120\n",
      "Epoch [810/10000], Training Loss: 12.2751, Validation Loss: 2.0878\n",
      "Epoch [815/10000], Training Loss: 11.3056, Validation Loss: 2.4522\n",
      "Epoch [820/10000], Training Loss: 11.7599, Validation Loss: 2.2382\n",
      "Epoch [825/10000], Training Loss: 11.6808, Validation Loss: 2.2494\n",
      "Epoch [830/10000], Training Loss: 11.6298, Validation Loss: 2.5579\n",
      "Epoch [835/10000], Training Loss: 12.3237, Validation Loss: 2.1167\n",
      "Epoch [840/10000], Training Loss: 11.1650, Validation Loss: 2.3900\n",
      "Epoch [845/10000], Training Loss: 11.5294, Validation Loss: 2.9234\n",
      "Epoch [850/10000], Training Loss: 11.3547, Validation Loss: 2.3496\n",
      "Epoch [855/10000], Training Loss: 11.8040, Validation Loss: 2.6907\n",
      "Epoch [860/10000], Training Loss: 9.9400, Validation Loss: 2.4204\n",
      "Epoch [865/10000], Training Loss: 11.1082, Validation Loss: 2.4023\n",
      "Epoch [870/10000], Training Loss: 10.8210, Validation Loss: 2.4468\n",
      "Epoch [875/10000], Training Loss: 10.6493, Validation Loss: 2.8255\n",
      "Epoch [880/10000], Training Loss: 11.0824, Validation Loss: 2.8852\n",
      "Epoch [885/10000], Training Loss: 11.2804, Validation Loss: 3.0142\n",
      "Epoch [890/10000], Training Loss: 11.0496, Validation Loss: 3.4230\n",
      "Epoch [895/10000], Training Loss: 10.7749, Validation Loss: 3.3643\n",
      "Epoch [900/10000], Training Loss: 9.9471, Validation Loss: 3.0648\n",
      "Epoch [905/10000], Training Loss: 10.1380, Validation Loss: 3.5915\n",
      "Epoch [910/10000], Training Loss: 9.1917, Validation Loss: 3.7382\n",
      "Epoch [915/10000], Training Loss: 10.3452, Validation Loss: 3.4943\n",
      "Epoch [920/10000], Training Loss: 10.7048, Validation Loss: 3.9828\n",
      "Epoch [925/10000], Training Loss: 9.6278, Validation Loss: 4.5246\n",
      "Epoch [930/10000], Training Loss: 9.4489, Validation Loss: 5.1128\n",
      "Epoch [935/10000], Training Loss: 10.0002, Validation Loss: 4.6530\n",
      "Epoch [940/10000], Training Loss: 10.6692, Validation Loss: 4.8549\n",
      "Epoch [945/10000], Training Loss: 9.2641, Validation Loss: 5.4134\n",
      "Epoch [950/10000], Training Loss: 10.1757, Validation Loss: 5.0007\n",
      "Epoch [955/10000], Training Loss: 9.5474, Validation Loss: 2.5640\n",
      "Epoch [960/10000], Training Loss: 9.4676, Validation Loss: 2.2680\n",
      "Epoch [965/10000], Training Loss: 9.1319, Validation Loss: 2.0845\n",
      "Epoch [970/10000], Training Loss: 9.6289, Validation Loss: 1.9168\n",
      "Epoch [975/10000], Training Loss: 10.1031, Validation Loss: 1.9151\n",
      "Epoch [980/10000], Training Loss: 9.8236, Validation Loss: 1.8943\n",
      "Epoch [985/10000], Training Loss: 8.8300, Validation Loss: 2.3518\n",
      "Epoch [990/10000], Training Loss: 8.7439, Validation Loss: 1.7471\n",
      "Epoch [995/10000], Training Loss: 9.0393, Validation Loss: 2.1253\n",
      "Epoch [1000/10000], Training Loss: 9.1084, Validation Loss: 2.1173\n",
      "Epoch [1005/10000], Training Loss: 9.1252, Validation Loss: 2.1796\n",
      "Epoch [1010/10000], Training Loss: 9.4777, Validation Loss: 2.0210\n",
      "Epoch [1015/10000], Training Loss: 8.7402, Validation Loss: 1.9879\n",
      "Epoch [1020/10000], Training Loss: 9.1331, Validation Loss: 2.1389\n",
      "Epoch [1025/10000], Training Loss: 9.0491, Validation Loss: 1.8678\n",
      "Epoch [1030/10000], Training Loss: 8.4595, Validation Loss: 1.9805\n",
      "Epoch [1035/10000], Training Loss: 8.5202, Validation Loss: 1.8817\n",
      "Epoch [1040/10000], Training Loss: 8.3044, Validation Loss: 2.0998\n",
      "Epoch [1045/10000], Training Loss: 8.9261, Validation Loss: 1.3750\n",
      "Epoch [1050/10000], Training Loss: 8.3519, Validation Loss: 2.1327\n",
      "Epoch [1055/10000], Training Loss: 7.9708, Validation Loss: 2.0756\n",
      "Epoch [1060/10000], Training Loss: 8.7836, Validation Loss: 3.4272\n",
      "Epoch [1065/10000], Training Loss: 9.3818, Validation Loss: 2.3237\n",
      "Epoch [1070/10000], Training Loss: 9.0212, Validation Loss: 4.0051\n",
      "Epoch [1075/10000], Training Loss: 8.1673, Validation Loss: 2.3344\n",
      "Epoch [1080/10000], Training Loss: 8.8805, Validation Loss: 3.0427\n",
      "Epoch [1085/10000], Training Loss: 8.5058, Validation Loss: 4.5293\n",
      "Epoch [1090/10000], Training Loss: 7.3470, Validation Loss: 6.4359\n",
      "Epoch [1095/10000], Training Loss: 7.8408, Validation Loss: 5.3873\n",
      "Epoch [1100/10000], Training Loss: 8.3550, Validation Loss: 3.4202\n",
      "Epoch [1105/10000], Training Loss: 8.1889, Validation Loss: 4.3710\n",
      "Epoch [1110/10000], Training Loss: 8.1277, Validation Loss: 2.8932\n",
      "Epoch [1115/10000], Training Loss: 8.3628, Validation Loss: 2.0130\n",
      "Epoch [1120/10000], Training Loss: 7.8988, Validation Loss: 2.1071\n",
      "Epoch [1125/10000], Training Loss: 8.4891, Validation Loss: 1.6360\n",
      "Epoch [1130/10000], Training Loss: 7.8190, Validation Loss: 3.1457\n",
      "Epoch [1135/10000], Training Loss: 8.2134, Validation Loss: 2.8025\n",
      "Epoch [1140/10000], Training Loss: 8.8885, Validation Loss: 4.0225\n",
      "Epoch [1145/10000], Training Loss: 7.6648, Validation Loss: 3.6930\n",
      "Epoch [1150/10000], Training Loss: 8.5771, Validation Loss: 2.5533\n",
      "Epoch [1155/10000], Training Loss: 6.9705, Validation Loss: 3.5220\n",
      "Epoch [1160/10000], Training Loss: 6.9955, Validation Loss: 2.7588\n",
      "Epoch [1165/10000], Training Loss: 7.9067, Validation Loss: 2.8618\n",
      "Epoch [1170/10000], Training Loss: 8.2930, Validation Loss: 2.6543\n",
      "Epoch [1175/10000], Training Loss: 7.8932, Validation Loss: 2.4342\n",
      "Epoch [1180/10000], Training Loss: 7.2267, Validation Loss: 1.4990\n",
      "Epoch [1185/10000], Training Loss: 6.4745, Validation Loss: 1.2678\n",
      "Epoch [1190/10000], Training Loss: 6.6235, Validation Loss: 2.0418\n",
      "Epoch [1195/10000], Training Loss: 6.4255, Validation Loss: 1.8899\n",
      "Epoch [1200/10000], Training Loss: 7.1092, Validation Loss: 3.2623\n",
      "Epoch [1205/10000], Training Loss: 6.8962, Validation Loss: 1.9196\n",
      "Epoch [1210/10000], Training Loss: 7.0222, Validation Loss: 4.0937\n",
      "Epoch [1215/10000], Training Loss: 6.1917, Validation Loss: 3.3312\n",
      "Epoch [1220/10000], Training Loss: 6.8357, Validation Loss: 4.3713\n",
      "Epoch [1225/10000], Training Loss: 6.7148, Validation Loss: 2.9994\n",
      "Epoch [1230/10000], Training Loss: 7.0940, Validation Loss: 2.7170\n",
      "Epoch [1235/10000], Training Loss: 6.3242, Validation Loss: 1.6480\n",
      "Epoch [1240/10000], Training Loss: 6.2909, Validation Loss: 1.8335\n",
      "Epoch [1245/10000], Training Loss: 6.1687, Validation Loss: 1.9492\n",
      "Epoch [1250/10000], Training Loss: 7.2649, Validation Loss: 2.1797\n",
      "Epoch [1255/10000], Training Loss: 6.4327, Validation Loss: 3.3725\n",
      "Epoch [1260/10000], Training Loss: 6.7863, Validation Loss: 2.7986\n",
      "Epoch [1265/10000], Training Loss: 6.7007, Validation Loss: 3.3294\n",
      "Epoch [1270/10000], Training Loss: 6.8518, Validation Loss: 5.5165\n",
      "Epoch [1275/10000], Training Loss: 6.1457, Validation Loss: 5.9556\n",
      "Epoch [1280/10000], Training Loss: 6.0952, Validation Loss: 5.4452\n",
      "Epoch [1285/10000], Training Loss: 6.8162, Validation Loss: 5.7249\n",
      "Epoch [1290/10000], Training Loss: 7.1776, Validation Loss: 4.9029\n",
      "Epoch [1295/10000], Training Loss: 6.4833, Validation Loss: 4.0413\n",
      "Epoch [1300/10000], Training Loss: 5.3826, Validation Loss: 3.7371\n",
      "Epoch [1305/10000], Training Loss: 6.0699, Validation Loss: 3.8540\n",
      "Epoch [1310/10000], Training Loss: 6.5140, Validation Loss: 3.3518\n",
      "Epoch [1315/10000], Training Loss: 6.0679, Validation Loss: 5.8523\n",
      "Epoch [1320/10000], Training Loss: 6.1591, Validation Loss: 4.1244\n",
      "Epoch [1325/10000], Training Loss: 6.0499, Validation Loss: 5.5824\n",
      "Epoch [1330/10000], Training Loss: 6.6409, Validation Loss: 4.7757\n",
      "Epoch [1335/10000], Training Loss: 5.9724, Validation Loss: 7.4797\n",
      "Epoch [1340/10000], Training Loss: 5.8260, Validation Loss: 6.3463\n",
      "Epoch [1345/10000], Training Loss: 6.5054, Validation Loss: 5.6991\n",
      "Epoch [1350/10000], Training Loss: 5.4980, Validation Loss: 3.6875\n",
      "Epoch [1355/10000], Training Loss: 6.4835, Validation Loss: 7.0886\n",
      "Epoch [1360/10000], Training Loss: 5.6155, Validation Loss: 4.0770\n",
      "Epoch [1365/10000], Training Loss: 5.4815, Validation Loss: 7.6182\n",
      "Epoch [1370/10000], Training Loss: 6.0360, Validation Loss: 6.5314\n",
      "Epoch [1375/10000], Training Loss: 5.5181, Validation Loss: 6.3183\n",
      "Epoch [1380/10000], Training Loss: 5.6953, Validation Loss: 4.1114\n",
      "Epoch [1385/10000], Training Loss: 6.1227, Validation Loss: 3.3477\n",
      "Epoch [1390/10000], Training Loss: 5.2238, Validation Loss: 3.2410\n",
      "Epoch [1395/10000], Training Loss: 5.6137, Validation Loss: 3.3316\n",
      "Epoch [1400/10000], Training Loss: 5.5753, Validation Loss: 4.0623\n",
      "Epoch [1405/10000], Training Loss: 6.4436, Validation Loss: 5.6489\n",
      "Epoch [1410/10000], Training Loss: 5.6385, Validation Loss: 5.8500\n",
      "Epoch [1415/10000], Training Loss: 6.0354, Validation Loss: 7.1077\n",
      "Epoch [1420/10000], Training Loss: 5.7363, Validation Loss: 6.0620\n",
      "Epoch [1425/10000], Training Loss: 5.5584, Validation Loss: 6.6039\n",
      "Epoch [1430/10000], Training Loss: 4.9903, Validation Loss: 3.0401\n",
      "Epoch [1435/10000], Training Loss: 5.1966, Validation Loss: 1.9565\n",
      "Epoch [1440/10000], Training Loss: 5.6058, Validation Loss: 2.2962\n",
      "Epoch [1445/10000], Training Loss: 4.8249, Validation Loss: 3.1358\n",
      "Epoch [1450/10000], Training Loss: 5.8897, Validation Loss: 4.0330\n",
      "Epoch [1455/10000], Training Loss: 5.4434, Validation Loss: 4.5182\n",
      "Epoch [1460/10000], Training Loss: 4.9241, Validation Loss: 4.8415\n",
      "Epoch [1465/10000], Training Loss: 4.7645, Validation Loss: 4.5675\n",
      "Epoch [1470/10000], Training Loss: 5.3751, Validation Loss: 3.8078\n",
      "Epoch [1475/10000], Training Loss: 5.0192, Validation Loss: 2.2141\n",
      "Epoch [1480/10000], Training Loss: 5.2915, Validation Loss: 1.7251\n",
      "Epoch [1485/10000], Training Loss: 5.1231, Validation Loss: 1.2136\n",
      "Epoch [1490/10000], Training Loss: 5.1344, Validation Loss: 1.0215\n",
      "Epoch [1495/10000], Training Loss: 5.3294, Validation Loss: 1.1015\n",
      "Epoch [1500/10000], Training Loss: 5.2381, Validation Loss: 1.4908\n",
      "Epoch [1505/10000], Training Loss: 5.0466, Validation Loss: 1.6266\n",
      "Epoch [1510/10000], Training Loss: 5.5343, Validation Loss: 2.2500\n",
      "Epoch [1515/10000], Training Loss: 5.2824, Validation Loss: 2.2418\n",
      "Epoch [1520/10000], Training Loss: 5.5616, Validation Loss: 1.9574\n",
      "Epoch [1525/10000], Training Loss: 4.9004, Validation Loss: 1.3649\n",
      "Epoch [1530/10000], Training Loss: 5.5923, Validation Loss: 1.9811\n",
      "Epoch [1535/10000], Training Loss: 5.2370, Validation Loss: 1.6687\n",
      "Epoch [1540/10000], Training Loss: 4.9970, Validation Loss: 2.2088\n",
      "Epoch [1545/10000], Training Loss: 5.5502, Validation Loss: 1.6895\n",
      "Epoch [1550/10000], Training Loss: 4.4777, Validation Loss: 3.0897\n",
      "Epoch [1555/10000], Training Loss: 4.6382, Validation Loss: 3.3343\n",
      "Epoch [1560/10000], Training Loss: 4.1317, Validation Loss: 4.8778\n",
      "Epoch [1565/10000], Training Loss: 5.0668, Validation Loss: 8.0037\n",
      "Epoch [1570/10000], Training Loss: 4.8797, Validation Loss: 7.7730\n",
      "Epoch [1575/10000], Training Loss: 4.8083, Validation Loss: 2.0448\n",
      "Epoch [1580/10000], Training Loss: 4.4240, Validation Loss: 3.9366\n",
      "Epoch [1585/10000], Training Loss: 4.3694, Validation Loss: 4.7862\n",
      "Epoch [1590/10000], Training Loss: 4.4392, Validation Loss: 2.8780\n",
      "Epoch [1595/10000], Training Loss: 4.9002, Validation Loss: 2.8263\n",
      "Epoch [1600/10000], Training Loss: 4.3993, Validation Loss: 5.9429\n",
      "Epoch [1605/10000], Training Loss: 4.8217, Validation Loss: 2.3995\n",
      "Epoch [1610/10000], Training Loss: 5.2086, Validation Loss: 2.6496\n",
      "Epoch [1615/10000], Training Loss: 4.4586, Validation Loss: 2.6656\n",
      "Epoch [1620/10000], Training Loss: 4.7500, Validation Loss: 3.4251\n",
      "Epoch [1625/10000], Training Loss: 4.1782, Validation Loss: 6.1195\n",
      "Epoch [1630/10000], Training Loss: 4.1709, Validation Loss: 1.8805\n",
      "Epoch [1635/10000], Training Loss: 4.3364, Validation Loss: 3.7394\n",
      "Epoch [1640/10000], Training Loss: 4.4553, Validation Loss: 2.9411\n",
      "Epoch [1645/10000], Training Loss: 4.5395, Validation Loss: 3.5385\n",
      "Epoch [1650/10000], Training Loss: 3.9560, Validation Loss: 5.4084\n",
      "Epoch [1655/10000], Training Loss: 4.5425, Validation Loss: 4.7814\n",
      "Epoch [1660/10000], Training Loss: 3.9559, Validation Loss: 5.0267\n",
      "Epoch [1665/10000], Training Loss: 4.3410, Validation Loss: 3.7034\n",
      "Epoch [1670/10000], Training Loss: 4.3290, Validation Loss: 3.1980\n",
      "Epoch [1675/10000], Training Loss: 3.5279, Validation Loss: 5.6766\n",
      "Epoch [1680/10000], Training Loss: 4.5770, Validation Loss: 3.5483\n",
      "Epoch [1685/10000], Training Loss: 4.4971, Validation Loss: 3.9930\n",
      "Epoch [1690/10000], Training Loss: 3.7683, Validation Loss: 3.8382\n",
      "Epoch [1695/10000], Training Loss: 4.0571, Validation Loss: 4.4932\n",
      "Epoch [1700/10000], Training Loss: 3.6998, Validation Loss: 3.2501\n",
      "Epoch [1705/10000], Training Loss: 4.3771, Validation Loss: 3.9226\n",
      "Epoch [1710/10000], Training Loss: 3.8847, Validation Loss: 2.9930\n",
      "Epoch [1715/10000], Training Loss: 3.7941, Validation Loss: 4.3796\n",
      "Epoch [1720/10000], Training Loss: 3.4512, Validation Loss: 6.3408\n",
      "Epoch [1725/10000], Training Loss: 3.6178, Validation Loss: 3.8981\n",
      "Epoch [1730/10000], Training Loss: 3.7601, Validation Loss: 7.1876\n",
      "Epoch [1735/10000], Training Loss: 3.8297, Validation Loss: 5.6093\n",
      "Epoch [1740/10000], Training Loss: 3.3121, Validation Loss: 5.0523\n",
      "Epoch [1745/10000], Training Loss: 3.9971, Validation Loss: 9.3854\n",
      "Epoch [1750/10000], Training Loss: 3.6293, Validation Loss: 10.1471\n",
      "Epoch [1755/10000], Training Loss: 3.7416, Validation Loss: 9.3981\n",
      "Epoch [1760/10000], Training Loss: 3.7737, Validation Loss: 18.0917\n",
      "Epoch [1765/10000], Training Loss: 3.3936, Validation Loss: 11.6297\n",
      "Epoch [1770/10000], Training Loss: 3.5616, Validation Loss: 10.7286\n",
      "Epoch [1775/10000], Training Loss: 3.5268, Validation Loss: 6.3967\n",
      "Epoch [1780/10000], Training Loss: 3.8451, Validation Loss: 7.3375\n",
      "Epoch [1785/10000], Training Loss: 3.6404, Validation Loss: 12.5407\n",
      "Epoch [1790/10000], Training Loss: 3.1488, Validation Loss: 16.4803\n",
      "Epoch [1795/10000], Training Loss: 3.8694, Validation Loss: 18.0303\n",
      "Epoch [1800/10000], Training Loss: 2.9945, Validation Loss: 19.2130\n",
      "Epoch [1805/10000], Training Loss: 3.6837, Validation Loss: 4.2497\n",
      "Epoch [1810/10000], Training Loss: 3.9583, Validation Loss: 7.6701\n",
      "Epoch [1815/10000], Training Loss: 3.3235, Validation Loss: 4.3236\n",
      "Epoch [1820/10000], Training Loss: 3.2049, Validation Loss: 4.1495\n",
      "Epoch [1825/10000], Training Loss: 3.6894, Validation Loss: 6.5736\n",
      "Epoch [1830/10000], Training Loss: 3.4837, Validation Loss: 7.0291\n",
      "Epoch [1835/10000], Training Loss: 3.1477, Validation Loss: 8.4829\n",
      "Epoch [1840/10000], Training Loss: 3.1910, Validation Loss: 8.0083\n",
      "Epoch [1845/10000], Training Loss: 3.2291, Validation Loss: 13.0612\n",
      "Epoch [1850/10000], Training Loss: 3.4874, Validation Loss: 16.0006\n",
      "Epoch [1855/10000], Training Loss: 3.3877, Validation Loss: 13.3147\n",
      "Epoch [1860/10000], Training Loss: 3.1324, Validation Loss: 14.2669\n",
      "Epoch [1865/10000], Training Loss: 3.1820, Validation Loss: 17.6597\n",
      "Epoch [1870/10000], Training Loss: 3.6813, Validation Loss: 6.6478\n",
      "Epoch [1875/10000], Training Loss: 3.8578, Validation Loss: 2.1164\n",
      "Epoch [1880/10000], Training Loss: 3.1605, Validation Loss: 3.1941\n",
      "Epoch [1885/10000], Training Loss: 3.4537, Validation Loss: 4.1189\n",
      "Epoch [1890/10000], Training Loss: 3.3420, Validation Loss: 3.7879\n",
      "Epoch [1895/10000], Training Loss: 3.0020, Validation Loss: 5.5321\n",
      "Epoch [1900/10000], Training Loss: 3.1149, Validation Loss: 5.2672\n",
      "Epoch [1905/10000], Training Loss: 3.1975, Validation Loss: 6.0603\n",
      "Epoch [1910/10000], Training Loss: 2.8106, Validation Loss: 7.0559\n",
      "Epoch [1915/10000], Training Loss: 3.4073, Validation Loss: 7.5827\n",
      "Epoch [1920/10000], Training Loss: 3.2098, Validation Loss: 7.9810\n",
      "Epoch [1925/10000], Training Loss: 2.9315, Validation Loss: 4.7358\n",
      "Epoch [1930/10000], Training Loss: 3.1456, Validation Loss: 3.1045\n",
      "Epoch [1935/10000], Training Loss: 3.2114, Validation Loss: 4.2617\n",
      "Epoch [1940/10000], Training Loss: 3.5188, Validation Loss: 3.6909\n",
      "Epoch [1945/10000], Training Loss: 3.5100, Validation Loss: 3.6187\n",
      "Epoch [1950/10000], Training Loss: 3.3309, Validation Loss: 5.6849\n",
      "Epoch [1955/10000], Training Loss: 3.0696, Validation Loss: 5.8610\n",
      "Epoch [1960/10000], Training Loss: 3.2806, Validation Loss: 7.2523\n",
      "Epoch [1965/10000], Training Loss: 3.2752, Validation Loss: 5.4047\n",
      "Epoch [1970/10000], Training Loss: 3.0209, Validation Loss: 2.9465\n",
      "Epoch [1975/10000], Training Loss: 3.2270, Validation Loss: 4.5137\n",
      "Epoch [1980/10000], Training Loss: 3.4998, Validation Loss: 2.7315\n",
      "Epoch [1985/10000], Training Loss: 3.1649, Validation Loss: 3.6317\n",
      "Epoch [1990/10000], Training Loss: 3.2572, Validation Loss: 1.8133\n",
      "Epoch [1995/10000], Training Loss: 2.8269, Validation Loss: 2.4342\n",
      "Epoch [2000/10000], Training Loss: 3.0208, Validation Loss: 3.7281\n",
      "Epoch [2005/10000], Training Loss: 3.3198, Validation Loss: 3.6344\n",
      "Epoch [2010/10000], Training Loss: 3.2170, Validation Loss: 4.2025\n",
      "Epoch [2015/10000], Training Loss: 3.1744, Validation Loss: 3.1782\n",
      "Epoch [2020/10000], Training Loss: 3.0368, Validation Loss: 2.6971\n",
      "Epoch [2025/10000], Training Loss: 3.1941, Validation Loss: 1.6301\n",
      "Epoch [2030/10000], Training Loss: 3.3789, Validation Loss: 1.1461\n",
      "Epoch [2035/10000], Training Loss: 2.6214, Validation Loss: 1.3104\n",
      "Epoch [2040/10000], Training Loss: 3.2338, Validation Loss: 1.2027\n",
      "Epoch [2045/10000], Training Loss: 3.4922, Validation Loss: 1.5157\n",
      "Epoch [2050/10000], Training Loss: 3.4651, Validation Loss: 3.1231\n",
      "Epoch [2055/10000], Training Loss: 3.3356, Validation Loss: 2.5801\n",
      "Epoch [2060/10000], Training Loss: 3.2224, Validation Loss: 2.6579\n",
      "Epoch [2065/10000], Training Loss: 2.8583, Validation Loss: 2.0043\n",
      "Epoch [2070/10000], Training Loss: 3.0046, Validation Loss: 2.9303\n",
      "Epoch [2075/10000], Training Loss: 3.0341, Validation Loss: 2.8974\n",
      "Epoch [2080/10000], Training Loss: 2.8431, Validation Loss: 1.8802\n",
      "Epoch [2085/10000], Training Loss: 2.9332, Validation Loss: 2.6960\n",
      "Epoch [2090/10000], Training Loss: 3.3976, Validation Loss: 4.3162\n",
      "Epoch [2095/10000], Training Loss: 3.0448, Validation Loss: 5.0216\n",
      "Epoch [2100/10000], Training Loss: 3.1850, Validation Loss: 6.0840\n",
      "Epoch [2105/10000], Training Loss: 2.4893, Validation Loss: 3.0999\n",
      "Epoch [2110/10000], Training Loss: 2.8769, Validation Loss: 1.9174\n",
      "Epoch [2115/10000], Training Loss: 2.5792, Validation Loss: 1.7496\n",
      "Epoch [2120/10000], Training Loss: 3.1016, Validation Loss: 1.8816\n",
      "Epoch [2125/10000], Training Loss: 3.0178, Validation Loss: 1.4429\n",
      "Epoch [2130/10000], Training Loss: 2.9134, Validation Loss: 1.2824\n",
      "Epoch [2135/10000], Training Loss: 2.7986, Validation Loss: 2.8073\n",
      "Epoch [2140/10000], Training Loss: 2.6110, Validation Loss: 3.8394\n",
      "Epoch [2145/10000], Training Loss: 2.5444, Validation Loss: 4.3156\n",
      "Epoch [2150/10000], Training Loss: 3.0246, Validation Loss: 4.1273\n",
      "Epoch [2155/10000], Training Loss: 2.8203, Validation Loss: 3.6992\n",
      "Epoch [2160/10000], Training Loss: 2.9954, Validation Loss: 3.7485\n",
      "Epoch [2165/10000], Training Loss: 2.7739, Validation Loss: 2.0829\n",
      "Epoch [2170/10000], Training Loss: 2.6799, Validation Loss: 2.8114\n",
      "Epoch [2175/10000], Training Loss: 2.4830, Validation Loss: 3.5221\n",
      "Epoch [2180/10000], Training Loss: 2.6520, Validation Loss: 3.8128\n",
      "Epoch [2185/10000], Training Loss: 3.0249, Validation Loss: 2.7807\n",
      "Epoch [2190/10000], Training Loss: 2.6897, Validation Loss: 2.0520\n",
      "Epoch [2195/10000], Training Loss: 2.6211, Validation Loss: 1.1644\n",
      "Epoch [2200/10000], Training Loss: 2.6624, Validation Loss: 1.9791\n",
      "Epoch [2205/10000], Training Loss: 3.1294, Validation Loss: 1.4375\n",
      "Epoch [2210/10000], Training Loss: 2.3733, Validation Loss: 2.3588\n",
      "Epoch [2215/10000], Training Loss: 2.8123, Validation Loss: 1.9184\n",
      "Epoch [2220/10000], Training Loss: 2.7492, Validation Loss: 2.3166\n",
      "Epoch [2225/10000], Training Loss: 2.7343, Validation Loss: 2.9699\n",
      "Epoch [2230/10000], Training Loss: 2.6905, Validation Loss: 1.7050\n",
      "Epoch [2235/10000], Training Loss: 2.8656, Validation Loss: 4.9433\n",
      "Epoch [2240/10000], Training Loss: 3.1905, Validation Loss: 4.0357\n",
      "Epoch [2245/10000], Training Loss: 2.7971, Validation Loss: 5.5375\n",
      "Epoch [2250/10000], Training Loss: 2.5167, Validation Loss: 8.7318\n",
      "Epoch [2255/10000], Training Loss: 2.4768, Validation Loss: 5.0563\n",
      "Epoch [2260/10000], Training Loss: 3.0280, Validation Loss: 8.2419\n",
      "Epoch [2265/10000], Training Loss: 2.7950, Validation Loss: 12.7755\n",
      "Epoch [2270/10000], Training Loss: 2.5127, Validation Loss: 13.7893\n",
      "Epoch [2275/10000], Training Loss: 2.7315, Validation Loss: 16.6895\n",
      "Epoch [2280/10000], Training Loss: 2.5961, Validation Loss: 9.8482\n",
      "Epoch [2285/10000], Training Loss: 2.8153, Validation Loss: 8.5681\n",
      "Epoch [2290/10000], Training Loss: 2.6743, Validation Loss: 10.6901\n",
      "Epoch [2295/10000], Training Loss: 2.4695, Validation Loss: 4.3577\n",
      "Epoch [2300/10000], Training Loss: 2.6188, Validation Loss: 2.2044\n",
      "Epoch [2305/10000], Training Loss: 2.6283, Validation Loss: 1.8394\n",
      "Epoch [2310/10000], Training Loss: 2.8427, Validation Loss: 7.0749\n",
      "Epoch [2315/10000], Training Loss: 3.0927, Validation Loss: 3.4624\n",
      "Epoch [2320/10000], Training Loss: 2.8113, Validation Loss: 1.8964\n",
      "Epoch [2325/10000], Training Loss: 2.3547, Validation Loss: 2.0038\n",
      "Epoch [2330/10000], Training Loss: 2.7053, Validation Loss: 1.6592\n",
      "Epoch [2335/10000], Training Loss: 2.4423, Validation Loss: 0.9842\n",
      "Epoch [2340/10000], Training Loss: 2.4720, Validation Loss: 0.9190\n",
      "Epoch [2345/10000], Training Loss: 2.6000, Validation Loss: 1.4184\n",
      "Epoch [2350/10000], Training Loss: 2.7446, Validation Loss: 1.1300\n",
      "Epoch [2355/10000], Training Loss: 2.6595, Validation Loss: 0.9372\n",
      "Epoch [2360/10000], Training Loss: 2.6227, Validation Loss: 1.6983\n",
      "Epoch [2365/10000], Training Loss: 2.3017, Validation Loss: 2.1634\n",
      "Epoch [2370/10000], Training Loss: 2.8614, Validation Loss: 2.3717\n",
      "Epoch [2375/10000], Training Loss: 2.1862, Validation Loss: 2.0390\n",
      "Epoch [2380/10000], Training Loss: 2.5518, Validation Loss: 1.6929\n",
      "Epoch [2385/10000], Training Loss: 2.5665, Validation Loss: 1.5452\n",
      "Epoch [2390/10000], Training Loss: 2.5313, Validation Loss: 1.4283\n",
      "Epoch [2395/10000], Training Loss: 2.5292, Validation Loss: 1.5769\n",
      "Epoch [2400/10000], Training Loss: 2.9695, Validation Loss: 2.4365\n",
      "Epoch [2405/10000], Training Loss: 2.5324, Validation Loss: 4.0338\n",
      "Epoch [2410/10000], Training Loss: 2.6883, Validation Loss: 3.5835\n",
      "Epoch [2415/10000], Training Loss: 2.6284, Validation Loss: 4.3862\n",
      "Epoch [2420/10000], Training Loss: 2.7015, Validation Loss: 2.0230\n",
      "Epoch [2425/10000], Training Loss: 2.6388, Validation Loss: 2.6328\n",
      "Epoch [2430/10000], Training Loss: 2.5946, Validation Loss: 4.6352\n",
      "Epoch [2435/10000], Training Loss: 2.5396, Validation Loss: 2.4933\n",
      "Epoch [2440/10000], Training Loss: 2.6947, Validation Loss: 4.0277\n",
      "Epoch [2445/10000], Training Loss: 2.5380, Validation Loss: 2.1420\n",
      "Epoch [2450/10000], Training Loss: 2.4947, Validation Loss: 1.6655\n",
      "Epoch [2455/10000], Training Loss: 2.4237, Validation Loss: 2.0601\n",
      "Epoch [2460/10000], Training Loss: 2.5712, Validation Loss: 1.9732\n",
      "Epoch [2465/10000], Training Loss: 2.5169, Validation Loss: 4.4202\n",
      "Epoch [2470/10000], Training Loss: 2.3414, Validation Loss: 3.0079\n",
      "Epoch [2475/10000], Training Loss: 2.5601, Validation Loss: 2.8165\n",
      "Epoch [2480/10000], Training Loss: 2.8056, Validation Loss: 2.7114\n",
      "Epoch [2485/10000], Training Loss: 2.5790, Validation Loss: 4.0091\n",
      "Epoch [2490/10000], Training Loss: 2.5820, Validation Loss: 2.5644\n",
      "Epoch [2495/10000], Training Loss: 2.4052, Validation Loss: 2.6561\n",
      "Epoch [2500/10000], Training Loss: 2.2569, Validation Loss: 2.2993\n",
      "Epoch [2505/10000], Training Loss: 2.3688, Validation Loss: 1.3744\n",
      "Epoch [2510/10000], Training Loss: 2.3803, Validation Loss: 1.0306\n",
      "Epoch [2515/10000], Training Loss: 2.3066, Validation Loss: 2.2678\n",
      "Epoch [2520/10000], Training Loss: 2.2250, Validation Loss: 2.7980\n",
      "Epoch [2525/10000], Training Loss: 2.2337, Validation Loss: 3.8849\n",
      "Epoch [2530/10000], Training Loss: 2.5923, Validation Loss: 2.6997\n",
      "Epoch [2535/10000], Training Loss: 2.5729, Validation Loss: 4.8809\n",
      "Epoch [2540/10000], Training Loss: 2.5123, Validation Loss: 2.8795\n",
      "Epoch [2545/10000], Training Loss: 2.4506, Validation Loss: 3.0351\n",
      "Epoch [2550/10000], Training Loss: 2.3133, Validation Loss: 3.1653\n",
      "Epoch [2555/10000], Training Loss: 2.3024, Validation Loss: 2.0403\n",
      "Epoch [2560/10000], Training Loss: 2.3720, Validation Loss: 1.6727\n",
      "Epoch [2565/10000], Training Loss: 2.2215, Validation Loss: 1.6389\n",
      "Epoch [2570/10000], Training Loss: 2.3936, Validation Loss: 1.3686\n",
      "Epoch [2575/10000], Training Loss: 2.5280, Validation Loss: 1.8662\n",
      "Epoch [2580/10000], Training Loss: 2.4061, Validation Loss: 3.8668\n",
      "Epoch [2585/10000], Training Loss: 2.3495, Validation Loss: 3.7588\n",
      "Epoch [2590/10000], Training Loss: 2.7364, Validation Loss: 4.4593\n",
      "Epoch [2595/10000], Training Loss: 2.3693, Validation Loss: 8.1207\n",
      "Epoch [2600/10000], Training Loss: 2.4471, Validation Loss: 3.0068\n",
      "Epoch [2605/10000], Training Loss: 2.3887, Validation Loss: 1.3310\n",
      "Epoch [2610/10000], Training Loss: 2.1733, Validation Loss: 3.1238\n",
      "Epoch [2615/10000], Training Loss: 2.2493, Validation Loss: 2.1725\n",
      "Epoch [2620/10000], Training Loss: 2.3888, Validation Loss: 2.1400\n",
      "Epoch [2625/10000], Training Loss: 2.3025, Validation Loss: 5.4914\n",
      "Epoch [2630/10000], Training Loss: 2.8634, Validation Loss: 10.7339\n",
      "Epoch [2635/10000], Training Loss: 2.2688, Validation Loss: 14.2171\n",
      "Epoch [2640/10000], Training Loss: 2.3684, Validation Loss: 13.7558\n",
      "Epoch [2645/10000], Training Loss: 2.3576, Validation Loss: 14.7615\n",
      "Epoch [2650/10000], Training Loss: 2.2908, Validation Loss: 10.7547\n",
      "Epoch [2655/10000], Training Loss: 2.5405, Validation Loss: 12.3384\n",
      "Epoch [2660/10000], Training Loss: 2.3882, Validation Loss: 8.5559\n",
      "Epoch [2665/10000], Training Loss: 2.2412, Validation Loss: 15.1095\n",
      "Epoch [2670/10000], Training Loss: 2.0700, Validation Loss: 16.0420\n",
      "Epoch [2675/10000], Training Loss: 2.3424, Validation Loss: 28.2005\n",
      "Epoch [2680/10000], Training Loss: 2.7326, Validation Loss: 31.8165\n",
      "Epoch [2685/10000], Training Loss: 2.4306, Validation Loss: 37.2959\n",
      "Epoch [2690/10000], Training Loss: 2.3720, Validation Loss: 34.0876\n",
      "Epoch [2695/10000], Training Loss: 2.6122, Validation Loss: 29.1898\n",
      "Epoch [2700/10000], Training Loss: 2.3024, Validation Loss: 23.0060\n",
      "Epoch [2705/10000], Training Loss: 2.4530, Validation Loss: 17.8432\n",
      "Epoch [2710/10000], Training Loss: 2.5322, Validation Loss: 17.1886\n",
      "Epoch [2715/10000], Training Loss: 2.4380, Validation Loss: 10.8175\n",
      "Epoch [2720/10000], Training Loss: 2.2613, Validation Loss: 10.2503\n",
      "Epoch [2725/10000], Training Loss: 2.5518, Validation Loss: 9.8994\n",
      "Epoch [2730/10000], Training Loss: 2.4830, Validation Loss: 11.8094\n",
      "Epoch [2735/10000], Training Loss: 2.3091, Validation Loss: 22.3534\n",
      "Epoch [2740/10000], Training Loss: 2.3809, Validation Loss: 24.3101\n",
      "Epoch [2745/10000], Training Loss: 2.2713, Validation Loss: 22.4628\n",
      "Epoch [2750/10000], Training Loss: 2.1075, Validation Loss: 24.9768\n",
      "Epoch [2755/10000], Training Loss: 2.3064, Validation Loss: 18.7341\n",
      "Epoch [2760/10000], Training Loss: 2.4734, Validation Loss: 21.3904\n",
      "Epoch [2765/10000], Training Loss: 2.4981, Validation Loss: 15.5000\n",
      "Epoch [2770/10000], Training Loss: 2.5435, Validation Loss: 17.8406\n",
      "Epoch [2775/10000], Training Loss: 2.2307, Validation Loss: 24.5445\n",
      "Epoch [2780/10000], Training Loss: 2.0876, Validation Loss: 31.5291\n",
      "Epoch [2785/10000], Training Loss: 2.3938, Validation Loss: 33.2836\n",
      "Epoch [2790/10000], Training Loss: 2.0884, Validation Loss: 29.3769\n",
      "Epoch [2795/10000], Training Loss: 2.3083, Validation Loss: 28.5235\n",
      "Epoch [2800/10000], Training Loss: 2.1744, Validation Loss: 24.3262\n",
      "Epoch [2805/10000], Training Loss: 2.3679, Validation Loss: 18.2216\n",
      "Epoch [2810/10000], Training Loss: 2.4270, Validation Loss: 22.5955\n",
      "Epoch [2815/10000], Training Loss: 2.0665, Validation Loss: 9.2358\n",
      "Epoch [2820/10000], Training Loss: 2.4115, Validation Loss: 20.0361\n",
      "Epoch [2825/10000], Training Loss: 2.5210, Validation Loss: 14.7501\n",
      "Epoch [2830/10000], Training Loss: 2.4318, Validation Loss: 10.1066\n",
      "Epoch [2835/10000], Training Loss: 2.0615, Validation Loss: 12.4707\n",
      "Epoch [2840/10000], Training Loss: 2.3359, Validation Loss: 12.3284\n",
      "Epoch [2845/10000], Training Loss: 2.1182, Validation Loss: 6.8270\n",
      "Epoch [2850/10000], Training Loss: 2.0985, Validation Loss: 7.8676\n",
      "Epoch [2855/10000], Training Loss: 2.6731, Validation Loss: 10.9626\n",
      "Epoch [2860/10000], Training Loss: 2.4218, Validation Loss: 10.7984\n",
      "Epoch [2865/10000], Training Loss: 2.3273, Validation Loss: 10.6356\n",
      "Epoch [2870/10000], Training Loss: 2.2280, Validation Loss: 8.2928\n",
      "Epoch [2875/10000], Training Loss: 2.2201, Validation Loss: 9.4472\n",
      "Epoch [2880/10000], Training Loss: 2.4299, Validation Loss: 15.4756\n",
      "Epoch [2885/10000], Training Loss: 2.7788, Validation Loss: 14.0906\n",
      "Epoch [2890/10000], Training Loss: 2.4837, Validation Loss: 13.8606\n",
      "Epoch [2895/10000], Training Loss: 1.9716, Validation Loss: 13.4156\n",
      "Epoch [2900/10000], Training Loss: 2.3373, Validation Loss: 22.1863\n",
      "Epoch [2905/10000], Training Loss: 2.5899, Validation Loss: 19.1007\n",
      "Epoch [2910/10000], Training Loss: 2.7376, Validation Loss: 16.8539\n",
      "Epoch [2915/10000], Training Loss: 2.2134, Validation Loss: 16.5005\n",
      "Epoch [2920/10000], Training Loss: 2.0823, Validation Loss: 13.2827\n",
      "Epoch [2925/10000], Training Loss: 2.3595, Validation Loss: 16.1152\n",
      "Epoch [2930/10000], Training Loss: 2.3567, Validation Loss: 23.9224\n",
      "Epoch [2935/10000], Training Loss: 2.1282, Validation Loss: 22.5891\n",
      "Epoch [2940/10000], Training Loss: 2.5261, Validation Loss: 25.7849\n",
      "Epoch [2945/10000], Training Loss: 2.3939, Validation Loss: 33.2757\n",
      "Epoch [2950/10000], Training Loss: 2.1720, Validation Loss: 39.0803\n",
      "Epoch [2955/10000], Training Loss: 2.2381, Validation Loss: 43.7686\n",
      "Epoch [2960/10000], Training Loss: 2.8738, Validation Loss: 29.9431\n",
      "Epoch [2965/10000], Training Loss: 2.3354, Validation Loss: 25.9399\n",
      "Epoch [2970/10000], Training Loss: 2.1993, Validation Loss: 24.6959\n",
      "Epoch [2975/10000], Training Loss: 2.7986, Validation Loss: 25.1449\n",
      "Epoch [2980/10000], Training Loss: 2.1198, Validation Loss: 26.6374\n",
      "Epoch [2985/10000], Training Loss: 2.2167, Validation Loss: 26.9139\n",
      "Epoch [2990/10000], Training Loss: 2.0546, Validation Loss: 29.4123\n",
      "Epoch [2995/10000], Training Loss: 2.4730, Validation Loss: 24.2020\n",
      "Epoch [3000/10000], Training Loss: 2.2159, Validation Loss: 25.3331\n",
      "Epoch [3005/10000], Training Loss: 2.5885, Validation Loss: 36.7487\n",
      "Epoch [3010/10000], Training Loss: 2.3747, Validation Loss: 46.4507\n",
      "Epoch [3015/10000], Training Loss: 2.5598, Validation Loss: 43.9626\n",
      "Epoch [3020/10000], Training Loss: 2.2119, Validation Loss: 38.5372\n",
      "Epoch [3025/10000], Training Loss: 2.4053, Validation Loss: 37.2967\n",
      "Epoch [3030/10000], Training Loss: 2.2154, Validation Loss: 29.0113\n",
      "Epoch [3035/10000], Training Loss: 2.4841, Validation Loss: 27.0602\n",
      "Epoch [3040/10000], Training Loss: 2.4845, Validation Loss: 27.7035\n",
      "Epoch [3045/10000], Training Loss: 2.1991, Validation Loss: 17.2131\n",
      "Epoch [3050/10000], Training Loss: 2.1835, Validation Loss: 12.3867\n",
      "Epoch [3055/10000], Training Loss: 2.3163, Validation Loss: 7.8431\n",
      "Epoch [3060/10000], Training Loss: 2.5052, Validation Loss: 11.3692\n",
      "Epoch [3065/10000], Training Loss: 2.4040, Validation Loss: 12.6885\n",
      "Epoch [3070/10000], Training Loss: 2.6154, Validation Loss: 15.5209\n",
      "Epoch [3075/10000], Training Loss: 1.9372, Validation Loss: 11.6895\n",
      "Epoch [3080/10000], Training Loss: 2.0721, Validation Loss: 15.7861\n",
      "Epoch [3085/10000], Training Loss: 2.1906, Validation Loss: 8.4139\n",
      "Epoch [3090/10000], Training Loss: 1.9339, Validation Loss: 13.4169\n",
      "Epoch [3095/10000], Training Loss: 2.0971, Validation Loss: 23.3891\n",
      "Epoch [3100/10000], Training Loss: 2.2124, Validation Loss: 27.0312\n",
      "Epoch [3105/10000], Training Loss: 2.0804, Validation Loss: 23.8395\n",
      "Epoch [3110/10000], Training Loss: 2.6430, Validation Loss: 24.1215\n",
      "Epoch [3115/10000], Training Loss: 2.3168, Validation Loss: 19.8928\n",
      "Epoch [3120/10000], Training Loss: 2.2512, Validation Loss: 17.1206\n",
      "Epoch [3125/10000], Training Loss: 1.8970, Validation Loss: 29.7617\n",
      "Epoch [3130/10000], Training Loss: 2.4521, Validation Loss: 32.6332\n",
      "Epoch [3135/10000], Training Loss: 2.4596, Validation Loss: 24.9319\n",
      "Epoch [3140/10000], Training Loss: 2.0873, Validation Loss: 25.9308\n",
      "Epoch [3145/10000], Training Loss: 2.5449, Validation Loss: 27.0896\n",
      "Epoch [3150/10000], Training Loss: 2.3526, Validation Loss: 20.2919\n",
      "Epoch [3155/10000], Training Loss: 2.3088, Validation Loss: 25.6514\n",
      "Epoch [3160/10000], Training Loss: 2.3714, Validation Loss: 27.9879\n",
      "Epoch [3165/10000], Training Loss: 2.0923, Validation Loss: 43.5555\n",
      "Epoch [3170/10000], Training Loss: 2.3477, Validation Loss: 40.8419\n",
      "Epoch [3175/10000], Training Loss: 2.0272, Validation Loss: 31.0703\n",
      "Epoch [3180/10000], Training Loss: 2.4131, Validation Loss: 27.8484\n",
      "Epoch [3185/10000], Training Loss: 2.2283, Validation Loss: 23.9430\n",
      "Epoch [3190/10000], Training Loss: 2.1921, Validation Loss: 35.6344\n",
      "Epoch [3195/10000], Training Loss: 2.5059, Validation Loss: 41.6105\n",
      "Epoch [3200/10000], Training Loss: 2.0928, Validation Loss: 58.7482\n",
      "Epoch [3205/10000], Training Loss: 2.3672, Validation Loss: 74.8222\n",
      "Epoch [3210/10000], Training Loss: 2.4178, Validation Loss: 84.1982\n",
      "Epoch [3215/10000], Training Loss: 2.7214, Validation Loss: 114.0870\n",
      "Epoch [3220/10000], Training Loss: 2.1963, Validation Loss: 138.6939\n",
      "Epoch [3225/10000], Training Loss: 2.4821, Validation Loss: 174.2444\n",
      "Epoch [3230/10000], Training Loss: 2.2640, Validation Loss: 214.4621\n",
      "Epoch [3235/10000], Training Loss: 2.3903, Validation Loss: 236.2271\n",
      "Epoch [3240/10000], Training Loss: 2.5862, Validation Loss: 242.9431\n",
      "Epoch [3245/10000], Training Loss: 2.7986, Validation Loss: 256.5839\n",
      "Epoch [3250/10000], Training Loss: 2.3233, Validation Loss: 215.6082\n",
      "Epoch [3255/10000], Training Loss: 2.1179, Validation Loss: 193.6637\n",
      "Epoch [3260/10000], Training Loss: 2.6382, Validation Loss: 204.4394\n",
      "Epoch [3265/10000], Training Loss: 2.1933, Validation Loss: 188.4352\n",
      "Epoch [3270/10000], Training Loss: 2.4405, Validation Loss: 218.0294\n",
      "Epoch [3275/10000], Training Loss: 2.1812, Validation Loss: 213.1030\n",
      "Epoch [3280/10000], Training Loss: 2.2637, Validation Loss: 217.2179\n",
      "Epoch [3285/10000], Training Loss: 2.2741, Validation Loss: 199.9999\n",
      "Epoch [3290/10000], Training Loss: 2.2688, Validation Loss: 215.3019\n",
      "Epoch [3295/10000], Training Loss: 2.3848, Validation Loss: 203.5669\n",
      "Epoch [3300/10000], Training Loss: 2.2137, Validation Loss: 214.9866\n",
      "Epoch [3305/10000], Training Loss: 2.3691, Validation Loss: 218.3520\n",
      "Epoch [3310/10000], Training Loss: 2.4053, Validation Loss: 204.4543\n",
      "Epoch [3315/10000], Training Loss: 2.2796, Validation Loss: 251.8788\n",
      "Epoch [3320/10000], Training Loss: 2.2843, Validation Loss: 244.0510\n",
      "Epoch [3325/10000], Training Loss: 2.4124, Validation Loss: 289.8611\n",
      "Epoch [3330/10000], Training Loss: 2.3296, Validation Loss: 272.9428\n",
      "Epoch [3335/10000], Training Loss: 2.3490, Validation Loss: 265.1642\n",
      "Epoch [3340/10000], Training Loss: 2.2825, Validation Loss: 212.0657\n",
      "Epoch [3345/10000], Training Loss: 2.3771, Validation Loss: 204.5431\n",
      "Epoch [3350/10000], Training Loss: 2.6015, Validation Loss: 226.4091\n",
      "Epoch [3355/10000], Training Loss: 2.6286, Validation Loss: 211.6241\n",
      "Epoch [3360/10000], Training Loss: 2.6278, Validation Loss: 142.8026\n",
      "Epoch [3365/10000], Training Loss: 2.3074, Validation Loss: 148.3626\n",
      "Epoch [3370/10000], Training Loss: 2.3072, Validation Loss: 179.1442\n",
      "Epoch [3375/10000], Training Loss: 2.2419, Validation Loss: 221.6891\n",
      "Epoch [3380/10000], Training Loss: 2.5138, Validation Loss: 254.0340\n",
      "Epoch [3385/10000], Training Loss: 2.3107, Validation Loss: 218.8704\n",
      "Epoch [3390/10000], Training Loss: 2.0947, Validation Loss: 153.5193\n",
      "Epoch [3395/10000], Training Loss: 2.1487, Validation Loss: 177.2236\n",
      "Epoch [3400/10000], Training Loss: 2.4168, Validation Loss: 214.4467\n",
      "Epoch [3405/10000], Training Loss: 2.3377, Validation Loss: 266.7787\n",
      "Epoch [3410/10000], Training Loss: 2.7913, Validation Loss: 285.2724\n",
      "Epoch [3415/10000], Training Loss: 2.2669, Validation Loss: 299.2682\n",
      "Epoch [3420/10000], Training Loss: 2.4367, Validation Loss: 308.1985\n",
      "Epoch [3425/10000], Training Loss: 2.5704, Validation Loss: 270.8698\n",
      "Epoch [3430/10000], Training Loss: 2.1932, Validation Loss: 268.6761\n",
      "Epoch [3435/10000], Training Loss: 2.3611, Validation Loss: 273.5707\n",
      "Epoch [3440/10000], Training Loss: 2.2043, Validation Loss: 276.2682\n",
      "Epoch [3445/10000], Training Loss: 2.3596, Validation Loss: 299.1025\n",
      "Epoch [3450/10000], Training Loss: 2.4617, Validation Loss: 274.0145\n",
      "Epoch [3455/10000], Training Loss: 2.0056, Validation Loss: 248.8584\n",
      "Epoch [3460/10000], Training Loss: 2.1256, Validation Loss: 273.0006\n",
      "Epoch [3465/10000], Training Loss: 2.2447, Validation Loss: 335.9359\n",
      "Epoch [3470/10000], Training Loss: 2.0445, Validation Loss: 333.6960\n",
      "Epoch [3475/10000], Training Loss: 2.2521, Validation Loss: 295.0460\n",
      "Epoch [3480/10000], Training Loss: 1.9974, Validation Loss: 247.9164\n",
      "Epoch [3485/10000], Training Loss: 2.4546, Validation Loss: 235.3909\n",
      "Epoch [3490/10000], Training Loss: 2.2645, Validation Loss: 247.7565\n",
      "Epoch [3495/10000], Training Loss: 2.3691, Validation Loss: 239.0642\n",
      "Epoch [3500/10000], Training Loss: 2.0921, Validation Loss: 209.2525\n",
      "Epoch [3505/10000], Training Loss: 2.4035, Validation Loss: 213.5280\n",
      "Epoch [3510/10000], Training Loss: 2.3586, Validation Loss: 238.9148\n",
      "Epoch [3515/10000], Training Loss: 2.1196, Validation Loss: 254.8303\n",
      "Epoch [3520/10000], Training Loss: 2.3937, Validation Loss: 230.4050\n",
      "Epoch [3525/10000], Training Loss: 2.5389, Validation Loss: 202.2372\n",
      "Epoch [3530/10000], Training Loss: 2.4239, Validation Loss: 178.3660\n",
      "Epoch [3535/10000], Training Loss: 2.4034, Validation Loss: 228.3530\n",
      "Epoch [3540/10000], Training Loss: 2.4216, Validation Loss: 281.7516\n",
      "Epoch [3545/10000], Training Loss: 2.7382, Validation Loss: 302.5484\n",
      "Epoch [3550/10000], Training Loss: 2.1067, Validation Loss: 266.7169\n",
      "Epoch [3555/10000], Training Loss: 2.2899, Validation Loss: 287.6931\n",
      "Epoch [3560/10000], Training Loss: 2.2089, Validation Loss: 294.8321\n",
      "Epoch [3565/10000], Training Loss: 2.2862, Validation Loss: 375.3293\n",
      "Epoch [3570/10000], Training Loss: 2.3686, Validation Loss: 357.6796\n",
      "Epoch [3575/10000], Training Loss: 2.5804, Validation Loss: 406.5160\n",
      "Epoch [3580/10000], Training Loss: 2.6595, Validation Loss: 438.4162\n",
      "Epoch [3585/10000], Training Loss: 2.3243, Validation Loss: 337.6157\n",
      "Epoch [3590/10000], Training Loss: 2.1028, Validation Loss: 296.7311\n",
      "Epoch [3595/10000], Training Loss: 2.3821, Validation Loss: 309.7542\n",
      "Epoch [3600/10000], Training Loss: 2.3200, Validation Loss: 283.7762\n",
      "Epoch [3605/10000], Training Loss: 2.2296, Validation Loss: 175.1828\n",
      "Epoch [3610/10000], Training Loss: 2.5246, Validation Loss: 190.8500\n",
      "Epoch [3615/10000], Training Loss: 2.2994, Validation Loss: 182.2688\n",
      "Epoch [3620/10000], Training Loss: 2.3756, Validation Loss: 215.5353\n",
      "Epoch [3625/10000], Training Loss: 2.4614, Validation Loss: 263.3888\n",
      "Epoch [3630/10000], Training Loss: 2.4996, Validation Loss: 345.7869\n",
      "Epoch [3635/10000], Training Loss: 2.2793, Validation Loss: 497.7109\n",
      "Epoch [3640/10000], Training Loss: 2.3685, Validation Loss: 476.0313\n",
      "Epoch [3645/10000], Training Loss: 2.6542, Validation Loss: 453.4723\n",
      "Epoch [3650/10000], Training Loss: 2.6718, Validation Loss: 351.3859\n",
      "Epoch [3655/10000], Training Loss: 2.3108, Validation Loss: 301.9415\n",
      "Epoch [3660/10000], Training Loss: 2.2497, Validation Loss: 330.4925\n",
      "Epoch [3665/10000], Training Loss: 2.2603, Validation Loss: 369.5093\n",
      "Epoch [3670/10000], Training Loss: 2.4467, Validation Loss: 289.6522\n",
      "Epoch [3675/10000], Training Loss: 2.1603, Validation Loss: 200.0576\n",
      "Epoch [3680/10000], Training Loss: 2.1428, Validation Loss: 176.3645\n",
      "Epoch [3685/10000], Training Loss: 2.1707, Validation Loss: 203.5562\n",
      "Epoch [3690/10000], Training Loss: 2.5405, Validation Loss: 192.3332\n",
      "Epoch [3695/10000], Training Loss: 2.4570, Validation Loss: 311.8692\n",
      "Epoch [3700/10000], Training Loss: 2.2058, Validation Loss: 412.7192\n",
      "Epoch [3705/10000], Training Loss: 2.1545, Validation Loss: 328.0244\n",
      "Epoch [3710/10000], Training Loss: 2.3787, Validation Loss: 273.3446\n",
      "Epoch [3715/10000], Training Loss: 2.3838, Validation Loss: 250.3180\n",
      "Epoch [3720/10000], Training Loss: 2.3455, Validation Loss: 227.6930\n",
      "Epoch [3725/10000], Training Loss: 2.2598, Validation Loss: 161.0616\n",
      "Epoch [3730/10000], Training Loss: 2.4329, Validation Loss: 188.7677\n",
      "Epoch [3735/10000], Training Loss: 2.0211, Validation Loss: 211.4394\n",
      "Epoch [3740/10000], Training Loss: 2.2116, Validation Loss: 225.5143\n",
      "Epoch [3745/10000], Training Loss: 2.1628, Validation Loss: 232.1463\n",
      "Epoch [3750/10000], Training Loss: 2.5404, Validation Loss: 187.1445\n",
      "Epoch [3755/10000], Training Loss: 2.0058, Validation Loss: 218.9706\n",
      "Epoch [3760/10000], Training Loss: 2.1564, Validation Loss: 245.2896\n",
      "Epoch [3765/10000], Training Loss: 2.5996, Validation Loss: 246.7717\n",
      "Epoch [3770/10000], Training Loss: 2.5079, Validation Loss: 285.7625\n",
      "Epoch [3775/10000], Training Loss: 2.3557, Validation Loss: 268.0063\n",
      "Epoch [3780/10000], Training Loss: 1.9817, Validation Loss: 206.1910\n",
      "Epoch [3785/10000], Training Loss: 2.3420, Validation Loss: 231.2210\n",
      "Epoch [3790/10000], Training Loss: 2.2518, Validation Loss: 303.1804\n",
      "Epoch [3795/10000], Training Loss: 2.4223, Validation Loss: 318.6691\n",
      "Epoch [3800/10000], Training Loss: 2.3624, Validation Loss: 313.2227\n",
      "Epoch [3805/10000], Training Loss: 2.4093, Validation Loss: 281.1048\n",
      "Epoch [3810/10000], Training Loss: 2.4787, Validation Loss: 263.1727\n",
      "Epoch [3815/10000], Training Loss: 2.3350, Validation Loss: 280.0748\n",
      "Epoch [3820/10000], Training Loss: 2.2245, Validation Loss: 276.7297\n",
      "Epoch [3825/10000], Training Loss: 2.1096, Validation Loss: 251.8465\n",
      "Epoch [3830/10000], Training Loss: 2.4808, Validation Loss: 306.9186\n",
      "Epoch [3835/10000], Training Loss: 2.0943, Validation Loss: 369.6809\n",
      "Epoch [3840/10000], Training Loss: 1.9360, Validation Loss: 430.6638\n",
      "Epoch [3845/10000], Training Loss: 2.0165, Validation Loss: 397.3504\n",
      "Epoch [3850/10000], Training Loss: 2.3876, Validation Loss: 289.7779\n",
      "Epoch [3855/10000], Training Loss: 2.4926, Validation Loss: 261.4591\n",
      "Epoch [3860/10000], Training Loss: 2.2084, Validation Loss: 308.1792\n",
      "Epoch [3865/10000], Training Loss: 2.1238, Validation Loss: 373.6891\n",
      "Epoch [3870/10000], Training Loss: 2.1558, Validation Loss: 461.6722\n",
      "Epoch [3875/10000], Training Loss: 2.3761, Validation Loss: 516.9090\n",
      "Epoch [3880/10000], Training Loss: 2.1765, Validation Loss: 399.0015\n",
      "Epoch [3885/10000], Training Loss: 2.2058, Validation Loss: 368.0702\n",
      "Epoch [3890/10000], Training Loss: 2.0627, Validation Loss: 309.3706\n",
      "Epoch [3895/10000], Training Loss: 2.1389, Validation Loss: 265.6473\n",
      "Epoch [3900/10000], Training Loss: 2.2326, Validation Loss: 337.2508\n",
      "Epoch [3905/10000], Training Loss: 2.1738, Validation Loss: 386.5114\n",
      "Epoch [3910/10000], Training Loss: 2.1442, Validation Loss: 317.1428\n",
      "Epoch [3915/10000], Training Loss: 1.9805, Validation Loss: 303.9931\n",
      "Epoch [3920/10000], Training Loss: 2.2646, Validation Loss: 337.3944\n",
      "Epoch [3925/10000], Training Loss: 1.9700, Validation Loss: 339.4374\n",
      "Epoch [3930/10000], Training Loss: 2.0947, Validation Loss: 311.5776\n",
      "Epoch [3935/10000], Training Loss: 2.4083, Validation Loss: 314.0979\n",
      "Epoch [3940/10000], Training Loss: 1.9886, Validation Loss: 325.9832\n",
      "Epoch [3945/10000], Training Loss: 1.9396, Validation Loss: 330.6108\n",
      "Epoch [3950/10000], Training Loss: 2.2775, Validation Loss: 391.9910\n",
      "Epoch [3955/10000], Training Loss: 2.2404, Validation Loss: 318.7814\n",
      "Epoch [3960/10000], Training Loss: 1.7793, Validation Loss: 289.8340\n",
      "Epoch [3965/10000], Training Loss: 2.2503, Validation Loss: 330.7260\n",
      "Epoch [3970/10000], Training Loss: 2.3253, Validation Loss: 296.1575\n",
      "Epoch [3975/10000], Training Loss: 1.9131, Validation Loss: 349.3411\n",
      "Epoch [3980/10000], Training Loss: 2.2736, Validation Loss: 435.1227\n",
      "Epoch [3985/10000], Training Loss: 2.5725, Validation Loss: 393.4825\n",
      "Epoch [3990/10000], Training Loss: 2.1605, Validation Loss: 274.4505\n",
      "Epoch [3995/10000], Training Loss: 2.2211, Validation Loss: 178.3236\n",
      "Epoch [4000/10000], Training Loss: 2.3488, Validation Loss: 151.4875\n",
      "Epoch [4005/10000], Training Loss: 2.3289, Validation Loss: 214.8938\n",
      "Epoch [4010/10000], Training Loss: 2.2497, Validation Loss: 324.5390\n",
      "Epoch [4015/10000], Training Loss: 2.3163, Validation Loss: 313.8047\n",
      "Epoch [4020/10000], Training Loss: 2.5812, Validation Loss: 399.6740\n",
      "Epoch [4025/10000], Training Loss: 2.3178, Validation Loss: 362.9265\n",
      "Epoch [4030/10000], Training Loss: 2.1202, Validation Loss: 302.4814\n",
      "Epoch [4035/10000], Training Loss: 2.2714, Validation Loss: 284.7866\n",
      "Epoch [4040/10000], Training Loss: 2.7055, Validation Loss: 242.5392\n",
      "Epoch [4045/10000], Training Loss: 2.2765, Validation Loss: 230.7620\n",
      "Epoch [4050/10000], Training Loss: 2.1726, Validation Loss: 304.8553\n",
      "Epoch [4055/10000], Training Loss: 2.3388, Validation Loss: 293.2039\n",
      "Epoch [4060/10000], Training Loss: 2.5917, Validation Loss: 264.2735\n",
      "Epoch [4065/10000], Training Loss: 2.0422, Validation Loss: 367.6035\n",
      "Epoch [4070/10000], Training Loss: 2.1014, Validation Loss: 405.6333\n",
      "Epoch [4075/10000], Training Loss: 2.2120, Validation Loss: 331.2762\n",
      "Epoch [4080/10000], Training Loss: 2.5504, Validation Loss: 233.3311\n",
      "Epoch [4085/10000], Training Loss: 2.3349, Validation Loss: 237.1259\n",
      "Epoch [4090/10000], Training Loss: 2.1005, Validation Loss: 244.0721\n",
      "Epoch [4095/10000], Training Loss: 2.0036, Validation Loss: 279.7063\n",
      "Epoch [4100/10000], Training Loss: 2.1724, Validation Loss: 391.7253\n",
      "Epoch [4105/10000], Training Loss: 2.3759, Validation Loss: 445.5877\n",
      "Epoch [4110/10000], Training Loss: 2.2943, Validation Loss: 512.0634\n",
      "Epoch [4115/10000], Training Loss: 2.1744, Validation Loss: 462.4473\n",
      "Epoch [4120/10000], Training Loss: 2.3959, Validation Loss: 374.5379\n",
      "Epoch [4125/10000], Training Loss: 2.3815, Validation Loss: 349.5020\n",
      "Epoch [4130/10000], Training Loss: 2.2794, Validation Loss: 474.9597\n",
      "Epoch [4135/10000], Training Loss: 2.2661, Validation Loss: 479.1090\n",
      "Epoch [4140/10000], Training Loss: 2.1223, Validation Loss: 376.1596\n",
      "Epoch [4145/10000], Training Loss: 2.2685, Validation Loss: 404.1967\n",
      "Epoch [4150/10000], Training Loss: 2.0942, Validation Loss: 647.2999\n",
      "Epoch [4155/10000], Training Loss: 2.3134, Validation Loss: 709.4200\n",
      "Epoch [4160/10000], Training Loss: 2.3156, Validation Loss: 639.2349\n",
      "Epoch [4165/10000], Training Loss: 1.9505, Validation Loss: 515.8511\n",
      "Epoch [4170/10000], Training Loss: 2.2342, Validation Loss: 453.3159\n",
      "Epoch [4175/10000], Training Loss: 2.3347, Validation Loss: 534.6204\n",
      "Epoch [4180/10000], Training Loss: 1.9639, Validation Loss: 664.6024\n",
      "Epoch [4185/10000], Training Loss: 2.1751, Validation Loss: 742.2733\n",
      "Epoch [4190/10000], Training Loss: 2.2644, Validation Loss: 765.3795\n",
      "Epoch [4195/10000], Training Loss: 2.0859, Validation Loss: 748.0609\n",
      "Epoch [4200/10000], Training Loss: 2.3076, Validation Loss: 523.6615\n",
      "Epoch [4205/10000], Training Loss: 1.9613, Validation Loss: 458.5723\n",
      "Epoch [4210/10000], Training Loss: 2.0216, Validation Loss: 553.5651\n",
      "Epoch [4215/10000], Training Loss: 2.2892, Validation Loss: 614.5518\n",
      "Epoch [4220/10000], Training Loss: 2.2177, Validation Loss: 590.3923\n",
      "Epoch [4225/10000], Training Loss: 2.3826, Validation Loss: 453.4139\n",
      "Epoch [4230/10000], Training Loss: 2.2232, Validation Loss: 520.9643\n",
      "Epoch [4235/10000], Training Loss: 2.1496, Validation Loss: 454.6102\n",
      "Epoch [4240/10000], Training Loss: 2.0798, Validation Loss: 312.8588\n",
      "Epoch [4245/10000], Training Loss: 2.0605, Validation Loss: 307.0239\n",
      "Epoch [4250/10000], Training Loss: 2.1852, Validation Loss: 282.9208\n",
      "Epoch [4255/10000], Training Loss: 2.2321, Validation Loss: 352.9987\n",
      "Epoch [4260/10000], Training Loss: 2.2791, Validation Loss: 447.5225\n",
      "Epoch [4265/10000], Training Loss: 2.1850, Validation Loss: 500.4860\n",
      "Epoch [4270/10000], Training Loss: 2.2897, Validation Loss: 528.9479\n",
      "Epoch [4275/10000], Training Loss: 2.1217, Validation Loss: 696.1095\n",
      "Epoch [4280/10000], Training Loss: 1.9511, Validation Loss: 711.6441\n",
      "Epoch [4285/10000], Training Loss: 2.1091, Validation Loss: 761.9732\n",
      "Epoch [4290/10000], Training Loss: 2.2071, Validation Loss: 760.7532\n",
      "Epoch [4295/10000], Training Loss: 2.1841, Validation Loss: 838.2503\n",
      "Epoch [4300/10000], Training Loss: 2.2663, Validation Loss: 757.3909\n",
      "Epoch [4305/10000], Training Loss: 2.1309, Validation Loss: 712.4562\n",
      "Epoch [4310/10000], Training Loss: 2.1406, Validation Loss: 642.1555\n",
      "Epoch [4315/10000], Training Loss: 2.0997, Validation Loss: 668.3877\n",
      "Epoch [4320/10000], Training Loss: 2.1296, Validation Loss: 731.3014\n",
      "Epoch [4325/10000], Training Loss: 2.3342, Validation Loss: 527.7380\n",
      "Epoch [4330/10000], Training Loss: 2.3861, Validation Loss: 538.3995\n",
      "Epoch [4335/10000], Training Loss: 2.2102, Validation Loss: 703.2258\n",
      "Epoch [4340/10000], Training Loss: 2.2120, Validation Loss: 701.4261\n",
      "Epoch [4345/10000], Training Loss: 1.9392, Validation Loss: 651.6381\n",
      "Epoch [4350/10000], Training Loss: 2.4841, Validation Loss: 469.5754\n",
      "Epoch [4355/10000], Training Loss: 2.0163, Validation Loss: 434.8504\n",
      "Epoch [4360/10000], Training Loss: 2.2022, Validation Loss: 552.5398\n",
      "Epoch [4365/10000], Training Loss: 1.9872, Validation Loss: 810.1859\n",
      "Epoch [4370/10000], Training Loss: 2.2241, Validation Loss: 902.1208\n",
      "Epoch [4375/10000], Training Loss: 2.3501, Validation Loss: 969.0121\n",
      "Epoch [4380/10000], Training Loss: 1.9845, Validation Loss: 832.9016\n",
      "Epoch [4385/10000], Training Loss: 2.0788, Validation Loss: 901.4941\n",
      "Epoch [4390/10000], Training Loss: 2.3829, Validation Loss: 1003.6268\n",
      "Epoch [4395/10000], Training Loss: 2.4283, Validation Loss: 840.8409\n",
      "Epoch [4400/10000], Training Loss: 2.1613, Validation Loss: 856.8154\n",
      "Epoch [4405/10000], Training Loss: 2.3695, Validation Loss: 1061.7206\n",
      "Epoch [4410/10000], Training Loss: 1.9909, Validation Loss: 906.9178\n",
      "Epoch [4415/10000], Training Loss: 2.2624, Validation Loss: 932.0419\n",
      "Epoch [4420/10000], Training Loss: 2.1026, Validation Loss: 784.9018\n",
      "Epoch [4425/10000], Training Loss: 2.3137, Validation Loss: 603.9153\n",
      "Epoch [4430/10000], Training Loss: 2.1744, Validation Loss: 681.7249\n",
      "Epoch [4435/10000], Training Loss: 1.9643, Validation Loss: 811.5569\n",
      "Epoch [4440/10000], Training Loss: 2.1786, Validation Loss: 796.3817\n",
      "Epoch [4445/10000], Training Loss: 2.0361, Validation Loss: 639.8785\n",
      "Epoch [4450/10000], Training Loss: 2.1926, Validation Loss: 386.3029\n",
      "Epoch [4455/10000], Training Loss: 2.1770, Validation Loss: 335.6137\n",
      "Epoch [4460/10000], Training Loss: 2.3814, Validation Loss: 477.1141\n",
      "Epoch [4465/10000], Training Loss: 2.0623, Validation Loss: 555.0312\n",
      "Epoch [4470/10000], Training Loss: 2.2481, Validation Loss: 611.4216\n",
      "Epoch [4475/10000], Training Loss: 2.4804, Validation Loss: 727.3207\n",
      "Epoch [4480/10000], Training Loss: 2.1651, Validation Loss: 818.2635\n",
      "Epoch [4485/10000], Training Loss: 2.3385, Validation Loss: 838.7581\n",
      "Epoch [4490/10000], Training Loss: 2.4054, Validation Loss: 731.5862\n",
      "Epoch [4495/10000], Training Loss: 1.8713, Validation Loss: 668.6666\n",
      "Epoch [4500/10000], Training Loss: 2.0251, Validation Loss: 513.4692\n",
      "Epoch [4505/10000], Training Loss: 2.1551, Validation Loss: 532.0390\n",
      "Epoch [4510/10000], Training Loss: 2.6007, Validation Loss: 604.5272\n",
      "Epoch [4515/10000], Training Loss: 2.1772, Validation Loss: 609.7263\n",
      "Epoch [4520/10000], Training Loss: 2.2934, Validation Loss: 568.1414\n",
      "Epoch [4525/10000], Training Loss: 2.0044, Validation Loss: 783.0679\n",
      "Epoch [4530/10000], Training Loss: 2.2556, Validation Loss: 760.6072\n",
      "Epoch [4535/10000], Training Loss: 2.3002, Validation Loss: 732.0490\n",
      "Epoch [4540/10000], Training Loss: 1.9646, Validation Loss: 593.3920\n",
      "Epoch [4545/10000], Training Loss: 1.8533, Validation Loss: 708.7543\n",
      "Epoch [4550/10000], Training Loss: 1.9974, Validation Loss: 951.4222\n",
      "Epoch [4555/10000], Training Loss: 2.0540, Validation Loss: 1129.0254\n",
      "Epoch [4560/10000], Training Loss: 2.1713, Validation Loss: 1094.2812\n",
      "Epoch [4565/10000], Training Loss: 2.1669, Validation Loss: 795.8073\n",
      "Epoch [4570/10000], Training Loss: 2.0966, Validation Loss: 720.5682\n",
      "Epoch [4575/10000], Training Loss: 2.3258, Validation Loss: 761.7197\n",
      "Epoch [4580/10000], Training Loss: 2.2985, Validation Loss: 876.0637\n",
      "Epoch [4585/10000], Training Loss: 2.1229, Validation Loss: 937.4626\n",
      "Epoch [4590/10000], Training Loss: 1.9099, Validation Loss: 1302.4199\n",
      "Epoch [4595/10000], Training Loss: 2.2034, Validation Loss: 1194.3256\n",
      "Epoch [4600/10000], Training Loss: 2.2224, Validation Loss: 1201.4869\n",
      "Epoch [4605/10000], Training Loss: 2.0758, Validation Loss: 1142.3732\n",
      "Epoch [4610/10000], Training Loss: 2.1128, Validation Loss: 1197.0328\n",
      "Epoch [4615/10000], Training Loss: 2.0596, Validation Loss: 951.2019\n",
      "Epoch [4620/10000], Training Loss: 2.3168, Validation Loss: 830.1980\n",
      "Epoch [4625/10000], Training Loss: 2.1148, Validation Loss: 595.3328\n",
      "Epoch [4630/10000], Training Loss: 1.9840, Validation Loss: 431.4190\n",
      "Epoch [4635/10000], Training Loss: 2.1611, Validation Loss: 586.4945\n",
      "Epoch [4640/10000], Training Loss: 2.2920, Validation Loss: 864.9651\n",
      "Epoch [4645/10000], Training Loss: 1.8248, Validation Loss: 1018.9387\n",
      "Epoch [4650/10000], Training Loss: 2.1541, Validation Loss: 936.6335\n",
      "Epoch [4655/10000], Training Loss: 1.9549, Validation Loss: 807.1694\n",
      "Epoch [4660/10000], Training Loss: 1.9316, Validation Loss: 642.7979\n",
      "Epoch [4665/10000], Training Loss: 2.1515, Validation Loss: 664.5972\n",
      "Epoch [4670/10000], Training Loss: 2.0315, Validation Loss: 971.3987\n",
      "Epoch [4675/10000], Training Loss: 2.0909, Validation Loss: 969.2430\n",
      "Epoch [4680/10000], Training Loss: 1.9903, Validation Loss: 1096.0365\n",
      "Epoch [4685/10000], Training Loss: 2.2624, Validation Loss: 1070.3071\n",
      "Epoch [4690/10000], Training Loss: 2.1347, Validation Loss: 800.6229\n",
      "Epoch [4695/10000], Training Loss: 2.0559, Validation Loss: 767.2727\n",
      "Epoch [4700/10000], Training Loss: 2.1269, Validation Loss: 835.7653\n",
      "Epoch [4705/10000], Training Loss: 1.7957, Validation Loss: 861.4401\n",
      "Epoch [4710/10000], Training Loss: 2.0035, Validation Loss: 865.9934\n",
      "Epoch [4715/10000], Training Loss: 2.2422, Validation Loss: 812.3271\n",
      "Epoch [4720/10000], Training Loss: 2.0233, Validation Loss: 956.6684\n",
      "Epoch [4725/10000], Training Loss: 2.0542, Validation Loss: 925.8936\n",
      "Epoch [4730/10000], Training Loss: 2.2547, Validation Loss: 841.3715\n",
      "Epoch [4735/10000], Training Loss: 2.1270, Validation Loss: 911.1973\n",
      "Epoch [4740/10000], Training Loss: 1.9420, Validation Loss: 1008.3613\n",
      "Epoch [4745/10000], Training Loss: 2.2042, Validation Loss: 820.4247\n",
      "Epoch [4750/10000], Training Loss: 2.0289, Validation Loss: 765.0286\n",
      "Epoch [4755/10000], Training Loss: 2.0496, Validation Loss: 853.0424\n",
      "Epoch [4760/10000], Training Loss: 2.1288, Validation Loss: 877.5670\n",
      "Epoch [4765/10000], Training Loss: 2.1589, Validation Loss: 1087.3219\n",
      "Epoch [4770/10000], Training Loss: 1.9955, Validation Loss: 1188.0859\n",
      "Epoch [4775/10000], Training Loss: 2.0263, Validation Loss: 1046.9534\n",
      "Epoch [4780/10000], Training Loss: 2.1106, Validation Loss: 1055.1079\n",
      "Epoch [4785/10000], Training Loss: 2.1055, Validation Loss: 862.5266\n",
      "Epoch [4790/10000], Training Loss: 2.1121, Validation Loss: 1040.6908\n",
      "Epoch [4795/10000], Training Loss: 2.0547, Validation Loss: 1077.7673\n",
      "Epoch [4800/10000], Training Loss: 2.0794, Validation Loss: 1103.3989\n",
      "Epoch [4805/10000], Training Loss: 2.0448, Validation Loss: 1269.4659\n",
      "Epoch [4810/10000], Training Loss: 1.7944, Validation Loss: 1110.3322\n",
      "Epoch [4815/10000], Training Loss: 2.0313, Validation Loss: 1006.8117\n",
      "Epoch [4820/10000], Training Loss: 1.9171, Validation Loss: 903.0272\n",
      "Epoch [4825/10000], Training Loss: 2.0689, Validation Loss: 934.8455\n",
      "Epoch [4830/10000], Training Loss: 1.9993, Validation Loss: 916.5769\n",
      "Epoch [4835/10000], Training Loss: 1.9398, Validation Loss: 769.1596\n",
      "Epoch [4840/10000], Training Loss: 2.1810, Validation Loss: 621.1524\n",
      "Epoch [4845/10000], Training Loss: 2.3164, Validation Loss: 832.2913\n",
      "Epoch [4850/10000], Training Loss: 1.8922, Validation Loss: 693.3649\n",
      "Epoch [4855/10000], Training Loss: 1.9827, Validation Loss: 680.0237\n",
      "Epoch [4860/10000], Training Loss: 2.3212, Validation Loss: 922.0776\n",
      "Epoch [4865/10000], Training Loss: 2.3668, Validation Loss: 1018.1326\n",
      "Epoch [4870/10000], Training Loss: 2.5118, Validation Loss: 1473.3467\n",
      "Epoch [4875/10000], Training Loss: 2.0305, Validation Loss: 1245.9645\n",
      "Epoch [4880/10000], Training Loss: 1.9335, Validation Loss: 1157.2207\n",
      "Epoch [4885/10000], Training Loss: 1.8306, Validation Loss: 1155.7325\n",
      "Epoch [4890/10000], Training Loss: 1.9757, Validation Loss: 1522.5929\n",
      "Epoch [4895/10000], Training Loss: 1.9410, Validation Loss: 1506.5638\n",
      "Epoch [4900/10000], Training Loss: 2.2007, Validation Loss: 1166.8419\n",
      "Epoch [4905/10000], Training Loss: 1.9871, Validation Loss: 1275.7181\n",
      "Epoch [4910/10000], Training Loss: 2.0214, Validation Loss: 1661.5366\n",
      "Epoch [4915/10000], Training Loss: 2.0838, Validation Loss: 1631.7871\n",
      "Epoch [4920/10000], Training Loss: 2.1033, Validation Loss: 1468.1636\n",
      "Epoch [4925/10000], Training Loss: 2.2990, Validation Loss: 1456.5273\n",
      "Epoch [4930/10000], Training Loss: 2.2194, Validation Loss: 1261.1587\n",
      "Epoch [4935/10000], Training Loss: 2.2353, Validation Loss: 1091.7200\n",
      "Epoch [4940/10000], Training Loss: 2.4062, Validation Loss: 1101.9633\n",
      "Epoch [4945/10000], Training Loss: 2.2004, Validation Loss: 1056.1780\n",
      "Epoch [4950/10000], Training Loss: 2.2575, Validation Loss: 1731.2018\n",
      "Epoch [4955/10000], Training Loss: 1.9600, Validation Loss: 1900.7278\n",
      "Epoch [4960/10000], Training Loss: 2.0579, Validation Loss: 2331.4207\n",
      "Epoch [4965/10000], Training Loss: 1.8913, Validation Loss: 2148.5828\n",
      "Epoch [4970/10000], Training Loss: 2.2474, Validation Loss: 1753.3468\n",
      "Epoch [4975/10000], Training Loss: 2.3341, Validation Loss: 1546.8926\n",
      "Epoch [4980/10000], Training Loss: 1.8699, Validation Loss: 1681.0089\n",
      "Epoch [4985/10000], Training Loss: 1.9789, Validation Loss: 1249.0137\n",
      "Epoch [4990/10000], Training Loss: 2.3891, Validation Loss: 976.7922\n",
      "Epoch [4995/10000], Training Loss: 2.1146, Validation Loss: 921.2455\n",
      "Epoch [5000/10000], Training Loss: 1.9772, Validation Loss: 778.8256\n",
      "Epoch [5005/10000], Training Loss: 1.8540, Validation Loss: 924.3253\n",
      "Epoch [5010/10000], Training Loss: 2.3186, Validation Loss: 1597.0536\n",
      "Epoch [5015/10000], Training Loss: 2.1047, Validation Loss: 1494.5312\n",
      "Epoch [5020/10000], Training Loss: 2.1954, Validation Loss: 1501.0441\n",
      "Epoch [5025/10000], Training Loss: 2.0309, Validation Loss: 1221.6841\n",
      "Epoch [5030/10000], Training Loss: 1.7889, Validation Loss: 1184.2675\n",
      "Epoch [5035/10000], Training Loss: 1.8996, Validation Loss: 1166.9659\n",
      "Epoch [5040/10000], Training Loss: 2.0431, Validation Loss: 1043.6855\n",
      "Epoch [5045/10000], Training Loss: 1.7602, Validation Loss: 1253.3408\n",
      "Epoch [5050/10000], Training Loss: 2.2282, Validation Loss: 1718.2920\n",
      "Epoch [5055/10000], Training Loss: 1.9553, Validation Loss: 1928.6293\n",
      "Epoch [5060/10000], Training Loss: 1.9012, Validation Loss: 1728.1385\n",
      "Epoch [5065/10000], Training Loss: 1.9749, Validation Loss: 1408.3696\n",
      "Epoch [5070/10000], Training Loss: 2.1023, Validation Loss: 1522.0662\n",
      "Epoch [5075/10000], Training Loss: 1.7043, Validation Loss: 1520.9689\n",
      "Epoch [5080/10000], Training Loss: 2.0973, Validation Loss: 1291.2654\n",
      "Epoch [5085/10000], Training Loss: 2.0237, Validation Loss: 881.0469\n",
      "Epoch [5090/10000], Training Loss: 2.1577, Validation Loss: 1009.0967\n",
      "Epoch [5095/10000], Training Loss: 1.8984, Validation Loss: 1103.7430\n",
      "Epoch [5100/10000], Training Loss: 2.0913, Validation Loss: 1263.0875\n",
      "Epoch [5105/10000], Training Loss: 2.1197, Validation Loss: 1337.5330\n",
      "Epoch [5110/10000], Training Loss: 1.9564, Validation Loss: 1225.4746\n",
      "Epoch [5115/10000], Training Loss: 2.1166, Validation Loss: 1159.5516\n",
      "Epoch [5120/10000], Training Loss: 2.2002, Validation Loss: 1306.9010\n",
      "Epoch [5125/10000], Training Loss: 2.0293, Validation Loss: 1374.7178\n",
      "Epoch [5130/10000], Training Loss: 1.9564, Validation Loss: 1183.2053\n",
      "Epoch [5135/10000], Training Loss: 1.8356, Validation Loss: 900.0128\n",
      "Epoch [5140/10000], Training Loss: 1.8695, Validation Loss: 1006.4435\n",
      "Epoch [5145/10000], Training Loss: 2.1393, Validation Loss: 1280.7290\n",
      "Epoch [5150/10000], Training Loss: 2.0609, Validation Loss: 1464.1830\n",
      "Epoch [5155/10000], Training Loss: 2.0878, Validation Loss: 1448.3973\n",
      "Epoch [5160/10000], Training Loss: 2.4524, Validation Loss: 1700.9844\n",
      "Epoch [5165/10000], Training Loss: 2.3278, Validation Loss: 1601.1534\n",
      "Epoch [5170/10000], Training Loss: 1.7734, Validation Loss: 1405.8918\n",
      "Epoch [5175/10000], Training Loss: 2.1287, Validation Loss: 1395.4478\n",
      "Epoch [5180/10000], Training Loss: 2.0894, Validation Loss: 1418.1144\n",
      "Epoch [5185/10000], Training Loss: 1.9632, Validation Loss: 1390.3802\n",
      "Epoch [5190/10000], Training Loss: 1.8679, Validation Loss: 1168.3418\n",
      "Epoch [5195/10000], Training Loss: 1.9703, Validation Loss: 1075.0636\n",
      "Epoch [5200/10000], Training Loss: 2.0551, Validation Loss: 1339.3479\n",
      "Epoch [5205/10000], Training Loss: 1.7701, Validation Loss: 1370.4878\n",
      "Epoch [5210/10000], Training Loss: 2.2020, Validation Loss: 1762.5488\n",
      "Epoch [5215/10000], Training Loss: 2.1585, Validation Loss: 1637.7043\n",
      "Epoch [5220/10000], Training Loss: 2.0430, Validation Loss: 1512.8993\n",
      "Epoch [5225/10000], Training Loss: 2.0590, Validation Loss: 1402.2659\n",
      "Epoch [5230/10000], Training Loss: 2.3472, Validation Loss: 1462.5760\n",
      "Epoch [5235/10000], Training Loss: 2.1442, Validation Loss: 1240.8367\n",
      "Epoch [5240/10000], Training Loss: 1.6966, Validation Loss: 1104.4991\n",
      "Epoch [5245/10000], Training Loss: 1.8292, Validation Loss: 1233.3745\n",
      "Epoch [5250/10000], Training Loss: 1.7807, Validation Loss: 1243.7327\n",
      "Epoch [5255/10000], Training Loss: 1.7695, Validation Loss: 1265.3276\n",
      "Epoch [5260/10000], Training Loss: 1.7912, Validation Loss: 1313.3816\n",
      "Epoch [5265/10000], Training Loss: 2.0837, Validation Loss: 1248.3406\n",
      "Epoch [5270/10000], Training Loss: 1.9729, Validation Loss: 1151.6572\n",
      "Epoch [5275/10000], Training Loss: 1.9997, Validation Loss: 1094.7592\n",
      "Epoch [5280/10000], Training Loss: 1.9445, Validation Loss: 1539.3440\n",
      "Epoch [5285/10000], Training Loss: 2.0334, Validation Loss: 1638.4412\n",
      "Epoch [5290/10000], Training Loss: 1.8215, Validation Loss: 1530.1443\n",
      "Epoch [5295/10000], Training Loss: 1.9338, Validation Loss: 1633.2734\n",
      "Epoch [5300/10000], Training Loss: 2.3155, Validation Loss: 1583.9286\n",
      "Epoch [5305/10000], Training Loss: 2.1600, Validation Loss: 1357.9615\n",
      "Epoch [5310/10000], Training Loss: 2.1177, Validation Loss: 1486.3345\n",
      "Epoch [5315/10000], Training Loss: 1.9947, Validation Loss: 1351.9430\n",
      "Epoch [5320/10000], Training Loss: 1.9897, Validation Loss: 1444.3571\n",
      "Epoch [5325/10000], Training Loss: 2.0045, Validation Loss: 1502.3373\n",
      "Epoch [5330/10000], Training Loss: 2.2078, Validation Loss: 1559.3356\n",
      "Epoch [5335/10000], Training Loss: 2.2839, Validation Loss: 1423.1467\n",
      "Epoch [5340/10000], Training Loss: 2.0908, Validation Loss: 1188.2660\n",
      "Epoch [5345/10000], Training Loss: 1.9160, Validation Loss: 1652.7684\n",
      "Epoch [5350/10000], Training Loss: 2.0587, Validation Loss: 1498.4293\n",
      "Epoch [5355/10000], Training Loss: 1.7667, Validation Loss: 1134.3461\n",
      "Epoch [5360/10000], Training Loss: 1.9138, Validation Loss: 1346.5375\n",
      "Epoch [5365/10000], Training Loss: 2.2244, Validation Loss: 1450.7719\n",
      "Epoch [5370/10000], Training Loss: 2.1991, Validation Loss: 1636.9596\n",
      "Epoch [5375/10000], Training Loss: 1.9860, Validation Loss: 1751.9928\n",
      "Epoch [5380/10000], Training Loss: 1.8259, Validation Loss: 1414.6428\n",
      "Epoch [5385/10000], Training Loss: 2.0401, Validation Loss: 1272.7225\n",
      "Epoch [5390/10000], Training Loss: 1.7125, Validation Loss: 1493.6354\n",
      "Epoch [5395/10000], Training Loss: 1.9732, Validation Loss: 1435.9612\n",
      "Epoch [5400/10000], Training Loss: 1.7661, Validation Loss: 1604.8307\n",
      "Epoch [5405/10000], Training Loss: 1.6798, Validation Loss: 1360.2023\n",
      "Epoch [5410/10000], Training Loss: 2.0531, Validation Loss: 1251.5790\n",
      "Epoch [5415/10000], Training Loss: 2.3113, Validation Loss: 1651.7346\n",
      "Epoch [5420/10000], Training Loss: 2.2174, Validation Loss: 1666.1001\n",
      "Epoch [5425/10000], Training Loss: 1.9401, Validation Loss: 1906.0369\n",
      "Epoch [5430/10000], Training Loss: 1.6878, Validation Loss: 1803.4115\n",
      "Epoch [5435/10000], Training Loss: 1.9346, Validation Loss: 1837.7283\n",
      "Epoch [5440/10000], Training Loss: 1.9563, Validation Loss: 1950.7290\n",
      "Epoch [5445/10000], Training Loss: 1.7867, Validation Loss: 1772.6797\n",
      "Epoch [5450/10000], Training Loss: 2.2420, Validation Loss: 1285.0847\n",
      "Epoch [5455/10000], Training Loss: 2.1111, Validation Loss: 1559.1742\n",
      "Epoch [5460/10000], Training Loss: 1.6775, Validation Loss: 1911.9768\n",
      "Epoch [5465/10000], Training Loss: 1.9169, Validation Loss: 1669.1056\n",
      "Epoch [5470/10000], Training Loss: 1.7196, Validation Loss: 1964.4265\n",
      "Epoch [5475/10000], Training Loss: 1.8692, Validation Loss: 2244.8167\n",
      "Epoch [5480/10000], Training Loss: 1.7977, Validation Loss: 1758.7283\n",
      "Epoch [5485/10000], Training Loss: 2.0136, Validation Loss: 1448.0714\n",
      "Epoch [5490/10000], Training Loss: 1.7022, Validation Loss: 1291.8026\n",
      "Epoch [5495/10000], Training Loss: 1.9053, Validation Loss: 1200.1890\n",
      "Epoch [5500/10000], Training Loss: 1.6639, Validation Loss: 1311.6732\n",
      "Epoch [5505/10000], Training Loss: 1.9487, Validation Loss: 1283.6417\n",
      "Epoch [5510/10000], Training Loss: 1.9589, Validation Loss: 1248.9618\n",
      "Epoch [5515/10000], Training Loss: 1.9733, Validation Loss: 1707.7806\n",
      "Epoch [5520/10000], Training Loss: 2.1101, Validation Loss: 1413.7863\n",
      "Epoch [5525/10000], Training Loss: 2.1284, Validation Loss: 1346.2388\n",
      "Epoch [5530/10000], Training Loss: 1.9587, Validation Loss: 1616.9805\n",
      "Epoch [5535/10000], Training Loss: 1.9431, Validation Loss: 1872.7715\n",
      "Epoch [5540/10000], Training Loss: 2.0590, Validation Loss: 1586.7821\n",
      "Epoch [5545/10000], Training Loss: 1.9120, Validation Loss: 1391.8683\n",
      "Epoch [5550/10000], Training Loss: 1.7906, Validation Loss: 1307.3356\n",
      "Epoch [5555/10000], Training Loss: 1.9287, Validation Loss: 1451.6543\n",
      "Epoch [5560/10000], Training Loss: 2.2591, Validation Loss: 1333.4146\n",
      "Epoch [5565/10000], Training Loss: 1.6055, Validation Loss: 1551.8761\n",
      "Epoch [5570/10000], Training Loss: 2.2939, Validation Loss: 1490.1569\n",
      "Epoch [5575/10000], Training Loss: 1.9335, Validation Loss: 1425.5739\n",
      "Epoch [5580/10000], Training Loss: 2.3448, Validation Loss: 1379.3148\n",
      "Epoch [5585/10000], Training Loss: 1.7846, Validation Loss: 1496.4198\n",
      "Epoch [5590/10000], Training Loss: 1.9862, Validation Loss: 1849.8478\n",
      "Epoch [5595/10000], Training Loss: 1.7964, Validation Loss: 2131.2258\n",
      "Epoch [5600/10000], Training Loss: 2.0564, Validation Loss: 1947.9408\n",
      "Epoch [5605/10000], Training Loss: 2.0779, Validation Loss: 2022.1605\n",
      "Epoch [5610/10000], Training Loss: 2.1738, Validation Loss: 1598.0573\n",
      "Epoch [5615/10000], Training Loss: 2.0588, Validation Loss: 1438.5842\n",
      "Epoch [5620/10000], Training Loss: 1.9230, Validation Loss: 2033.5219\n",
      "Epoch [5625/10000], Training Loss: 1.8916, Validation Loss: 2414.4814\n",
      "Epoch [5630/10000], Training Loss: 1.9572, Validation Loss: 2482.0427\n",
      "Epoch [5635/10000], Training Loss: 1.9259, Validation Loss: 1970.9336\n",
      "Epoch [5640/10000], Training Loss: 1.7550, Validation Loss: 1318.9559\n",
      "Epoch [5645/10000], Training Loss: 1.9585, Validation Loss: 1390.8348\n",
      "Epoch [5650/10000], Training Loss: 1.9808, Validation Loss: 1195.9473\n",
      "Epoch [5655/10000], Training Loss: 1.8004, Validation Loss: 1567.2615\n",
      "Epoch [5660/10000], Training Loss: 2.0745, Validation Loss: 1720.8861\n",
      "Epoch [5665/10000], Training Loss: 2.0121, Validation Loss: 1652.6265\n",
      "Epoch [5670/10000], Training Loss: 1.8198, Validation Loss: 1422.3014\n",
      "Epoch [5675/10000], Training Loss: 2.1291, Validation Loss: 1796.8292\n",
      "Epoch [5680/10000], Training Loss: 2.0412, Validation Loss: 1500.2959\n",
      "Epoch [5685/10000], Training Loss: 2.1047, Validation Loss: 1396.2281\n",
      "Epoch [5690/10000], Training Loss: 2.0162, Validation Loss: 1198.1895\n",
      "Epoch [5695/10000], Training Loss: 2.0342, Validation Loss: 1284.3047\n",
      "Epoch [5700/10000], Training Loss: 2.0047, Validation Loss: 1197.0703\n",
      "Epoch [5705/10000], Training Loss: 1.9394, Validation Loss: 1093.2800\n",
      "Epoch [5710/10000], Training Loss: 2.0558, Validation Loss: 1056.5376\n",
      "Epoch [5715/10000], Training Loss: 1.7163, Validation Loss: 1200.7478\n",
      "Epoch [5720/10000], Training Loss: 1.5748, Validation Loss: 1457.1617\n",
      "Epoch [5725/10000], Training Loss: 1.8520, Validation Loss: 1593.6434\n",
      "Epoch [5730/10000], Training Loss: 1.8724, Validation Loss: 1618.7825\n",
      "Epoch [5735/10000], Training Loss: 1.8265, Validation Loss: 1364.2802\n",
      "Epoch [5740/10000], Training Loss: 1.8005, Validation Loss: 1405.2345\n",
      "Epoch [5745/10000], Training Loss: 1.9029, Validation Loss: 1321.6160\n",
      "Epoch [5750/10000], Training Loss: 1.8107, Validation Loss: 1277.1444\n",
      "Epoch [5755/10000], Training Loss: 1.6375, Validation Loss: 1287.2574\n",
      "Epoch [5760/10000], Training Loss: 2.0308, Validation Loss: 1371.5889\n",
      "Epoch [5765/10000], Training Loss: 1.8718, Validation Loss: 1585.9098\n",
      "Epoch [5770/10000], Training Loss: 1.7979, Validation Loss: 1717.4462\n",
      "Epoch [5775/10000], Training Loss: 1.7794, Validation Loss: 1882.9683\n",
      "Epoch [5780/10000], Training Loss: 2.2997, Validation Loss: 1644.0187\n",
      "Epoch [5785/10000], Training Loss: 1.7290, Validation Loss: 1644.8337\n",
      "Epoch [5790/10000], Training Loss: 1.9525, Validation Loss: 1430.0044\n",
      "Epoch [5795/10000], Training Loss: 2.1838, Validation Loss: 1707.8517\n",
      "Epoch [5800/10000], Training Loss: 2.0393, Validation Loss: 1664.8439\n",
      "Epoch [5805/10000], Training Loss: 1.9299, Validation Loss: 1541.7045\n",
      "Epoch [5810/10000], Training Loss: 1.8625, Validation Loss: 1480.1459\n",
      "Epoch [5815/10000], Training Loss: 1.9419, Validation Loss: 1484.2797\n",
      "Epoch [5820/10000], Training Loss: 2.0184, Validation Loss: 1311.0476\n",
      "Epoch [5825/10000], Training Loss: 2.0176, Validation Loss: 1195.2788\n",
      "Epoch [5830/10000], Training Loss: 1.7701, Validation Loss: 1211.3110\n",
      "Epoch [5835/10000], Training Loss: 1.9042, Validation Loss: 1395.8812\n",
      "Epoch [5840/10000], Training Loss: 1.8329, Validation Loss: 1684.7952\n",
      "Epoch [5845/10000], Training Loss: 1.7217, Validation Loss: 1316.7140\n",
      "Epoch [5850/10000], Training Loss: 1.7875, Validation Loss: 1312.1682\n",
      "Epoch [5855/10000], Training Loss: 1.9858, Validation Loss: 1344.8650\n",
      "Epoch [5860/10000], Training Loss: 2.2022, Validation Loss: 1258.7288\n",
      "Epoch [5865/10000], Training Loss: 1.8646, Validation Loss: 1444.2131\n",
      "Epoch [5870/10000], Training Loss: 1.8932, Validation Loss: 1521.8271\n",
      "Epoch [5875/10000], Training Loss: 2.0947, Validation Loss: 1582.1465\n",
      "Epoch [5880/10000], Training Loss: 2.1346, Validation Loss: 1626.5111\n",
      "Epoch [5885/10000], Training Loss: 1.8961, Validation Loss: 1656.3489\n",
      "Epoch [5890/10000], Training Loss: 1.9638, Validation Loss: 1333.3344\n",
      "Epoch [5895/10000], Training Loss: 2.0181, Validation Loss: 1262.1381\n",
      "Epoch [5900/10000], Training Loss: 1.9251, Validation Loss: 1375.5280\n",
      "Epoch [5905/10000], Training Loss: 1.5897, Validation Loss: 1566.4120\n",
      "Epoch [5910/10000], Training Loss: 2.2885, Validation Loss: 1472.0787\n",
      "Epoch [5915/10000], Training Loss: 1.8945, Validation Loss: 1516.5627\n",
      "Epoch [5920/10000], Training Loss: 2.3275, Validation Loss: 934.1833\n",
      "Epoch [5925/10000], Training Loss: 2.1600, Validation Loss: 1134.0321\n",
      "Epoch [5930/10000], Training Loss: 1.6860, Validation Loss: 1224.9484\n",
      "Epoch [5935/10000], Training Loss: 1.9390, Validation Loss: 1097.0867\n",
      "Epoch [5940/10000], Training Loss: 1.9437, Validation Loss: 1427.5432\n",
      "Epoch [5945/10000], Training Loss: 1.9402, Validation Loss: 1406.8286\n",
      "Epoch [5950/10000], Training Loss: 2.1964, Validation Loss: 1401.3943\n",
      "Epoch [5955/10000], Training Loss: 1.7789, Validation Loss: 1466.8752\n",
      "Epoch [5960/10000], Training Loss: 1.9897, Validation Loss: 1321.1558\n",
      "Epoch [5965/10000], Training Loss: 1.8578, Validation Loss: 1032.2606\n",
      "Epoch [5970/10000], Training Loss: 2.1496, Validation Loss: 1376.0530\n",
      "Epoch [5975/10000], Training Loss: 1.8787, Validation Loss: 1915.6853\n",
      "Epoch [5980/10000], Training Loss: 1.8536, Validation Loss: 2144.8435\n",
      "Epoch [5985/10000], Training Loss: 1.9006, Validation Loss: 1466.4547\n",
      "Epoch [5990/10000], Training Loss: 2.3010, Validation Loss: 1213.0396\n",
      "Epoch [5995/10000], Training Loss: 2.0107, Validation Loss: 1700.8839\n",
      "Epoch [6000/10000], Training Loss: 1.9655, Validation Loss: 1768.0760\n",
      "Epoch [6005/10000], Training Loss: 2.0485, Validation Loss: 1270.8175\n",
      "Epoch [6010/10000], Training Loss: 1.8960, Validation Loss: 1292.0820\n",
      "Epoch [6015/10000], Training Loss: 1.7823, Validation Loss: 1367.1680\n",
      "Epoch [6020/10000], Training Loss: 1.8156, Validation Loss: 1584.6376\n",
      "Epoch [6025/10000], Training Loss: 1.8176, Validation Loss: 1643.7334\n",
      "Epoch [6030/10000], Training Loss: 1.9058, Validation Loss: 1813.2611\n",
      "Epoch [6035/10000], Training Loss: 1.7917, Validation Loss: 2020.9989\n",
      "Epoch [6040/10000], Training Loss: 1.9709, Validation Loss: 2344.3621\n",
      "Epoch [6045/10000], Training Loss: 2.0135, Validation Loss: 1793.0128\n",
      "Epoch [6050/10000], Training Loss: 1.9593, Validation Loss: 1682.4451\n",
      "Epoch [6055/10000], Training Loss: 2.0829, Validation Loss: 1738.5347\n",
      "Epoch [6060/10000], Training Loss: 1.8893, Validation Loss: 2180.4631\n",
      "Epoch [6065/10000], Training Loss: 1.9460, Validation Loss: 2088.8823\n",
      "Epoch [6070/10000], Training Loss: 1.5598, Validation Loss: 1646.1758\n",
      "Epoch [6075/10000], Training Loss: 1.8472, Validation Loss: 1969.5072\n",
      "Epoch [6080/10000], Training Loss: 1.8405, Validation Loss: 1818.0254\n",
      "Epoch [6085/10000], Training Loss: 1.7308, Validation Loss: 2425.8301\n",
      "Epoch [6090/10000], Training Loss: 1.9769, Validation Loss: 2659.3335\n",
      "Epoch [6095/10000], Training Loss: 2.0067, Validation Loss: 2284.8044\n",
      "Epoch [6100/10000], Training Loss: 1.7323, Validation Loss: 2692.3638\n",
      "Epoch [6105/10000], Training Loss: 1.8812, Validation Loss: 1619.4081\n",
      "Epoch [6110/10000], Training Loss: 1.8244, Validation Loss: 1522.0721\n",
      "Epoch [6115/10000], Training Loss: 1.8074, Validation Loss: 1620.2340\n",
      "Epoch [6120/10000], Training Loss: 1.5536, Validation Loss: 1537.4333\n",
      "Epoch [6125/10000], Training Loss: 1.7626, Validation Loss: 1892.9894\n",
      "Epoch [6130/10000], Training Loss: 1.8282, Validation Loss: 1504.6981\n",
      "Epoch [6135/10000], Training Loss: 1.8979, Validation Loss: 1825.1442\n",
      "Epoch [6140/10000], Training Loss: 1.7198, Validation Loss: 2270.5796\n",
      "Epoch [6145/10000], Training Loss: 1.6400, Validation Loss: 1803.0775\n",
      "Epoch [6150/10000], Training Loss: 1.9130, Validation Loss: 1728.9813\n",
      "Epoch [6155/10000], Training Loss: 1.9952, Validation Loss: 1821.3176\n",
      "Epoch [6160/10000], Training Loss: 1.8688, Validation Loss: 1464.8594\n",
      "Epoch [6165/10000], Training Loss: 1.9462, Validation Loss: 1812.8812\n",
      "Epoch [6170/10000], Training Loss: 2.0294, Validation Loss: 2259.9741\n",
      "Epoch [6175/10000], Training Loss: 1.8049, Validation Loss: 1715.3231\n",
      "Epoch [6180/10000], Training Loss: 1.9971, Validation Loss: 1425.3909\n",
      "Epoch [6185/10000], Training Loss: 1.8858, Validation Loss: 1324.5006\n",
      "Epoch [6190/10000], Training Loss: 1.8080, Validation Loss: 1350.4225\n",
      "Epoch [6195/10000], Training Loss: 2.0016, Validation Loss: 1762.2997\n",
      "Epoch [6200/10000], Training Loss: 2.3341, Validation Loss: 2398.6956\n",
      "Epoch [6205/10000], Training Loss: 1.8124, Validation Loss: 2258.8164\n",
      "Epoch [6210/10000], Training Loss: 2.0454, Validation Loss: 2049.8950\n",
      "Epoch [6215/10000], Training Loss: 1.8387, Validation Loss: 1907.9987\n",
      "Epoch [6220/10000], Training Loss: 1.9819, Validation Loss: 2023.0762\n",
      "Epoch [6225/10000], Training Loss: 1.9760, Validation Loss: 1773.1509\n",
      "Epoch [6230/10000], Training Loss: 1.9982, Validation Loss: 1602.3423\n",
      "Epoch [6235/10000], Training Loss: 1.8048, Validation Loss: 1888.3541\n",
      "Epoch [6240/10000], Training Loss: 1.7828, Validation Loss: 1743.8223\n",
      "Epoch [6245/10000], Training Loss: 1.9178, Validation Loss: 1442.6682\n",
      "Epoch [6250/10000], Training Loss: 1.6973, Validation Loss: 1419.0647\n",
      "Epoch [6255/10000], Training Loss: 1.8649, Validation Loss: 1655.8334\n",
      "Epoch [6260/10000], Training Loss: 1.7181, Validation Loss: 1847.1595\n",
      "Epoch [6265/10000], Training Loss: 1.9867, Validation Loss: 1539.9739\n",
      "Epoch [6270/10000], Training Loss: 1.7900, Validation Loss: 1283.6160\n",
      "Epoch [6275/10000], Training Loss: 1.9538, Validation Loss: 1266.9315\n",
      "Epoch [6280/10000], Training Loss: 1.7558, Validation Loss: 1449.6099\n",
      "Epoch [6285/10000], Training Loss: 1.8294, Validation Loss: 1416.2572\n",
      "Epoch [6290/10000], Training Loss: 1.7968, Validation Loss: 1627.4355\n",
      "Epoch [6295/10000], Training Loss: 1.8106, Validation Loss: 1523.8652\n",
      "Epoch [6300/10000], Training Loss: 1.9819, Validation Loss: 1503.2881\n",
      "Epoch [6305/10000], Training Loss: 1.6205, Validation Loss: 1730.8950\n",
      "Epoch [6310/10000], Training Loss: 1.7594, Validation Loss: 1551.1018\n",
      "Epoch [6315/10000], Training Loss: 1.9281, Validation Loss: 1709.5360\n",
      "Epoch [6320/10000], Training Loss: 1.9777, Validation Loss: 1614.1292\n",
      "Epoch [6325/10000], Training Loss: 1.8086, Validation Loss: 1740.3680\n",
      "Epoch [6330/10000], Training Loss: 1.7677, Validation Loss: 1722.3530\n",
      "Epoch [6335/10000], Training Loss: 2.0487, Validation Loss: 1740.5298\n",
      "Epoch [6340/10000], Training Loss: 2.0180, Validation Loss: 1706.0157\n",
      "Epoch [6345/10000], Training Loss: 1.9663, Validation Loss: 1714.6198\n",
      "Epoch [6350/10000], Training Loss: 2.0968, Validation Loss: 1792.0151\n",
      "Epoch [6355/10000], Training Loss: 2.2117, Validation Loss: 2195.9150\n",
      "Epoch [6360/10000], Training Loss: 1.7965, Validation Loss: 2128.9319\n",
      "Epoch [6365/10000], Training Loss: 1.9453, Validation Loss: 2240.4209\n",
      "Epoch [6370/10000], Training Loss: 1.7405, Validation Loss: 1832.9701\n",
      "Epoch [6375/10000], Training Loss: 1.9996, Validation Loss: 1807.7913\n",
      "Epoch [6380/10000], Training Loss: 2.0855, Validation Loss: 2084.0613\n",
      "Epoch [6385/10000], Training Loss: 1.8935, Validation Loss: 1748.9064\n",
      "Epoch [6390/10000], Training Loss: 1.6171, Validation Loss: 1861.9015\n",
      "Epoch [6395/10000], Training Loss: 1.7815, Validation Loss: 1665.8794\n",
      "Epoch [6400/10000], Training Loss: 2.0273, Validation Loss: 1302.6824\n",
      "Epoch [6405/10000], Training Loss: 1.6440, Validation Loss: 1684.2032\n",
      "Epoch [6410/10000], Training Loss: 2.1508, Validation Loss: 1986.5353\n",
      "Epoch [6415/10000], Training Loss: 1.9487, Validation Loss: 2129.4175\n",
      "Epoch [6420/10000], Training Loss: 1.9585, Validation Loss: 2459.7673\n",
      "Epoch [6425/10000], Training Loss: 1.6581, Validation Loss: 2417.0833\n",
      "Epoch [6430/10000], Training Loss: 1.8474, Validation Loss: 2864.0781\n",
      "Epoch [6435/10000], Training Loss: 1.9031, Validation Loss: 2579.4634\n",
      "Epoch [6440/10000], Training Loss: 1.9953, Validation Loss: 1551.0745\n",
      "Epoch [6445/10000], Training Loss: 1.8684, Validation Loss: 1659.1211\n",
      "Epoch [6450/10000], Training Loss: 1.9210, Validation Loss: 2181.6477\n",
      "Epoch [6455/10000], Training Loss: 2.1118, Validation Loss: 2649.7405\n",
      "Epoch [6460/10000], Training Loss: 1.9426, Validation Loss: 2383.3162\n",
      "Epoch [6465/10000], Training Loss: 1.6625, Validation Loss: 2064.8906\n",
      "Epoch [6470/10000], Training Loss: 1.7964, Validation Loss: 1725.9706\n",
      "Epoch [6475/10000], Training Loss: 2.0240, Validation Loss: 2110.1636\n",
      "Epoch [6480/10000], Training Loss: 2.0592, Validation Loss: 2482.8691\n",
      "Epoch [6485/10000], Training Loss: 1.8764, Validation Loss: 2618.7175\n",
      "Epoch [6490/10000], Training Loss: 1.9013, Validation Loss: 2236.7429\n",
      "Epoch [6495/10000], Training Loss: 1.6181, Validation Loss: 1801.1178\n",
      "Epoch [6500/10000], Training Loss: 2.0480, Validation Loss: 2008.6655\n",
      "Epoch [6505/10000], Training Loss: 1.8251, Validation Loss: 2178.0764\n",
      "Epoch [6510/10000], Training Loss: 2.0371, Validation Loss: 2348.5164\n",
      "Epoch [6515/10000], Training Loss: 1.9332, Validation Loss: 2513.0469\n",
      "Epoch [6520/10000], Training Loss: 1.5859, Validation Loss: 2098.7371\n",
      "Epoch [6525/10000], Training Loss: 1.9281, Validation Loss: 1971.9611\n",
      "Epoch [6530/10000], Training Loss: 1.6511, Validation Loss: 2035.9941\n",
      "Epoch [6535/10000], Training Loss: 1.6120, Validation Loss: 2114.3264\n",
      "Epoch [6540/10000], Training Loss: 1.8533, Validation Loss: 2566.6548\n",
      "Epoch [6545/10000], Training Loss: 1.9647, Validation Loss: 3476.4075\n",
      "Epoch [6550/10000], Training Loss: 2.1614, Validation Loss: 3765.7471\n",
      "Epoch [6555/10000], Training Loss: 1.8831, Validation Loss: 3072.2358\n",
      "Epoch [6560/10000], Training Loss: 1.8669, Validation Loss: 2471.0066\n",
      "Epoch [6565/10000], Training Loss: 2.0743, Validation Loss: 2220.8330\n",
      "Epoch [6570/10000], Training Loss: 1.9745, Validation Loss: 1930.8572\n",
      "Epoch [6575/10000], Training Loss: 1.9900, Validation Loss: 1951.2229\n",
      "Epoch [6580/10000], Training Loss: 2.0519, Validation Loss: 2419.9143\n",
      "Epoch [6585/10000], Training Loss: 1.8169, Validation Loss: 2880.3140\n",
      "Epoch [6590/10000], Training Loss: 2.0221, Validation Loss: 2651.0139\n",
      "Epoch [6595/10000], Training Loss: 1.7811, Validation Loss: 1995.9795\n",
      "Epoch [6600/10000], Training Loss: 1.8407, Validation Loss: 2231.3950\n",
      "Epoch [6605/10000], Training Loss: 1.8786, Validation Loss: 2316.5396\n",
      "Epoch [6610/10000], Training Loss: 1.9264, Validation Loss: 2556.6890\n",
      "Epoch [6615/10000], Training Loss: 1.9703, Validation Loss: 2950.0796\n",
      "Epoch [6620/10000], Training Loss: 1.7460, Validation Loss: 2527.1106\n",
      "Epoch [6625/10000], Training Loss: 1.6002, Validation Loss: 2716.5317\n",
      "Epoch [6630/10000], Training Loss: 1.9624, Validation Loss: 2674.3574\n",
      "Epoch [6635/10000], Training Loss: 1.8558, Validation Loss: 2429.7986\n",
      "Epoch [6640/10000], Training Loss: 2.0926, Validation Loss: 2952.5786\n",
      "Epoch [6645/10000], Training Loss: 1.8767, Validation Loss: 4296.2471\n",
      "Epoch [6650/10000], Training Loss: 2.1926, Validation Loss: 3925.1130\n",
      "Epoch [6655/10000], Training Loss: 1.7630, Validation Loss: 3220.5647\n",
      "Epoch [6660/10000], Training Loss: 2.0190, Validation Loss: 4186.4966\n",
      "Epoch [6665/10000], Training Loss: 2.0514, Validation Loss: 3538.6575\n",
      "Epoch [6670/10000], Training Loss: 1.6804, Validation Loss: 2882.1130\n",
      "Epoch [6675/10000], Training Loss: 1.9619, Validation Loss: 3718.9866\n",
      "Epoch [6680/10000], Training Loss: 1.8521, Validation Loss: 3453.4119\n",
      "Epoch [6685/10000], Training Loss: 1.7527, Validation Loss: 3366.3909\n",
      "Epoch [6690/10000], Training Loss: 1.8123, Validation Loss: 4616.6201\n",
      "Epoch [6695/10000], Training Loss: 1.7619, Validation Loss: 4703.4565\n",
      "Epoch [6700/10000], Training Loss: 1.7973, Validation Loss: 4319.8223\n",
      "Epoch [6705/10000], Training Loss: 1.7752, Validation Loss: 2922.5813\n",
      "Epoch [6710/10000], Training Loss: 1.9238, Validation Loss: 2504.4915\n",
      "Epoch [6715/10000], Training Loss: 1.8335, Validation Loss: 2961.0664\n",
      "Epoch [6720/10000], Training Loss: 1.7442, Validation Loss: 4084.5793\n",
      "Epoch [6725/10000], Training Loss: 1.7679, Validation Loss: 5313.6812\n",
      "Epoch [6730/10000], Training Loss: 2.1027, Validation Loss: 4517.2173\n",
      "Epoch [6735/10000], Training Loss: 1.8332, Validation Loss: 3446.2969\n",
      "Epoch [6740/10000], Training Loss: 1.8694, Validation Loss: 3643.1919\n",
      "Epoch [6745/10000], Training Loss: 1.8898, Validation Loss: 3307.9370\n",
      "Epoch [6750/10000], Training Loss: 1.7600, Validation Loss: 3211.0305\n",
      "Epoch [6755/10000], Training Loss: 1.8227, Validation Loss: 3030.4099\n",
      "Epoch [6760/10000], Training Loss: 1.6974, Validation Loss: 3014.4058\n",
      "Epoch [6765/10000], Training Loss: 1.6826, Validation Loss: 3032.4050\n",
      "Epoch [6770/10000], Training Loss: 1.7896, Validation Loss: 2790.6736\n",
      "Epoch [6775/10000], Training Loss: 1.8919, Validation Loss: 3008.6499\n",
      "Epoch [6780/10000], Training Loss: 2.0065, Validation Loss: 3381.0347\n",
      "Epoch [6785/10000], Training Loss: 2.2589, Validation Loss: 3086.0139\n",
      "Epoch [6790/10000], Training Loss: 1.8019, Validation Loss: 3993.7615\n",
      "Epoch [6795/10000], Training Loss: 1.7883, Validation Loss: 4321.1597\n",
      "Epoch [6800/10000], Training Loss: 1.9171, Validation Loss: 4817.0225\n",
      "Epoch [6805/10000], Training Loss: 1.8603, Validation Loss: 4938.5049\n",
      "Epoch [6810/10000], Training Loss: 2.0982, Validation Loss: 5136.4731\n",
      "Epoch [6815/10000], Training Loss: 1.4543, Validation Loss: 3980.8862\n",
      "Epoch [6820/10000], Training Loss: 1.6911, Validation Loss: 2860.5215\n",
      "Epoch [6825/10000], Training Loss: 1.7275, Validation Loss: 2231.6919\n",
      "Epoch [6830/10000], Training Loss: 1.9344, Validation Loss: 1817.9484\n",
      "Epoch [6835/10000], Training Loss: 1.8006, Validation Loss: 2428.3250\n",
      "Epoch [6840/10000], Training Loss: 1.8856, Validation Loss: 3928.7334\n",
      "Epoch [6845/10000], Training Loss: 1.8775, Validation Loss: 4674.9653\n",
      "Epoch [6850/10000], Training Loss: 1.7563, Validation Loss: 4700.4829\n",
      "Epoch [6855/10000], Training Loss: 1.6236, Validation Loss: 5421.5581\n",
      "Epoch [6860/10000], Training Loss: 1.8162, Validation Loss: 4965.7788\n",
      "Epoch [6865/10000], Training Loss: 1.9677, Validation Loss: 4097.1953\n",
      "Epoch [6870/10000], Training Loss: 1.8781, Validation Loss: 4861.1021\n",
      "Epoch [6875/10000], Training Loss: 1.6986, Validation Loss: 4394.6382\n",
      "Epoch [6880/10000], Training Loss: 1.5928, Validation Loss: 4158.2363\n",
      "Epoch [6885/10000], Training Loss: 1.9116, Validation Loss: 4288.5347\n",
      "Epoch [6890/10000], Training Loss: 1.7912, Validation Loss: 3653.4761\n",
      "Epoch [6895/10000], Training Loss: 1.7553, Validation Loss: 3807.8171\n",
      "Epoch [6900/10000], Training Loss: 1.8267, Validation Loss: 3415.1042\n",
      "Epoch [6905/10000], Training Loss: 1.8509, Validation Loss: 2946.7805\n",
      "Epoch [6910/10000], Training Loss: 1.7262, Validation Loss: 3684.6062\n",
      "Epoch [6915/10000], Training Loss: 1.8102, Validation Loss: 3592.6619\n",
      "Epoch [6920/10000], Training Loss: 2.1115, Validation Loss: 2540.5420\n",
      "Epoch [6925/10000], Training Loss: 1.8824, Validation Loss: 2974.3428\n",
      "Epoch [6930/10000], Training Loss: 1.6154, Validation Loss: 2732.3877\n",
      "Epoch [6935/10000], Training Loss: 2.1881, Validation Loss: 3401.4995\n",
      "Epoch [6940/10000], Training Loss: 1.9065, Validation Loss: 2761.9546\n",
      "Epoch [6945/10000], Training Loss: 1.8060, Validation Loss: 2453.4351\n",
      "Epoch [6950/10000], Training Loss: 1.8903, Validation Loss: 2707.2896\n",
      "Epoch [6955/10000], Training Loss: 1.9646, Validation Loss: 2354.0378\n",
      "Epoch [6960/10000], Training Loss: 2.2835, Validation Loss: 2657.7246\n",
      "Epoch [6965/10000], Training Loss: 1.9673, Validation Loss: 2348.3435\n",
      "Epoch [6970/10000], Training Loss: 1.7271, Validation Loss: 2275.0764\n",
      "Epoch [6975/10000], Training Loss: 1.6326, Validation Loss: 3376.5781\n",
      "Epoch [6980/10000], Training Loss: 1.7242, Validation Loss: 3970.4165\n",
      "Epoch [6985/10000], Training Loss: 1.7884, Validation Loss: 3260.8096\n",
      "Epoch [6990/10000], Training Loss: 1.5554, Validation Loss: 3952.1570\n",
      "Epoch [6995/10000], Training Loss: 1.8829, Validation Loss: 5431.7344\n",
      "Epoch [7000/10000], Training Loss: 1.8108, Validation Loss: 5075.2798\n",
      "Epoch [7005/10000], Training Loss: 1.5927, Validation Loss: 4825.2764\n",
      "Epoch [7010/10000], Training Loss: 1.6550, Validation Loss: 3169.9023\n",
      "Epoch [7015/10000], Training Loss: 1.7415, Validation Loss: 2350.3811\n",
      "Epoch [7020/10000], Training Loss: 1.8671, Validation Loss: 2570.3755\n",
      "Epoch [7025/10000], Training Loss: 1.7715, Validation Loss: 3624.3003\n",
      "Epoch [7030/10000], Training Loss: 1.7640, Validation Loss: 3212.1685\n",
      "Epoch [7035/10000], Training Loss: 1.7715, Validation Loss: 4095.7952\n",
      "Epoch [7040/10000], Training Loss: 1.8630, Validation Loss: 4375.7607\n",
      "Epoch [7045/10000], Training Loss: 1.8074, Validation Loss: 4149.7988\n",
      "Epoch [7050/10000], Training Loss: 1.7095, Validation Loss: 3288.0291\n",
      "Epoch [7055/10000], Training Loss: 1.7596, Validation Loss: 3984.4219\n",
      "Epoch [7060/10000], Training Loss: 1.8033, Validation Loss: 4403.6436\n",
      "Epoch [7065/10000], Training Loss: 1.5726, Validation Loss: 4264.8892\n",
      "Epoch [7070/10000], Training Loss: 1.6198, Validation Loss: 3660.2808\n",
      "Epoch [7075/10000], Training Loss: 1.6261, Validation Loss: 2277.6147\n",
      "Epoch [7080/10000], Training Loss: 1.5618, Validation Loss: 2414.3452\n",
      "Epoch [7085/10000], Training Loss: 2.0317, Validation Loss: 3006.4009\n",
      "Epoch [7090/10000], Training Loss: 1.5890, Validation Loss: 4228.0942\n",
      "Epoch [7095/10000], Training Loss: 1.8477, Validation Loss: 6449.0566\n",
      "Epoch [7100/10000], Training Loss: 1.7372, Validation Loss: 6858.2812\n",
      "Epoch [7105/10000], Training Loss: 1.7577, Validation Loss: 4921.4829\n",
      "Epoch [7110/10000], Training Loss: 1.7657, Validation Loss: 3349.9866\n",
      "Epoch [7115/10000], Training Loss: 1.8804, Validation Loss: 3024.1594\n",
      "Epoch [7120/10000], Training Loss: 1.5818, Validation Loss: 3714.1267\n",
      "Epoch [7125/10000], Training Loss: 1.6075, Validation Loss: 4249.4722\n",
      "Epoch [7130/10000], Training Loss: 1.9279, Validation Loss: 4905.8467\n",
      "Epoch [7135/10000], Training Loss: 1.8557, Validation Loss: 5307.6514\n",
      "Epoch [7140/10000], Training Loss: 1.5142, Validation Loss: 4842.9121\n",
      "Epoch [7145/10000], Training Loss: 1.9531, Validation Loss: 4507.1484\n",
      "Epoch [7150/10000], Training Loss: 1.7548, Validation Loss: 3591.4421\n",
      "Epoch [7155/10000], Training Loss: 1.9592, Validation Loss: 3405.7422\n",
      "Epoch [7160/10000], Training Loss: 1.6149, Validation Loss: 3330.7778\n",
      "Epoch [7165/10000], Training Loss: 1.8337, Validation Loss: 3631.4385\n",
      "Epoch [7170/10000], Training Loss: 1.8037, Validation Loss: 3557.4011\n",
      "Epoch [7175/10000], Training Loss: 1.8917, Validation Loss: 3297.9519\n",
      "Epoch [7180/10000], Training Loss: 1.6093, Validation Loss: 3419.5295\n",
      "Epoch [7185/10000], Training Loss: 1.8667, Validation Loss: 4789.3154\n",
      "Epoch [7190/10000], Training Loss: 1.6657, Validation Loss: 6144.8311\n",
      "Epoch [7195/10000], Training Loss: 1.6669, Validation Loss: 4473.3936\n",
      "Epoch [7200/10000], Training Loss: 1.4926, Validation Loss: 3399.3362\n",
      "Epoch [7205/10000], Training Loss: 1.6334, Validation Loss: 3894.3293\n",
      "Epoch [7210/10000], Training Loss: 1.6576, Validation Loss: 3239.1519\n",
      "Epoch [7215/10000], Training Loss: 1.7910, Validation Loss: 2822.6758\n",
      "Epoch [7220/10000], Training Loss: 1.7863, Validation Loss: 3419.1108\n",
      "Epoch [7225/10000], Training Loss: 1.6403, Validation Loss: 3366.0225\n",
      "Epoch [7230/10000], Training Loss: 1.8883, Validation Loss: 3595.8337\n",
      "Epoch [7235/10000], Training Loss: 1.9364, Validation Loss: 4156.2588\n",
      "Epoch [7240/10000], Training Loss: 1.8204, Validation Loss: 4166.2773\n",
      "Epoch [7245/10000], Training Loss: 2.0890, Validation Loss: 3637.5095\n",
      "Epoch [7250/10000], Training Loss: 1.8488, Validation Loss: 3526.8850\n",
      "Epoch [7255/10000], Training Loss: 1.6320, Validation Loss: 3078.3459\n",
      "Epoch [7260/10000], Training Loss: 1.6557, Validation Loss: 4822.3848\n",
      "Epoch [7265/10000], Training Loss: 1.5332, Validation Loss: 4881.0903\n",
      "Epoch [7270/10000], Training Loss: 1.8195, Validation Loss: 3597.9690\n",
      "Epoch [7275/10000], Training Loss: 1.8792, Validation Loss: 4832.2861\n",
      "Epoch [7280/10000], Training Loss: 1.5414, Validation Loss: 6336.3154\n",
      "Epoch [7285/10000], Training Loss: 1.5465, Validation Loss: 5929.1973\n",
      "Epoch [7290/10000], Training Loss: 1.7439, Validation Loss: 5706.9644\n",
      "Epoch [7295/10000], Training Loss: 1.8492, Validation Loss: 5954.3672\n",
      "Epoch [7300/10000], Training Loss: 1.7146, Validation Loss: 5128.6709\n",
      "Epoch [7305/10000], Training Loss: 1.5844, Validation Loss: 4912.9302\n",
      "Epoch [7310/10000], Training Loss: 1.8992, Validation Loss: 4874.2559\n",
      "Epoch [7315/10000], Training Loss: 1.4989, Validation Loss: 6158.8813\n",
      "Epoch [7320/10000], Training Loss: 1.6189, Validation Loss: 5844.2603\n",
      "Epoch [7325/10000], Training Loss: 1.5706, Validation Loss: 3876.5461\n",
      "Epoch [7330/10000], Training Loss: 1.7486, Validation Loss: 2881.2568\n",
      "Epoch [7335/10000], Training Loss: 1.4821, Validation Loss: 2351.7927\n",
      "Epoch [7340/10000], Training Loss: 1.7003, Validation Loss: 2970.5430\n",
      "Epoch [7345/10000], Training Loss: 1.4188, Validation Loss: 3516.2183\n",
      "Epoch [7350/10000], Training Loss: 1.4796, Validation Loss: 4122.3647\n",
      "Epoch [7355/10000], Training Loss: 1.8680, Validation Loss: 4597.0933\n",
      "Epoch [7360/10000], Training Loss: 1.6913, Validation Loss: 4414.3579\n",
      "Epoch [7365/10000], Training Loss: 1.6081, Validation Loss: 6642.9360\n",
      "Epoch [7370/10000], Training Loss: 1.5405, Validation Loss: 7511.7017\n",
      "Epoch [7375/10000], Training Loss: 2.0257, Validation Loss: 6678.4521\n",
      "Epoch [7380/10000], Training Loss: 1.6151, Validation Loss: 5128.5200\n",
      "Epoch [7385/10000], Training Loss: 1.6743, Validation Loss: 4529.0864\n",
      "Epoch [7390/10000], Training Loss: 1.6826, Validation Loss: 6041.8315\n",
      "Epoch [7395/10000], Training Loss: 1.8248, Validation Loss: 5552.0112\n",
      "Epoch [7400/10000], Training Loss: 1.6814, Validation Loss: 4533.1572\n",
      "Epoch [7405/10000], Training Loss: 1.6458, Validation Loss: 4181.2363\n",
      "Epoch [7410/10000], Training Loss: 1.6924, Validation Loss: 5922.5801\n",
      "Epoch [7415/10000], Training Loss: 1.9963, Validation Loss: 6954.1528\n",
      "Epoch [7420/10000], Training Loss: 1.6530, Validation Loss: 4944.7969\n",
      "Epoch [7425/10000], Training Loss: 1.4611, Validation Loss: 5073.1582\n",
      "Epoch [7430/10000], Training Loss: 1.7700, Validation Loss: 6885.4653\n",
      "Epoch [7435/10000], Training Loss: 1.8153, Validation Loss: 7127.7373\n",
      "Epoch [7440/10000], Training Loss: 1.4833, Validation Loss: 6677.9463\n",
      "Epoch [7445/10000], Training Loss: 1.7523, Validation Loss: 6850.8560\n",
      "Epoch [7450/10000], Training Loss: 1.7134, Validation Loss: 7803.2930\n",
      "Epoch [7455/10000], Training Loss: 1.6956, Validation Loss: 9215.0508\n",
      "Epoch [7460/10000], Training Loss: 1.8097, Validation Loss: 9936.4502\n",
      "Epoch [7465/10000], Training Loss: 1.6710, Validation Loss: 6891.1768\n",
      "Epoch [7470/10000], Training Loss: 1.9167, Validation Loss: 4634.9253\n",
      "Epoch [7475/10000], Training Loss: 1.9328, Validation Loss: 3629.1921\n",
      "Epoch [7480/10000], Training Loss: 1.7678, Validation Loss: 4375.9487\n",
      "Epoch [7485/10000], Training Loss: 1.4813, Validation Loss: 8671.7793\n",
      "Epoch [7490/10000], Training Loss: 1.8592, Validation Loss: 10524.1738\n",
      "Epoch [7495/10000], Training Loss: 1.5178, Validation Loss: 9834.3867\n",
      "Epoch [7500/10000], Training Loss: 1.6566, Validation Loss: 11438.8877\n",
      "Epoch [7505/10000], Training Loss: 1.4961, Validation Loss: 13278.9736\n",
      "Epoch [7510/10000], Training Loss: 1.6846, Validation Loss: 11511.4482\n",
      "Epoch [7515/10000], Training Loss: 1.5637, Validation Loss: 9245.0273\n",
      "Epoch [7520/10000], Training Loss: 1.5282, Validation Loss: 12740.5225\n",
      "Epoch [7525/10000], Training Loss: 1.9019, Validation Loss: 11552.5537\n",
      "Epoch [7530/10000], Training Loss: 1.8352, Validation Loss: 11118.8877\n",
      "Epoch [7535/10000], Training Loss: 1.7227, Validation Loss: 10179.1309\n",
      "Epoch [7540/10000], Training Loss: 1.5627, Validation Loss: 7668.9277\n",
      "Epoch [7545/10000], Training Loss: 1.8171, Validation Loss: 6954.9165\n",
      "Epoch [7550/10000], Training Loss: 1.7609, Validation Loss: 5540.5068\n",
      "Epoch [7555/10000], Training Loss: 1.6518, Validation Loss: 5052.5332\n",
      "Epoch [7560/10000], Training Loss: 1.9387, Validation Loss: 7319.9062\n",
      "Epoch [7565/10000], Training Loss: 1.6878, Validation Loss: 9690.0312\n",
      "Epoch [7570/10000], Training Loss: 1.5697, Validation Loss: 15698.1992\n",
      "Epoch [7575/10000], Training Loss: 1.7091, Validation Loss: 17379.4473\n",
      "Epoch [7580/10000], Training Loss: 1.5294, Validation Loss: 17235.1738\n",
      "Epoch [7585/10000], Training Loss: 1.6823, Validation Loss: 11621.1582\n",
      "Epoch [7590/10000], Training Loss: 1.7544, Validation Loss: 8890.4814\n",
      "Epoch [7595/10000], Training Loss: 1.5826, Validation Loss: 14499.5000\n",
      "Epoch [7600/10000], Training Loss: 1.6631, Validation Loss: 17090.2676\n",
      "Epoch [7605/10000], Training Loss: 1.7006, Validation Loss: 20434.3965\n",
      "Epoch [7610/10000], Training Loss: 1.9212, Validation Loss: 16510.5879\n",
      "Epoch [7615/10000], Training Loss: 1.8656, Validation Loss: 12725.2500\n",
      "Epoch [7620/10000], Training Loss: 1.5846, Validation Loss: 13625.3760\n",
      "Epoch [7625/10000], Training Loss: 1.9054, Validation Loss: 20609.2754\n",
      "Epoch [7630/10000], Training Loss: 1.6013, Validation Loss: 27037.3066\n",
      "Epoch [7635/10000], Training Loss: 1.6530, Validation Loss: 24836.1152\n",
      "Epoch [7640/10000], Training Loss: 1.8881, Validation Loss: 27259.3926\n",
      "Epoch [7645/10000], Training Loss: 1.9582, Validation Loss: 26108.5918\n",
      "Epoch [7650/10000], Training Loss: 1.6515, Validation Loss: 24823.5469\n",
      "Epoch [7655/10000], Training Loss: 1.9800, Validation Loss: 15719.6562\n",
      "Epoch [7660/10000], Training Loss: 1.8701, Validation Loss: 10402.6279\n",
      "Epoch [7665/10000], Training Loss: 1.5575, Validation Loss: 10684.6348\n",
      "Epoch [7670/10000], Training Loss: 1.6845, Validation Loss: 12748.2637\n",
      "Epoch [7675/10000], Training Loss: 1.7551, Validation Loss: 13558.6934\n",
      "Epoch [7680/10000], Training Loss: 1.9968, Validation Loss: 16022.9609\n",
      "Epoch [7685/10000], Training Loss: 1.6749, Validation Loss: 14706.0156\n",
      "Epoch [7690/10000], Training Loss: 1.5352, Validation Loss: 19030.9766\n",
      "Epoch [7695/10000], Training Loss: 1.6037, Validation Loss: 18492.6367\n",
      "Epoch [7700/10000], Training Loss: 1.7032, Validation Loss: 15332.1348\n",
      "Epoch [7705/10000], Training Loss: 1.5277, Validation Loss: 17354.4844\n",
      "Epoch [7710/10000], Training Loss: 1.7362, Validation Loss: 16866.9199\n",
      "Epoch [7715/10000], Training Loss: 1.8629, Validation Loss: 18032.2617\n",
      "Epoch [7720/10000], Training Loss: 2.2083, Validation Loss: 15947.9883\n",
      "Epoch [7725/10000], Training Loss: 1.4979, Validation Loss: 15294.7031\n",
      "Epoch [7730/10000], Training Loss: 1.7849, Validation Loss: 14236.6875\n",
      "Epoch [7735/10000], Training Loss: 1.8029, Validation Loss: 22096.2871\n",
      "Epoch [7740/10000], Training Loss: 1.8037, Validation Loss: 27846.6777\n",
      "Epoch [7745/10000], Training Loss: 1.3946, Validation Loss: 33395.1562\n",
      "Epoch [7750/10000], Training Loss: 1.7347, Validation Loss: 28016.0469\n",
      "Epoch [7755/10000], Training Loss: 1.5355, Validation Loss: 13727.1934\n",
      "Epoch [7760/10000], Training Loss: 1.6231, Validation Loss: 19184.1367\n",
      "Epoch [7765/10000], Training Loss: 1.6192, Validation Loss: 33940.3203\n",
      "Epoch [7770/10000], Training Loss: 1.9286, Validation Loss: 27319.9668\n",
      "Epoch [7775/10000], Training Loss: 1.5164, Validation Loss: 18615.2305\n",
      "Epoch [7780/10000], Training Loss: 1.6992, Validation Loss: 16098.4609\n",
      "Epoch [7785/10000], Training Loss: 1.5571, Validation Loss: 12032.5059\n",
      "Epoch [7790/10000], Training Loss: 1.7178, Validation Loss: 12879.1494\n",
      "Epoch [7795/10000], Training Loss: 1.7960, Validation Loss: 12761.7822\n",
      "Epoch [7800/10000], Training Loss: 1.4565, Validation Loss: 17795.5391\n",
      "Epoch [7805/10000], Training Loss: 1.7801, Validation Loss: 22866.9844\n",
      "Epoch [7810/10000], Training Loss: 1.6917, Validation Loss: 22227.1289\n",
      "Epoch [7815/10000], Training Loss: 1.8119, Validation Loss: 26384.7363\n",
      "Epoch [7820/10000], Training Loss: 1.4640, Validation Loss: 22885.3438\n",
      "Epoch [7825/10000], Training Loss: 1.7893, Validation Loss: 18524.2402\n",
      "Epoch [7830/10000], Training Loss: 1.7850, Validation Loss: 20965.5664\n",
      "Epoch [7835/10000], Training Loss: 1.8593, Validation Loss: 18023.1465\n",
      "Epoch [7840/10000], Training Loss: 1.8184, Validation Loss: 12958.5693\n",
      "Epoch [7845/10000], Training Loss: 1.8306, Validation Loss: 20026.3652\n",
      "Epoch [7850/10000], Training Loss: 1.8330, Validation Loss: 21429.2461\n",
      "Epoch [7855/10000], Training Loss: 1.8895, Validation Loss: 22836.6289\n",
      "Epoch [7860/10000], Training Loss: 1.7432, Validation Loss: 16667.8789\n",
      "Epoch [7865/10000], Training Loss: 1.5626, Validation Loss: 13738.0322\n",
      "Epoch [7870/10000], Training Loss: 1.5825, Validation Loss: 15187.7012\n",
      "Epoch [7875/10000], Training Loss: 1.6156, Validation Loss: 22320.7461\n",
      "Epoch [7880/10000], Training Loss: 1.5395, Validation Loss: 21419.1367\n",
      "Epoch [7885/10000], Training Loss: 1.8068, Validation Loss: 17118.9316\n",
      "Epoch [7890/10000], Training Loss: 1.8684, Validation Loss: 19575.4629\n",
      "Epoch [7895/10000], Training Loss: 1.6545, Validation Loss: 19497.2949\n",
      "Epoch [7900/10000], Training Loss: 1.7705, Validation Loss: 18318.0312\n",
      "Epoch [7905/10000], Training Loss: 1.7641, Validation Loss: 16274.4932\n",
      "Epoch [7910/10000], Training Loss: 1.9237, Validation Loss: 12040.9893\n",
      "Epoch [7915/10000], Training Loss: 1.6438, Validation Loss: 13266.3125\n",
      "Epoch [7920/10000], Training Loss: 1.6851, Validation Loss: 12255.7227\n",
      "Epoch [7925/10000], Training Loss: 1.6693, Validation Loss: 12673.1523\n",
      "Epoch [7930/10000], Training Loss: 1.9637, Validation Loss: 14976.8779\n",
      "Epoch [7935/10000], Training Loss: 1.5783, Validation Loss: 10639.8408\n",
      "Epoch [7940/10000], Training Loss: 1.6689, Validation Loss: 9368.6074\n",
      "Epoch [7945/10000], Training Loss: 1.5007, Validation Loss: 9176.3799\n",
      "Epoch [7950/10000], Training Loss: 1.6493, Validation Loss: 9634.9082\n",
      "Epoch [7955/10000], Training Loss: 1.7705, Validation Loss: 15752.1357\n",
      "Epoch [7960/10000], Training Loss: 1.6257, Validation Loss: 26368.4570\n",
      "Epoch [7965/10000], Training Loss: 1.6452, Validation Loss: 27226.3145\n",
      "Epoch [7970/10000], Training Loss: 1.6460, Validation Loss: 26839.4902\n",
      "Epoch [7975/10000], Training Loss: 1.8059, Validation Loss: 21348.5352\n",
      "Epoch [7980/10000], Training Loss: 1.7679, Validation Loss: 20134.1250\n",
      "Epoch [7985/10000], Training Loss: 1.7749, Validation Loss: 16707.4668\n",
      "Epoch [7990/10000], Training Loss: 1.6282, Validation Loss: 17008.4238\n",
      "Epoch [7995/10000], Training Loss: 1.7733, Validation Loss: 21099.1367\n",
      "Epoch [8000/10000], Training Loss: 1.6832, Validation Loss: 17561.3496\n",
      "Epoch [8005/10000], Training Loss: 1.5768, Validation Loss: 25401.5039\n",
      "Epoch [8010/10000], Training Loss: 2.0450, Validation Loss: 20631.0332\n",
      "Epoch [8015/10000], Training Loss: 1.9971, Validation Loss: 17664.7812\n",
      "Epoch [8020/10000], Training Loss: 1.7921, Validation Loss: 18247.3203\n",
      "Epoch [8025/10000], Training Loss: 1.6338, Validation Loss: 19379.8906\n",
      "Epoch [8030/10000], Training Loss: 1.6059, Validation Loss: 21811.8027\n",
      "Epoch [8035/10000], Training Loss: 1.6359, Validation Loss: 36310.3750\n",
      "Epoch [8040/10000], Training Loss: 1.6489, Validation Loss: 35211.6758\n",
      "Epoch [8045/10000], Training Loss: 1.5410, Validation Loss: 34386.5078\n",
      "Epoch [8050/10000], Training Loss: 1.6510, Validation Loss: 28989.8340\n",
      "Epoch [8055/10000], Training Loss: 1.7147, Validation Loss: 21890.0820\n",
      "Epoch [8060/10000], Training Loss: 1.8072, Validation Loss: 16261.2900\n",
      "Epoch [8065/10000], Training Loss: 1.3336, Validation Loss: 19236.2676\n",
      "Epoch [8070/10000], Training Loss: 1.7571, Validation Loss: 19365.4258\n",
      "Epoch [8075/10000], Training Loss: 1.6333, Validation Loss: 22383.0293\n",
      "Epoch [8080/10000], Training Loss: 1.5329, Validation Loss: 22311.6641\n",
      "Epoch [8085/10000], Training Loss: 1.5407, Validation Loss: 22210.1211\n",
      "Epoch [8090/10000], Training Loss: 1.6169, Validation Loss: 26037.6543\n",
      "Epoch [8095/10000], Training Loss: 1.4643, Validation Loss: 30694.8066\n",
      "Epoch [8100/10000], Training Loss: 1.4941, Validation Loss: 24873.0293\n",
      "Epoch [8105/10000], Training Loss: 1.6738, Validation Loss: 23314.9746\n",
      "Epoch [8110/10000], Training Loss: 1.5315, Validation Loss: 19794.5527\n",
      "Epoch [8115/10000], Training Loss: 1.6562, Validation Loss: 18426.0410\n",
      "Epoch [8120/10000], Training Loss: 1.8534, Validation Loss: 16321.8643\n",
      "Epoch [8125/10000], Training Loss: 1.5498, Validation Loss: 11996.1133\n",
      "Epoch [8130/10000], Training Loss: 1.4813, Validation Loss: 10113.8994\n",
      "Epoch [8135/10000], Training Loss: 1.5185, Validation Loss: 11710.3623\n",
      "Epoch [8140/10000], Training Loss: 1.5901, Validation Loss: 12252.0801\n",
      "Epoch [8145/10000], Training Loss: 1.8625, Validation Loss: 10871.7480\n",
      "Epoch [8150/10000], Training Loss: 1.7984, Validation Loss: 16488.3301\n",
      "Epoch [8155/10000], Training Loss: 1.6351, Validation Loss: 22674.8398\n",
      "Epoch [8160/10000], Training Loss: 1.5737, Validation Loss: 20252.5918\n",
      "Epoch [8165/10000], Training Loss: 1.4813, Validation Loss: 15390.0742\n",
      "Epoch [8170/10000], Training Loss: 1.6450, Validation Loss: 16033.5400\n",
      "Epoch [8175/10000], Training Loss: 1.7755, Validation Loss: 19311.5586\n",
      "Epoch [8180/10000], Training Loss: 1.5552, Validation Loss: 21942.8750\n",
      "Epoch [8185/10000], Training Loss: 1.4099, Validation Loss: 21345.7500\n",
      "Epoch [8190/10000], Training Loss: 1.5723, Validation Loss: 29177.6348\n",
      "Epoch [8195/10000], Training Loss: 1.4752, Validation Loss: 24184.8262\n",
      "Epoch [8200/10000], Training Loss: 1.3090, Validation Loss: 25472.0898\n",
      "Epoch [8205/10000], Training Loss: 1.6186, Validation Loss: 19647.4238\n",
      "Epoch [8210/10000], Training Loss: 1.5389, Validation Loss: 18851.7246\n",
      "Epoch [8215/10000], Training Loss: 1.5350, Validation Loss: 21344.0020\n",
      "Epoch [8220/10000], Training Loss: 1.6498, Validation Loss: 16933.3164\n",
      "Epoch [8225/10000], Training Loss: 1.4338, Validation Loss: 16010.9619\n",
      "Epoch [8230/10000], Training Loss: 1.5028, Validation Loss: 15087.6270\n",
      "Epoch [8235/10000], Training Loss: 1.7136, Validation Loss: 17280.7734\n",
      "Epoch [8240/10000], Training Loss: 1.6559, Validation Loss: 19854.6699\n",
      "Epoch [8245/10000], Training Loss: 1.7778, Validation Loss: 24415.5020\n",
      "Epoch [8250/10000], Training Loss: 1.6731, Validation Loss: 27124.9121\n",
      "Epoch [8255/10000], Training Loss: 1.8988, Validation Loss: 23852.3945\n",
      "Epoch [8260/10000], Training Loss: 1.7731, Validation Loss: 15009.3047\n",
      "Epoch [8265/10000], Training Loss: 1.5805, Validation Loss: 13963.1445\n",
      "Epoch [8270/10000], Training Loss: 1.8908, Validation Loss: 17612.3203\n",
      "Epoch [8275/10000], Training Loss: 1.5317, Validation Loss: 18508.2852\n",
      "Epoch [8280/10000], Training Loss: 1.4949, Validation Loss: 12254.5352\n",
      "Epoch [8285/10000], Training Loss: 1.4988, Validation Loss: 12368.2070\n",
      "Epoch [8290/10000], Training Loss: 1.9247, Validation Loss: 15094.3711\n",
      "Epoch [8295/10000], Training Loss: 1.5711, Validation Loss: 15994.2598\n",
      "Epoch [8300/10000], Training Loss: 1.4681, Validation Loss: 24406.8223\n",
      "Epoch [8305/10000], Training Loss: 1.5991, Validation Loss: 35432.7695\n",
      "Epoch [8310/10000], Training Loss: 1.8216, Validation Loss: 33014.0000\n",
      "Epoch [8315/10000], Training Loss: 1.7435, Validation Loss: 24236.8770\n",
      "Epoch [8320/10000], Training Loss: 1.8142, Validation Loss: 20333.0176\n",
      "Epoch [8325/10000], Training Loss: 1.5779, Validation Loss: 23622.3652\n",
      "Epoch [8330/10000], Training Loss: 1.5822, Validation Loss: 24336.8848\n",
      "Epoch [8335/10000], Training Loss: 1.7514, Validation Loss: 26880.8516\n",
      "Epoch [8340/10000], Training Loss: 1.5270, Validation Loss: 25646.3125\n",
      "Epoch [8345/10000], Training Loss: 1.8992, Validation Loss: 23535.5176\n",
      "Epoch [8350/10000], Training Loss: 1.4610, Validation Loss: 19064.0840\n",
      "Epoch [8355/10000], Training Loss: 1.7787, Validation Loss: 21996.1953\n",
      "Epoch [8360/10000], Training Loss: 1.6434, Validation Loss: 34223.8516\n",
      "Epoch [8365/10000], Training Loss: 1.4870, Validation Loss: 35200.1953\n",
      "Epoch [8370/10000], Training Loss: 1.7603, Validation Loss: 25606.4863\n",
      "Epoch [8375/10000], Training Loss: 1.5498, Validation Loss: 22602.2129\n",
      "Epoch [8380/10000], Training Loss: 1.6552, Validation Loss: 23667.3770\n",
      "Epoch [8385/10000], Training Loss: 1.7611, Validation Loss: 20498.5527\n",
      "Epoch [8390/10000], Training Loss: 1.8112, Validation Loss: 20545.3809\n",
      "Epoch [8395/10000], Training Loss: 1.6088, Validation Loss: 21137.1367\n",
      "Epoch [8400/10000], Training Loss: 1.7356, Validation Loss: 16885.8398\n",
      "Epoch [8405/10000], Training Loss: 1.5090, Validation Loss: 18351.9629\n",
      "Epoch [8410/10000], Training Loss: 1.5584, Validation Loss: 17407.1328\n",
      "Epoch [8415/10000], Training Loss: 1.5636, Validation Loss: 17123.5586\n",
      "Epoch [8420/10000], Training Loss: 1.3555, Validation Loss: 23531.1035\n",
      "Epoch [8425/10000], Training Loss: 1.7472, Validation Loss: 33801.5156\n",
      "Epoch [8430/10000], Training Loss: 1.5999, Validation Loss: 30412.9414\n",
      "Epoch [8435/10000], Training Loss: 1.3853, Validation Loss: 28756.2441\n",
      "Epoch [8440/10000], Training Loss: 1.7279, Validation Loss: 20490.1426\n",
      "Epoch [8445/10000], Training Loss: 1.6114, Validation Loss: 19448.6680\n",
      "Epoch [8450/10000], Training Loss: 1.4965, Validation Loss: 24980.9453\n",
      "Epoch [8455/10000], Training Loss: 1.5119, Validation Loss: 24575.7402\n",
      "Epoch [8460/10000], Training Loss: 1.6772, Validation Loss: 20502.8340\n",
      "Epoch [8465/10000], Training Loss: 1.4743, Validation Loss: 18472.9512\n",
      "Epoch [8470/10000], Training Loss: 1.4910, Validation Loss: 16681.2617\n",
      "Epoch [8475/10000], Training Loss: 1.6560, Validation Loss: 23685.6113\n",
      "Epoch [8480/10000], Training Loss: 1.6796, Validation Loss: 30944.7266\n",
      "Epoch [8485/10000], Training Loss: 1.6964, Validation Loss: 41276.3945\n",
      "Epoch [8490/10000], Training Loss: 1.4492, Validation Loss: 41414.2188\n",
      "Epoch [8495/10000], Training Loss: 1.5167, Validation Loss: 31448.1895\n",
      "Epoch [8500/10000], Training Loss: 1.3741, Validation Loss: 23999.4824\n",
      "Epoch [8505/10000], Training Loss: 1.4261, Validation Loss: 22417.4727\n",
      "Epoch [8510/10000], Training Loss: 1.7431, Validation Loss: 21454.9512\n",
      "Epoch [8515/10000], Training Loss: 1.3682, Validation Loss: 19486.2500\n",
      "Epoch [8520/10000], Training Loss: 1.5348, Validation Loss: 21686.5391\n",
      "Epoch [8525/10000], Training Loss: 1.2331, Validation Loss: 17377.9102\n",
      "Epoch [8530/10000], Training Loss: 1.5798, Validation Loss: 21436.0527\n",
      "Epoch [8535/10000], Training Loss: 1.4773, Validation Loss: 29008.9297\n",
      "Epoch [8540/10000], Training Loss: 1.5095, Validation Loss: 28541.9512\n",
      "Epoch [8545/10000], Training Loss: 1.4291, Validation Loss: 18435.7891\n",
      "Epoch [8550/10000], Training Loss: 1.6416, Validation Loss: 19459.1699\n",
      "Epoch [8555/10000], Training Loss: 1.6643, Validation Loss: 13189.7734\n",
      "Epoch [8560/10000], Training Loss: 1.4380, Validation Loss: 11465.3125\n",
      "Epoch [8565/10000], Training Loss: 1.5147, Validation Loss: 10016.2412\n",
      "Epoch [8570/10000], Training Loss: 1.6293, Validation Loss: 11896.9111\n",
      "Epoch [8575/10000], Training Loss: 1.5078, Validation Loss: 11896.6113\n",
      "Epoch [8580/10000], Training Loss: 1.6345, Validation Loss: 9812.5928\n",
      "Epoch [8585/10000], Training Loss: 1.6861, Validation Loss: 14226.8408\n",
      "Epoch [8590/10000], Training Loss: 1.6088, Validation Loss: 17459.6211\n",
      "Epoch [8595/10000], Training Loss: 1.5624, Validation Loss: 15143.0342\n",
      "Epoch [8600/10000], Training Loss: 1.6241, Validation Loss: 14125.4756\n",
      "Epoch [8605/10000], Training Loss: 1.4168, Validation Loss: 18530.6641\n",
      "Epoch [8610/10000], Training Loss: 1.6896, Validation Loss: 21363.2227\n",
      "Epoch [8615/10000], Training Loss: 1.7540, Validation Loss: 18700.5332\n",
      "Epoch [8620/10000], Training Loss: 1.2770, Validation Loss: 15741.0088\n",
      "Epoch [8625/10000], Training Loss: 1.5361, Validation Loss: 17393.3027\n",
      "Epoch [8630/10000], Training Loss: 1.5022, Validation Loss: 16968.4629\n",
      "Epoch [8635/10000], Training Loss: 1.6088, Validation Loss: 19114.0176\n",
      "Epoch [8640/10000], Training Loss: 1.5174, Validation Loss: 22134.4668\n",
      "Epoch [8645/10000], Training Loss: 1.7378, Validation Loss: 26105.4062\n",
      "Epoch [8650/10000], Training Loss: 1.5797, Validation Loss: 25654.6074\n",
      "Epoch [8655/10000], Training Loss: 1.6449, Validation Loss: 27324.7344\n",
      "Epoch [8660/10000], Training Loss: 1.7998, Validation Loss: 22484.8945\n",
      "Epoch [8665/10000], Training Loss: 1.3964, Validation Loss: 27447.2852\n",
      "Epoch [8670/10000], Training Loss: 1.5171, Validation Loss: 32635.9922\n",
      "Epoch [8675/10000], Training Loss: 1.4882, Validation Loss: 42545.5703\n",
      "Epoch [8680/10000], Training Loss: 1.7346, Validation Loss: 46073.4453\n",
      "Epoch [8685/10000], Training Loss: 1.5164, Validation Loss: 43980.3984\n",
      "Epoch [8690/10000], Training Loss: 1.4688, Validation Loss: 25997.6113\n",
      "Epoch [8695/10000], Training Loss: 1.4278, Validation Loss: 14448.2500\n",
      "Epoch [8700/10000], Training Loss: 1.6993, Validation Loss: 14476.0859\n",
      "Epoch [8705/10000], Training Loss: 1.5787, Validation Loss: 17158.9414\n",
      "Epoch [8710/10000], Training Loss: 1.5357, Validation Loss: 20496.2402\n",
      "Epoch [8715/10000], Training Loss: 1.3203, Validation Loss: 20454.4824\n",
      "Epoch [8720/10000], Training Loss: 1.4660, Validation Loss: 18194.3828\n",
      "Epoch [8725/10000], Training Loss: 1.5790, Validation Loss: 16871.8242\n",
      "Epoch [8730/10000], Training Loss: 1.4856, Validation Loss: 18165.3164\n",
      "Epoch [8735/10000], Training Loss: 1.6905, Validation Loss: 13800.8926\n",
      "Epoch [8740/10000], Training Loss: 1.3706, Validation Loss: 17253.2930\n",
      "Epoch [8745/10000], Training Loss: 1.5370, Validation Loss: 24071.9629\n",
      "Epoch [8750/10000], Training Loss: 1.4268, Validation Loss: 33074.3477\n",
      "Epoch [8755/10000], Training Loss: 1.4016, Validation Loss: 24404.7461\n",
      "Epoch [8760/10000], Training Loss: 1.4931, Validation Loss: 18000.9512\n",
      "Epoch [8765/10000], Training Loss: 1.4681, Validation Loss: 18110.4727\n",
      "Epoch [8770/10000], Training Loss: 1.2779, Validation Loss: 20796.2637\n",
      "Epoch [8775/10000], Training Loss: 1.3946, Validation Loss: 23388.1426\n",
      "Epoch [8780/10000], Training Loss: 1.3894, Validation Loss: 20510.8555\n",
      "Epoch [8785/10000], Training Loss: 1.5573, Validation Loss: 27148.0742\n",
      "Epoch [8790/10000], Training Loss: 1.5673, Validation Loss: 24440.2051\n",
      "Epoch [8795/10000], Training Loss: 1.5334, Validation Loss: 22993.3223\n",
      "Epoch [8800/10000], Training Loss: 1.5020, Validation Loss: 18097.2598\n",
      "Epoch [8805/10000], Training Loss: 1.5293, Validation Loss: 17905.2246\n",
      "Epoch [8810/10000], Training Loss: 1.6410, Validation Loss: 17908.8516\n",
      "Epoch [8815/10000], Training Loss: 1.6285, Validation Loss: 20193.8809\n",
      "Epoch [8820/10000], Training Loss: 1.3669, Validation Loss: 25718.1543\n",
      "Epoch [8825/10000], Training Loss: 1.6078, Validation Loss: 28348.6719\n",
      "Epoch [8830/10000], Training Loss: 1.6303, Validation Loss: 33585.2227\n",
      "Epoch [8835/10000], Training Loss: 1.5220, Validation Loss: 26463.5117\n",
      "Epoch [8840/10000], Training Loss: 1.4594, Validation Loss: 18482.3105\n",
      "Epoch [8845/10000], Training Loss: 1.4990, Validation Loss: 13549.0664\n",
      "Epoch [8850/10000], Training Loss: 1.8132, Validation Loss: 14370.2568\n",
      "Epoch [8855/10000], Training Loss: 1.4733, Validation Loss: 17959.9980\n",
      "Epoch [8860/10000], Training Loss: 1.4913, Validation Loss: 21317.1816\n",
      "Epoch [8865/10000], Training Loss: 1.5638, Validation Loss: 22406.1289\n",
      "Epoch [8870/10000], Training Loss: 1.6532, Validation Loss: 23907.1445\n",
      "Epoch [8875/10000], Training Loss: 1.6211, Validation Loss: 26830.5469\n",
      "Epoch [8880/10000], Training Loss: 1.3616, Validation Loss: 22577.3574\n",
      "Epoch [8885/10000], Training Loss: 1.5031, Validation Loss: 21237.9844\n",
      "Epoch [8890/10000], Training Loss: 1.5362, Validation Loss: 20142.4707\n",
      "Epoch [8895/10000], Training Loss: 1.3834, Validation Loss: 19218.8066\n",
      "Epoch [8900/10000], Training Loss: 1.5834, Validation Loss: 22999.0840\n",
      "Epoch [8905/10000], Training Loss: 1.5910, Validation Loss: 22854.3535\n",
      "Epoch [8910/10000], Training Loss: 1.7423, Validation Loss: 22817.0020\n",
      "Epoch [8915/10000], Training Loss: 1.2797, Validation Loss: 21326.1172\n",
      "Epoch [8920/10000], Training Loss: 1.5885, Validation Loss: 25838.4141\n",
      "Epoch [8925/10000], Training Loss: 1.5839, Validation Loss: 22596.2930\n",
      "Epoch [8930/10000], Training Loss: 1.4916, Validation Loss: 24915.6953\n",
      "Epoch [8935/10000], Training Loss: 1.5240, Validation Loss: 23508.3438\n",
      "Epoch [8940/10000], Training Loss: 1.7040, Validation Loss: 20719.5527\n",
      "Epoch [8945/10000], Training Loss: 1.4200, Validation Loss: 18445.6113\n",
      "Epoch [8950/10000], Training Loss: 1.3626, Validation Loss: 18247.1113\n",
      "Epoch [8955/10000], Training Loss: 1.5466, Validation Loss: 17842.6758\n",
      "Epoch [8960/10000], Training Loss: 1.5700, Validation Loss: 15717.5850\n",
      "Epoch [8965/10000], Training Loss: 1.6201, Validation Loss: 11855.7744\n",
      "Epoch [8970/10000], Training Loss: 1.5764, Validation Loss: 10321.2412\n",
      "Epoch [8975/10000], Training Loss: 1.4485, Validation Loss: 10560.1094\n",
      "Epoch [8980/10000], Training Loss: 1.6235, Validation Loss: 13286.1484\n",
      "Epoch [8985/10000], Training Loss: 1.3894, Validation Loss: 13471.7109\n",
      "Epoch [8990/10000], Training Loss: 1.6607, Validation Loss: 15739.7920\n",
      "Epoch [8995/10000], Training Loss: 1.5371, Validation Loss: 16308.5449\n",
      "Epoch [9000/10000], Training Loss: 1.4483, Validation Loss: 15037.9551\n",
      "Epoch [9005/10000], Training Loss: 1.5543, Validation Loss: 13159.6934\n",
      "Epoch [9010/10000], Training Loss: 1.4910, Validation Loss: 11281.6035\n",
      "Epoch [9015/10000], Training Loss: 1.5674, Validation Loss: 11421.0146\n",
      "Epoch [9020/10000], Training Loss: 1.5878, Validation Loss: 11134.0938\n",
      "Epoch [9025/10000], Training Loss: 1.3712, Validation Loss: 12704.2920\n",
      "Epoch [9030/10000], Training Loss: 1.5690, Validation Loss: 12562.0000\n",
      "Epoch [9035/10000], Training Loss: 1.4923, Validation Loss: 12390.4893\n",
      "Epoch [9040/10000], Training Loss: 1.5140, Validation Loss: 13042.1709\n",
      "Epoch [9045/10000], Training Loss: 1.4279, Validation Loss: 18600.6855\n",
      "Epoch [9050/10000], Training Loss: 1.5523, Validation Loss: 21249.8145\n",
      "Epoch [9055/10000], Training Loss: 1.8852, Validation Loss: 24964.8828\n",
      "Epoch [9060/10000], Training Loss: 1.5226, Validation Loss: 27145.7441\n",
      "Epoch [9065/10000], Training Loss: 1.4876, Validation Loss: 25641.0664\n",
      "Epoch [9070/10000], Training Loss: 1.4895, Validation Loss: 20401.1797\n",
      "Epoch [9075/10000], Training Loss: 1.5197, Validation Loss: 19304.4688\n",
      "Epoch [9080/10000], Training Loss: 1.6706, Validation Loss: 18457.3301\n",
      "Epoch [9085/10000], Training Loss: 1.2294, Validation Loss: 18663.7051\n",
      "Epoch [9090/10000], Training Loss: 1.4537, Validation Loss: 21438.9219\n",
      "Epoch [9095/10000], Training Loss: 1.4556, Validation Loss: 24095.2090\n",
      "Epoch [9100/10000], Training Loss: 1.3081, Validation Loss: 23314.1660\n",
      "Epoch [9105/10000], Training Loss: 1.3281, Validation Loss: 24236.0586\n",
      "Epoch [9110/10000], Training Loss: 1.3157, Validation Loss: 20076.2695\n",
      "Epoch [9115/10000], Training Loss: 1.4772, Validation Loss: 20434.2266\n",
      "Epoch [9120/10000], Training Loss: 1.3128, Validation Loss: 23601.8965\n",
      "Epoch [9125/10000], Training Loss: 1.3631, Validation Loss: 23366.2812\n",
      "Epoch [9130/10000], Training Loss: 1.7293, Validation Loss: 13237.6230\n",
      "Epoch [9135/10000], Training Loss: 1.5602, Validation Loss: 9852.3867\n",
      "Epoch [9140/10000], Training Loss: 1.3231, Validation Loss: 14905.8574\n",
      "Epoch [9145/10000], Training Loss: 1.5020, Validation Loss: 16090.7529\n",
      "Epoch [9150/10000], Training Loss: 1.2791, Validation Loss: 15967.8555\n",
      "Epoch [9155/10000], Training Loss: 1.6662, Validation Loss: 15488.4014\n",
      "Epoch [9160/10000], Training Loss: 1.7216, Validation Loss: 11644.6143\n",
      "Epoch [9165/10000], Training Loss: 1.6386, Validation Loss: 10440.0645\n",
      "Epoch [9170/10000], Training Loss: 1.3737, Validation Loss: 12277.1846\n",
      "Epoch [9175/10000], Training Loss: 1.6073, Validation Loss: 16287.0186\n",
      "Epoch [9180/10000], Training Loss: 1.4096, Validation Loss: 18483.8555\n",
      "Epoch [9185/10000], Training Loss: 1.4693, Validation Loss: 15619.3945\n",
      "Epoch [9190/10000], Training Loss: 1.4668, Validation Loss: 14709.9258\n",
      "Epoch [9195/10000], Training Loss: 1.5539, Validation Loss: 15256.2520\n",
      "Epoch [9200/10000], Training Loss: 1.6831, Validation Loss: 16866.9922\n",
      "Epoch [9205/10000], Training Loss: 1.4839, Validation Loss: 13569.1582\n",
      "Epoch [9210/10000], Training Loss: 1.5264, Validation Loss: 10081.8037\n",
      "Epoch [9215/10000], Training Loss: 1.3811, Validation Loss: 12651.7520\n",
      "Epoch [9220/10000], Training Loss: 1.4052, Validation Loss: 15432.6299\n",
      "Epoch [9225/10000], Training Loss: 1.3279, Validation Loss: 23079.4043\n",
      "Epoch [9230/10000], Training Loss: 1.1556, Validation Loss: 31794.7148\n",
      "Epoch [9235/10000], Training Loss: 1.6879, Validation Loss: 29242.8418\n",
      "Epoch [9240/10000], Training Loss: 1.5236, Validation Loss: 28392.5234\n",
      "Epoch [9245/10000], Training Loss: 1.2935, Validation Loss: 30916.9785\n",
      "Epoch [9250/10000], Training Loss: 1.4735, Validation Loss: 30255.5449\n",
      "Epoch [9255/10000], Training Loss: 1.4340, Validation Loss: 18592.9492\n",
      "Epoch [9260/10000], Training Loss: 1.6503, Validation Loss: 13643.9258\n",
      "Epoch [9265/10000], Training Loss: 1.3014, Validation Loss: 11620.5098\n",
      "Epoch [9270/10000], Training Loss: 1.7089, Validation Loss: 15270.9248\n",
      "Epoch [9275/10000], Training Loss: 1.5931, Validation Loss: 17749.7578\n",
      "Epoch [9280/10000], Training Loss: 1.3271, Validation Loss: 20134.5820\n",
      "Epoch [9285/10000], Training Loss: 1.2507, Validation Loss: 15356.3105\n",
      "Epoch [9290/10000], Training Loss: 1.4045, Validation Loss: 17483.1816\n",
      "Epoch [9295/10000], Training Loss: 1.7082, Validation Loss: 15766.7461\n",
      "Epoch [9300/10000], Training Loss: 1.3414, Validation Loss: 16946.4180\n",
      "Epoch [9305/10000], Training Loss: 1.4693, Validation Loss: 16480.9590\n",
      "Epoch [9310/10000], Training Loss: 1.5523, Validation Loss: 17974.4902\n",
      "Epoch [9315/10000], Training Loss: 1.3738, Validation Loss: 17435.9414\n",
      "Epoch [9320/10000], Training Loss: 1.4967, Validation Loss: 20499.9766\n",
      "Epoch [9325/10000], Training Loss: 1.6663, Validation Loss: 25792.8379\n",
      "Epoch [9330/10000], Training Loss: 1.3896, Validation Loss: 28197.0859\n",
      "Epoch [9335/10000], Training Loss: 1.4603, Validation Loss: 30855.9609\n",
      "Epoch [9340/10000], Training Loss: 1.6045, Validation Loss: 34559.6016\n",
      "Epoch [9345/10000], Training Loss: 1.7172, Validation Loss: 25915.1621\n",
      "Epoch [9350/10000], Training Loss: 1.5051, Validation Loss: 21488.2461\n",
      "Epoch [9355/10000], Training Loss: 1.5658, Validation Loss: 16062.3838\n",
      "Epoch [9360/10000], Training Loss: 1.4147, Validation Loss: 17174.6816\n",
      "Epoch [9365/10000], Training Loss: 1.6844, Validation Loss: 18101.3516\n",
      "Epoch [9370/10000], Training Loss: 1.1872, Validation Loss: 16437.1191\n",
      "Epoch [9375/10000], Training Loss: 1.4147, Validation Loss: 16961.3164\n",
      "Epoch [9380/10000], Training Loss: 1.2320, Validation Loss: 16287.6768\n",
      "Epoch [9385/10000], Training Loss: 1.6901, Validation Loss: 18001.9180\n",
      "Epoch [9390/10000], Training Loss: 1.3750, Validation Loss: 20202.4336\n",
      "Epoch [9395/10000], Training Loss: 1.6803, Validation Loss: 21776.0000\n",
      "Epoch [9400/10000], Training Loss: 1.5297, Validation Loss: 23151.4746\n",
      "Epoch [9405/10000], Training Loss: 1.3533, Validation Loss: 20165.4980\n",
      "Epoch [9410/10000], Training Loss: 1.3616, Validation Loss: 15828.8496\n",
      "Epoch [9415/10000], Training Loss: 1.4059, Validation Loss: 18914.5117\n",
      "Epoch [9420/10000], Training Loss: 1.7098, Validation Loss: 23922.1367\n",
      "Epoch [9425/10000], Training Loss: 1.7381, Validation Loss: 23072.5391\n",
      "Epoch [9430/10000], Training Loss: 1.4463, Validation Loss: 24817.7949\n",
      "Epoch [9435/10000], Training Loss: 1.5309, Validation Loss: 22419.4961\n",
      "Epoch [9440/10000], Training Loss: 1.5661, Validation Loss: 21365.0996\n",
      "Epoch [9445/10000], Training Loss: 1.3388, Validation Loss: 22760.7676\n",
      "Epoch [9450/10000], Training Loss: 1.4034, Validation Loss: 25095.5547\n",
      "Epoch [9455/10000], Training Loss: 1.3921, Validation Loss: 29943.4277\n",
      "Epoch [9460/10000], Training Loss: 1.3761, Validation Loss: 29614.2070\n",
      "Epoch [9465/10000], Training Loss: 1.2026, Validation Loss: 25607.3242\n",
      "Epoch [9470/10000], Training Loss: 1.2249, Validation Loss: 23466.0879\n",
      "Epoch [9475/10000], Training Loss: 1.6879, Validation Loss: 16925.1777\n",
      "Epoch [9480/10000], Training Loss: 1.4840, Validation Loss: 15301.3896\n",
      "Epoch [9485/10000], Training Loss: 1.2916, Validation Loss: 18833.6426\n",
      "Epoch [9490/10000], Training Loss: 1.4420, Validation Loss: 26940.6641\n",
      "Epoch [9495/10000], Training Loss: 1.4256, Validation Loss: 30555.8711\n",
      "Epoch [9500/10000], Training Loss: 1.2291, Validation Loss: 37046.0234\n",
      "Epoch [9505/10000], Training Loss: 1.6038, Validation Loss: 26421.7852\n",
      "Epoch [9510/10000], Training Loss: 1.4334, Validation Loss: 18539.9219\n",
      "Epoch [9515/10000], Training Loss: 1.3347, Validation Loss: 17057.4238\n",
      "Epoch [9520/10000], Training Loss: 1.4053, Validation Loss: 20877.5254\n",
      "Epoch [9525/10000], Training Loss: 1.4831, Validation Loss: 21334.6211\n",
      "Epoch [9530/10000], Training Loss: 1.4123, Validation Loss: 22055.4805\n",
      "Epoch [9535/10000], Training Loss: 1.3887, Validation Loss: 22079.4863\n",
      "Epoch [9540/10000], Training Loss: 1.5478, Validation Loss: 20526.4512\n",
      "Epoch [9545/10000], Training Loss: 1.4181, Validation Loss: 20914.6055\n",
      "Epoch [9550/10000], Training Loss: 1.4541, Validation Loss: 23340.3945\n",
      "Epoch [9555/10000], Training Loss: 1.3434, Validation Loss: 24694.9004\n",
      "Epoch [9560/10000], Training Loss: 1.4664, Validation Loss: 27116.3887\n",
      "Epoch [9565/10000], Training Loss: 1.2504, Validation Loss: 19672.0000\n",
      "Epoch [9570/10000], Training Loss: 1.3431, Validation Loss: 16101.4932\n",
      "Epoch [9575/10000], Training Loss: 1.3851, Validation Loss: 14857.5195\n",
      "Epoch [9580/10000], Training Loss: 1.5153, Validation Loss: 16931.5898\n",
      "Epoch [9585/10000], Training Loss: 1.6967, Validation Loss: 22158.6074\n",
      "Epoch [9590/10000], Training Loss: 1.4522, Validation Loss: 22063.6777\n",
      "Epoch [9595/10000], Training Loss: 1.5293, Validation Loss: 19862.3828\n",
      "Epoch [9600/10000], Training Loss: 1.3553, Validation Loss: 22306.9414\n",
      "Epoch [9605/10000], Training Loss: 1.3337, Validation Loss: 27046.7715\n",
      "Epoch [9610/10000], Training Loss: 1.5316, Validation Loss: 26010.8340\n",
      "Epoch [9615/10000], Training Loss: 1.5082, Validation Loss: 22950.8340\n",
      "Epoch [9620/10000], Training Loss: 1.3984, Validation Loss: 26265.2051\n",
      "Epoch [9625/10000], Training Loss: 1.2625, Validation Loss: 23625.5371\n",
      "Epoch [9630/10000], Training Loss: 1.4421, Validation Loss: 21297.3750\n",
      "Epoch [9635/10000], Training Loss: 1.3854, Validation Loss: 19995.1055\n",
      "Epoch [9640/10000], Training Loss: 1.3406, Validation Loss: 23791.7441\n",
      "Epoch [9645/10000], Training Loss: 1.3099, Validation Loss: 24371.9941\n",
      "Epoch [9650/10000], Training Loss: 1.3881, Validation Loss: 25195.5566\n",
      "Epoch [9655/10000], Training Loss: 1.6422, Validation Loss: 21816.9121\n",
      "Epoch [9660/10000], Training Loss: 1.3160, Validation Loss: 21150.9922\n",
      "Epoch [9665/10000], Training Loss: 1.5364, Validation Loss: 18742.7441\n",
      "Epoch [9670/10000], Training Loss: 1.3192, Validation Loss: 16675.2168\n",
      "Epoch [9675/10000], Training Loss: 1.3445, Validation Loss: 22015.8438\n",
      "Epoch [9680/10000], Training Loss: 1.3859, Validation Loss: 22757.2793\n",
      "Epoch [9685/10000], Training Loss: 1.5274, Validation Loss: 24841.7871\n",
      "Epoch [9690/10000], Training Loss: 1.4815, Validation Loss: 25960.9688\n",
      "Epoch [9695/10000], Training Loss: 1.6819, Validation Loss: 25561.3379\n",
      "Epoch [9700/10000], Training Loss: 1.3596, Validation Loss: 29293.6172\n",
      "Epoch [9705/10000], Training Loss: 1.4566, Validation Loss: 25421.5547\n",
      "Epoch [9710/10000], Training Loss: 1.3667, Validation Loss: 24586.1250\n",
      "Epoch [9715/10000], Training Loss: 1.6106, Validation Loss: 21363.3867\n",
      "Epoch [9720/10000], Training Loss: 1.5588, Validation Loss: 18890.1973\n",
      "Epoch [9725/10000], Training Loss: 1.4626, Validation Loss: 21292.9199\n",
      "Epoch [9730/10000], Training Loss: 1.2897, Validation Loss: 22948.1270\n",
      "Epoch [9735/10000], Training Loss: 1.5990, Validation Loss: 25125.3066\n",
      "Epoch [9740/10000], Training Loss: 1.4892, Validation Loss: 29323.8340\n",
      "Epoch [9745/10000], Training Loss: 1.3017, Validation Loss: 25158.6992\n",
      "Epoch [9750/10000], Training Loss: 1.4125, Validation Loss: 20381.2012\n",
      "Epoch [9755/10000], Training Loss: 1.4664, Validation Loss: 23061.0977\n",
      "Epoch [9760/10000], Training Loss: 1.4707, Validation Loss: 23689.5449\n",
      "Epoch [9765/10000], Training Loss: 1.1708, Validation Loss: 21687.8047\n",
      "Epoch [9770/10000], Training Loss: 1.3927, Validation Loss: 23633.1270\n",
      "Epoch [9775/10000], Training Loss: 1.3638, Validation Loss: 26640.4473\n",
      "Epoch [9780/10000], Training Loss: 1.3504, Validation Loss: 28860.6426\n",
      "Epoch [9785/10000], Training Loss: 1.3719, Validation Loss: 27064.0703\n",
      "Epoch [9790/10000], Training Loss: 1.0793, Validation Loss: 24524.2422\n",
      "Epoch [9795/10000], Training Loss: 1.3750, Validation Loss: 20069.6289\n",
      "Epoch [9800/10000], Training Loss: 1.2696, Validation Loss: 18149.3203\n",
      "Epoch [9805/10000], Training Loss: 1.3127, Validation Loss: 16926.7266\n",
      "Epoch [9810/10000], Training Loss: 1.3861, Validation Loss: 15929.1143\n",
      "Epoch [9815/10000], Training Loss: 1.3899, Validation Loss: 14516.4473\n",
      "Epoch [9820/10000], Training Loss: 1.3422, Validation Loss: 14016.3174\n",
      "Epoch [9825/10000], Training Loss: 1.3752, Validation Loss: 15507.1924\n",
      "Epoch [9830/10000], Training Loss: 1.5001, Validation Loss: 17753.3027\n",
      "Epoch [9835/10000], Training Loss: 1.3671, Validation Loss: 19658.0176\n",
      "Epoch [9840/10000], Training Loss: 1.6045, Validation Loss: 24219.3945\n",
      "Epoch [9845/10000], Training Loss: 1.5875, Validation Loss: 26620.7949\n",
      "Epoch [9850/10000], Training Loss: 1.5112, Validation Loss: 24969.8926\n",
      "Epoch [9855/10000], Training Loss: 1.4739, Validation Loss: 26442.2539\n",
      "Epoch [9860/10000], Training Loss: 1.3133, Validation Loss: 24394.2246\n",
      "Epoch [9865/10000], Training Loss: 1.7940, Validation Loss: 17051.3086\n",
      "Epoch [9870/10000], Training Loss: 1.2665, Validation Loss: 15815.8896\n",
      "Epoch [9875/10000], Training Loss: 1.4750, Validation Loss: 19401.8848\n",
      "Epoch [9880/10000], Training Loss: 1.4436, Validation Loss: 21557.1562\n",
      "Epoch [9885/10000], Training Loss: 1.6795, Validation Loss: 24445.1445\n",
      "Epoch [9890/10000], Training Loss: 1.4131, Validation Loss: 27694.7070\n",
      "Epoch [9895/10000], Training Loss: 1.4566, Validation Loss: 29119.8945\n",
      "Epoch [9900/10000], Training Loss: 1.2060, Validation Loss: 28764.5234\n",
      "Epoch [9905/10000], Training Loss: 1.6700, Validation Loss: 26879.2188\n",
      "Epoch [9910/10000], Training Loss: 1.2929, Validation Loss: 23712.3418\n",
      "Epoch [9915/10000], Training Loss: 1.4264, Validation Loss: 22770.4414\n",
      "Epoch [9920/10000], Training Loss: 1.4955, Validation Loss: 21859.6094\n",
      "Epoch [9925/10000], Training Loss: 1.2518, Validation Loss: 23672.7051\n",
      "Epoch [9930/10000], Training Loss: 1.4641, Validation Loss: 23936.0020\n",
      "Epoch [9935/10000], Training Loss: 1.1782, Validation Loss: 22970.3516\n",
      "Epoch [9940/10000], Training Loss: 1.5278, Validation Loss: 23810.1992\n",
      "Epoch [9945/10000], Training Loss: 1.5922, Validation Loss: 25290.1836\n",
      "Epoch [9950/10000], Training Loss: 1.4690, Validation Loss: 23363.6504\n",
      "Epoch [9955/10000], Training Loss: 1.2207, Validation Loss: 20638.3438\n",
      "Epoch [9960/10000], Training Loss: 1.5940, Validation Loss: 24599.8477\n",
      "Epoch [9965/10000], Training Loss: 1.4342, Validation Loss: 22916.4590\n",
      "Epoch [9970/10000], Training Loss: 1.2950, Validation Loss: 23164.0000\n",
      "Epoch [9975/10000], Training Loss: 1.2113, Validation Loss: 18807.5938\n",
      "Epoch [9980/10000], Training Loss: 1.3979, Validation Loss: 19626.6543\n",
      "Epoch [9985/10000], Training Loss: 1.4408, Validation Loss: 22261.8047\n",
      "Epoch [9990/10000], Training Loss: 1.1339, Validation Loss: 19501.1328\n",
      "Epoch [9995/10000], Training Loss: 1.5855, Validation Loss: 18580.4121\n",
      "Epoch [10000/10000], Training Loss: 1.5077, Validation Loss: 22597.8262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 訓練 SVR 模型\n",
    "svr_model = SVR(kernel='rbf', C=1000, gamma=0.1)\n",
    "svr_model.fit(x_train.reshape(-1, 1), y_train.ravel())\n",
    "y_pred_svr = svr_model.predict(x_train.reshape(-1, 1))\n",
    "\n",
    "# 訓練模型\n",
    "num_epochs = 10000\n",
    "csv_file = 'training_results.csv'\n",
    "data_buffer = []\n",
    "\n",
    "    \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 設置模型為訓練模式\n",
    "    # 前向傳播\n",
    "    outputs = model(x_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # 反向傳播與優化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 驗證\n",
    "    model.eval()  # 設置模型為評估模式\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "        if (epoch+1) % 5 == 0:\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "            # 預測模型的輸出（使用 eval 模式，不需要 train）\n",
    "            y_pred_tensor = model(x_train_tensor).detach().cpu().numpy()\n",
    "\n",
    "            # 將數據存入 buffer 而不是直接寫入 CSV\n",
    "            for i in range(len(x_train)):\n",
    "                data_buffer.append([epoch+1, x_train[i], y_train[i], y_pred_tensor[i][0], y_pred_svr[i]])\n",
    "\n",
    "# 訓練結束後再一次性寫入 CSV\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'x_train', 'y_train', 'y_pred_tensor', 'y_pred_svr'])\n",
    "    writer.writerows(data_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4fc7b557fb4739b608fc4f65c53277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "csv_file = 'training_results.csv'\n",
    "\n",
    "\n",
    "# 創建儲存圖片的資料夾\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "\n",
    "\n",
    "def plot_from_csv(csv_file):\n",
    "    # 讀取數據時明確指定數據格式，以確保讀取為二維陣列\n",
    "    data = np.genfromtxt(csv_file, delimiter=',', skip_header=1, dtype=float)\n",
    "\n",
    "    # 檢查數據的形狀，避免索引錯誤\n",
    "    if data.ndim == 1:\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "\n",
    "    # 提取不同的 epoch\n",
    "    epochs = np.unique(data[:, 0])\n",
    "\n",
    "    for epoch in tqdm(epochs):\n",
    "          epoch_data = data[data[:, 0] == epoch]\n",
    "          x_train = epoch_data[:, 1]\n",
    "          y_train = epoch_data[:, 2]\n",
    "          y_pred_tensor = epoch_data[:, 3]\n",
    "          y_pred_svr = epoch_data[:, 4]\n",
    "\n",
    "          # 生成圖表\n",
    "          plt.figure(dpi=225)\n",
    "          plt.ylim([min(y_train), max(y_train) * 1.1])\n",
    "          plt.plot(x_train, y_train, label='True Function')\n",
    "          plt.plot(x_train, y_pred_tensor, label='Neural Network Approximation', linestyle='--')\n",
    "          plt.plot(x_train, y_pred_svr, label='SVR', linestyle='--')\n",
    "\n",
    "          plt.legend()\n",
    "          plt.xlabel('x')\n",
    "          plt.ylabel('f(x)')\n",
    "          plt.title(f'Epoch {int(epoch)}')\n",
    "          plt.grid(True)\n",
    "\n",
    "          plt.savefig(f'images/epoch_{int(epoch)}.png')\n",
    "          plt.close()\n",
    "          plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "# 訓練完成後，從 CSV 生成圖表\n",
    "plot_from_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "\n",
    "frame_rate = 60 \n",
    "frame_size = (640, 480) \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "\n",
    "out = cv2.VideoWriter('polynomial_approximation.mp4', fourcc, frame_rate, frame_size)\n",
    "for epoch in range(5, num_epochs+1, 5):\n",
    "    filename = f'images/epoch_{epoch}.png'\n",
    "    img = cv2.imread(filename)\n",
    "    img_resized = cv2.resize(img, frame_size)\n",
    "    out.write(img_resized)\n",
    "\n",
    "out.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
